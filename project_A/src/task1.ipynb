{"cells":[{"cell_type":"markdown","metadata":{"id":"hajAmAsD3Tuo"},"source":["## Project Initialization"]},{"cell_type":"markdown","metadata":{"id":"x1k_-2JDjdVP"},"source":["### MHISTDataet.py"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3723,"status":"ok","timestamp":1729642574507,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"uhhL6chvjaoW"},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","from PIL import Image\n","from torch.utils.data import Dataset\n","\n","\n","class MHISTDataset(Dataset):\n","    def __init__(self, split_dir, train=True, transform=None):\n","        \"\"\"\n","        Args:\n","            split_dir (str): Directory containing the split images (e.g., 'images-split/train' or 'images-split/test').\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.split_dir = Path(split_dir)\n","        if train:\n","            self.split_dir = self.split_dir / 'train'\n","        else:\n","            self.split_dir = self.split_dir / 'test'\n","\n","        self.transform = transform\n","\n","        # Map folder names to class labels\n","        self.label_map = {\"HP\": 0, \"SSA\": 1}\n","\n","        # Gather image file paths and their corresponding labels\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for label_name, label_value in self.label_map.items():\n","            class_dir = self.split_dir / label_name\n","            if class_dir.exists():\n","                for img_path in class_dir.rglob('*.*'):  # Recursively list all image files\n","                    self.image_paths.append(img_path)\n","                    self.labels.append(label_value)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Get the image path and corresponding label\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path)\n","\n","        label = self.labels[idx]\n","        # map label to class name, if 0 then HP, if 1 then SSA\n","\n","        name = \"HP\" if label == 0 else \"SSA\"\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"B0rkzMf_jtH9"},"source":["### Networks.py"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1729642574507,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"s2hOogVujv9I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# Acknowledgement to\n","# https://github.com/kuangliu/pytorch-cifar,\n","# https://github.com/BIGBALLON/CIFAR-ZOO,\n","\n","\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","\n","''' Swish activation '''\n","class Swish(nn.Module): # Swish(x) = x∗σ(x)\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, input):\n","        return input * torch.sigmoid(input)\n","\n","\n","''' MLP '''\n","class MLP(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(MLP, self).__init__()\n","        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n","        self.fc_2 = nn.Linear(128, 128)\n","        self.fc_3 = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        out = x.view(x.size(0), -1)\n","        out = F.relu(self.fc_1(out))\n","        out = F.relu(self.fc_2(out))\n","        out = self.fc_3(out)\n","        return out\n","\n","\n","\n","''' ConvNet '''\n","class ConvNet(nn.Module):\n","    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n","        super(ConvNet, self).__init__()\n","\n","        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n","        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n","        self.classifier = nn.Linear(num_feat, num_classes)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","    def _get_activation(self, net_act):\n","        if net_act == 'sigmoid':\n","            return nn.Sigmoid()\n","        elif net_act == 'relu':\n","            return nn.ReLU(inplace=True)\n","        elif net_act == 'leakyrelu':\n","            return nn.LeakyReLU(negative_slope=0.01)\n","        elif net_act == 'swish':\n","            return Swish()\n","        else:\n","            exit('unknown activation function: %s'%net_act)\n","\n","    def _get_pooling(self, net_pooling):\n","        if net_pooling == 'maxpooling':\n","            return nn.MaxPool2d(kernel_size=2, stride=2)\n","        elif net_pooling == 'avgpooling':\n","            return nn.AvgPool2d(kernel_size=2, stride=2)\n","        elif net_pooling == 'none':\n","            return None\n","        else:\n","            exit('unknown net_pooling: %s'%net_pooling)\n","\n","    def _get_normlayer(self, net_norm, shape_feat):\n","        # shape_feat = (c*h*w)\n","        if net_norm == 'batchnorm':\n","            return nn.BatchNorm2d(shape_feat[0], affine=True)\n","        elif net_norm == 'layernorm':\n","            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n","        elif net_norm == 'instancenorm':\n","            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n","        elif net_norm == 'groupnorm':\n","            return nn.GroupNorm(4, shape_feat[0], affine=True)\n","        elif net_norm == 'none':\n","            return None\n","        else:\n","            exit('unknown net_norm: %s'%net_norm)\n","\n","    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n","        layers = []\n","        in_channels = channel\n","        if im_size[0] == 28:\n","            im_size = (32, 32)\n","        shape_feat = [in_channels, im_size[0], im_size[1]]\n","        for d in range(net_depth):\n","            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n","            shape_feat[0] = net_width\n","            if net_norm != 'none':\n","                layers += [self._get_normlayer(net_norm, shape_feat)]\n","            layers += [self._get_activation(net_act)]\n","            in_channels = net_width\n","            if net_pooling != 'none':\n","                layers += [self._get_pooling(net_pooling)]\n","                shape_feat[1] //= 2\n","                shape_feat[2] //= 2\n","\n","        return nn.Sequential(*layers), shape_feat\n","\n","\n","\n","''' LeNet '''\n","class LeNet(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(LeNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(6, 16, kernel_size=5),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc_2 = nn.Linear(120, 84)\n","        self.fc_3 = nn.Linear(84, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc_1(x))\n","        x = F.relu(self.fc_2(x))\n","        x = self.fc_3(x)\n","        return x\n","\n","\n","\n","''' AlexNet '''\n","class AlexNet(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","\n","''' AlexNetBN '''\n","class AlexNetBN(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(AlexNetBN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","\n","''' VGG '''\n","cfg_vgg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name, channel, num_classes, norm='instancenorm'):\n","        super(VGG, self).__init__()\n","        self.channel = channel\n","        self.features = self._make_layers(cfg_vgg[vgg_name], norm)\n","        self.classifier = nn.Linear(512 if vgg_name != 'VGGS' else 128, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def _make_layers(self, cfg, norm):\n","        layers = []\n","        in_channels = self.channel\n","        for ic, x in enumerate(cfg):\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=3 if self.channel==1 and ic==0 else 1),\n","                           nn.GroupNorm(x, x, affine=True) if norm=='instancenorm' else nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","\n","def VGG11(channel, num_classes):\n","    return VGG('VGG11', channel, num_classes)\n","def VGG11BN(channel, num_classes):\n","    return VGG('VGG11', channel, num_classes, norm='batchnorm')\n","def VGG13(channel, num_classes):\n","    return VGG('VGG13', channel, num_classes)\n","def VGG16(channel, num_classes):\n","    return VGG('VGG16', channel, num_classes)\n","def VGG19(channel, num_classes):\n","    return VGG('VGG19', channel, num_classes)\n","\n","\n","''' ResNet_AP '''\n","# The conv(stride=2) is replaced by conv(stride=1) + avgpool(kernel_size=2, stride=2)\n","\n","class BasicBlock_AP(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(BasicBlock_AP, self).__init__()\n","        self.norm = norm\n","        self.stride = stride\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n","                nn.AvgPool2d(kernel_size=2, stride=2), # modification\n","                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        if self.stride != 1: # modification\n","            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck_AP(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(Bottleneck_AP, self).__init__()\n","        self.norm = norm\n","        self.stride = stride\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n","                nn.AvgPool2d(kernel_size=2, stride=2),  # modification\n","                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        if self.stride != 1: # modification\n","            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet_AP(nn.Module):\n","    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n","        super(ResNet_AP, self).__init__()\n","        self.in_planes = 64\n","        self.norm = norm\n","\n","        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.classifier = nn.Linear(512 * block.expansion * 3 * 3 if channel==1 else 512 * block.expansion * 4 * 4, num_classes)  # modification\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride, self.norm))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","def ResNet18BN_AP(channel, num_classes):\n","    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n","\n","def ResNet18_AP(channel, num_classes):\n","    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes)\n","\n","\n","''' ResNet '''\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(BasicBlock, self).__init__()\n","        self.norm = norm\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(Bottleneck, self).__init__()\n","        self.norm = norm\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.norm = norm\n","\n","        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.classifier = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride, self.norm))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","\n","def ResNet18BN(channel, num_classes):\n","    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n","\n","def ResNet18(channel, num_classes):\n","    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes)\n","\n","def ResNet34(channel, num_classes):\n","    return ResNet(BasicBlock, [3,4,6,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet50(channel, num_classes):\n","    return ResNet(Bottleneck, [3,4,6,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet101(channel, num_classes):\n","    return ResNet(Bottleneck, [3,4,23,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet152(channel, num_classes):\n","    return ResNet(Bottleneck, [3,8,36,3], channel=channel, num_classes=num_classes)"]},{"cell_type":"markdown","metadata":{"id":"2FQSh2D_jiC-"},"source":["### Utils.py"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4706,"status":"ok","timestamp":1729642585107,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"eKjR99HKjj46"},"outputs":[],"source":["import time\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torchvision import datasets, transforms\n","from scipy.ndimage import rotate as scipyrotate\n","import sys\n","#from networks import MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n","#import MHISTDataset\n","\n","\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","def get_attention(feature_set, param=0, exp=4, norm='l2'):\n","    if param==0:\n","        attention_map = torch.sum(torch.abs(feature_set), dim=1)\n","\n","    elif param ==1:\n","        attention_map =  torch.sum(torch.abs(feature_set)**exp, dim=1)\n","\n","    elif param == 2:\n","        attention_map =  torch.max(torch.abs(feature_set)**exp, dim=1)\n","\n","    if norm == 'l2':\n","        # Dimension: [B x (H*W)] -- Vectorized\n","        vectorized_attention_map =  attention_map.view(feature_set.size(0), -1)\n","        normalized_attention_maps = F.normalize(vectorized_attention_map, p=2.0)\n","\n","    elif norm == 'fro':\n","        # Dimension: [B x H x W] -- Un-Vectorized\n","        un_vectorized_attention_map =  attention_map\n","        # Dimension: [B]\n","        fro_norm = torch.sum(torch.sum(torch.abs(attention_map)**2, dim=1), dim=1)\n","        # Dimension: [B x H x W] -- Un-Vectorized)\n","        normalized_attention_maps = un_vectorized_attention_map / fro_norm.unsqueeze(dim=-1).unsqueeze(dim=-1)\n","    elif norm == 'l1':\n","        # Dimension: [B x (H*W)] -- Vectorized\n","        vectorized_attention_map =  attention_map.view(feature_set.size(0), -1)\n","        normalized_attention_maps = F.normalize(vectorized_attention_map, p=1.0)\n","\n","    elif norm =='none':\n","        normalized_attention_maps = attention_map\n","\n","    elif norm == 'none-vectorized':\n","        normalized_attention_maps =  attention_map.view(feature_set.size(0), -1)\n","\n","    return normalized_attention_maps\n","\n","def get_dataset(dataset, data_path):\n","    if dataset == 'MNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.1307]\n","        std = [0.3081]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","\n","    elif dataset == 'FashionMNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.2861]\n","        std = [0.3530]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'SVHN':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4377, 0.4438, 0.4728]\n","        std = [0.1980, 0.2010, 0.1970]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.SVHN(data_path, split='train', download=True, transform=transform)  # no augmentation\n","        dst_test = datasets.SVHN(data_path, split='test', download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","\n","    elif dataset == 'CIFAR10':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4914, 0.4822, 0.4465]\n","        std = [0.2023, 0.1994, 0.2010]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'CIFAR100':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 100\n","        mean = [0.5071, 0.4866, 0.4409]\n","        std = [0.2673, 0.2564, 0.2762]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'TinyImageNet':\n","        channel = 3\n","        im_size = (64, 64)\n","        num_classes = 200\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","        data = torch.load(os.path.join(data_path, 'tinyimagenet.pt'), map_location='cpu')\n","\n","        class_names = data['classes']\n","\n","        images_train = data['images_train']\n","        labels_train = data['labels_train']\n","        images_train = images_train.detach().float() / 255.0\n","        labels_train = labels_train.detach()\n","        for c in range(channel):\n","            images_train[:,c] = (images_train[:,c] - mean[c])/std[c]\n","        dst_train = TensorDataset(images_train, labels_train)  # no augmentation\n","\n","        images_val = data['images_val']\n","        labels_val = data['labels_val']\n","        images_val = images_val.detach().float() / 255.0\n","        labels_val = labels_val.detach()\n","\n","        for c in range(channel):\n","            images_val[:, c] = (images_val[:, c] - mean[c]) / std[c]\n","\n","        dst_test = TensorDataset(images_val, labels_val)  # no augmentation\n","\n","    elif dataset == 'MHIST':\n","        channel = 3\n","        im_size = (224, 224)\n","        num_classes = 2\n","        class_names = ['HP', 'SSA']\n","        data_path = '../mhist_dataset/images-split'\n","        to_tensor = transforms.ToTensor()\n","\n","        dst_train = MHISTDataset.MHISTDataset(data_path, train=True, transform=to_tensor)\n","        dst_test = MHISTDataset.MHISTDataset(data_path, train=False, transform=to_tensor)\n","\n","        # calculate mean and std\n","        mean = [0.5, 0.5, 0.5]  # Adjust these values based on dataset statistics\n","        std = [0.1, 0.1, 0.1]  # Adjust these values based on dataset statistics\n","\n","    else:\n","        exit('unknown dataset: %s'%dataset)\n","\n","\n","    testloader = torch.utils.data.DataLoader(dst_test, batch_size=64, shuffle=False, num_workers=0)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=64, shuffle=True, num_workers=0)\n","    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, trainloader\n","\n","\n","\n","class TensorDataset(Dataset):\n","    def __init__(self, images, labels): # images: n x c x h x w tensor\n","        self.images = images.detach().float()\n","        self.labels = labels.detach()\n","\n","    def __getitem__(self, index):\n","        return self.images[index], self.labels[index]\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","\n","\n","def get_default_convnet_setting():\n","    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n","    return net_width, net_depth, net_act, net_norm, net_pooling\n","\n","\n","\n","def get_network(model, channel, num_classes, im_size=(32, 32)):\n","    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n","    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n","\n","    if model == 'MLP':\n","        net = MLP(channel=channel, num_classes=num_classes)\n","    elif model == 'ConvNet':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'LeNet':\n","        net = LeNet(channel=channel, num_classes=num_classes)\n","    elif model == 'AlexNet':\n","        net = AlexNet(channel=channel, num_classes=num_classes)\n","    elif model == 'AlexNetBN':\n","        net = AlexNetBN(channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11':\n","        net = VGG11( channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11BN':\n","        net = VGG11BN(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18':\n","        net = ResNet18(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN_AP':\n","        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN':\n","        net = ResNet18BN(channel=channel, num_classes=num_classes)\n","\n","    elif model == 'ConvNetD1':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD2':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD3':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD4':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD7':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=7, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW32':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW64':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW128':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW256':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetAS':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetAR':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetAL':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetASwish':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetASwishBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetNN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetLN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetIN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetGN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetNP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none', im_size=im_size)\n","    elif model == 'ConvNetMP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling', im_size=im_size)\n","    elif model == 'ConvNetAP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling', im_size=im_size)\n","\n","    else:\n","        net = None\n","        exit('unknown model: %s'%model)\n","\n","    gpu_num = torch.cuda.device_count()\n","    if gpu_num>0:\n","        device = 'cuda'\n","        if gpu_num>1:\n","            net = nn.DataParallel(net)\n","    else:\n","        device = 'cpu'\n","    net = net.to(device)\n","\n","    return net\n","\n","\n","\n","def get_time():\n","    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n","\n","\n","\n","def distance_wb(gwr, gws):\n","    shape = gwr.shape\n","    if len(shape) == 4: # conv, out*in*h*w\n","        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n","        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n","    elif len(shape) == 3:  # layernorm, C*h*w\n","        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n","        gws = gws.reshape(shape[0], shape[1] * shape[2])\n","    elif len(shape) == 2: # linear, out*in\n","        tmp = 'do nothing'\n","    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n","        gwr = gwr.reshape(1, shape[0])\n","        gws = gws.reshape(1, shape[0])\n","        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n","\n","    dis_weight = torch.sum(1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001))\n","    dis = dis_weight\n","    return dis\n","\n","\n","\n","def match_loss(gw_syn, gw_real, args):\n","    dis = torch.tensor(0.0).to(args.device)\n","\n","    if args.dis_metric == 'ours':\n","        for ig in range(len(gw_real)):\n","            gwr = gw_real[ig]\n","            gws = gw_syn[ig]\n","            dis += distance_wb(gwr, gws)\n","\n","    elif args.dis_metric == 'mse':\n","        gw_real_vec = []\n","        gw_syn_vec = []\n","        for ig in range(len(gw_real)):\n","            gw_real_vec.append(gw_real[ig].reshape((-1)))\n","            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n","        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n","        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n","        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n","\n","    elif args.dis_metric == 'cos':\n","        gw_real_vec = []\n","        gw_syn_vec = []\n","        for ig in range(len(gw_real)):\n","            gw_real_vec.append(gw_real[ig].reshape((-1)))\n","            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n","        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n","        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n","        dis = 1 - torch.sum(gw_real_vec * gw_syn_vec, dim=-1) / (torch.norm(gw_real_vec, dim=-1) * torch.norm(gw_syn_vec, dim=-1) + 0.000001)\n","\n","    else:\n","        exit('unknown distance function: %s'%args.dis_metric)\n","\n","    return dis\n","\n","\n","\n","def get_loops(ipc):\n","    # Get the two hyper-parameters of outer-loop and inner-loop.\n","    # The following values are empirically good.\n","    if ipc == 1:\n","        outer_loop, inner_loop = 1, 1\n","    elif ipc == 10:\n","        outer_loop, inner_loop = 10, 50\n","    elif ipc == 20:\n","        outer_loop, inner_loop = 20, 25\n","    elif ipc == 30:\n","        outer_loop, inner_loop = 30, 20\n","    elif ipc == 40:\n","        outer_loop, inner_loop = 40, 15\n","    elif ipc == 50:\n","        outer_loop, inner_loop = 50, 10\n","    else:\n","        outer_loop, inner_loop = 0, 0\n","        exit('loop hyper-parameters are not defined for %d ipc'%ipc)\n","    return outer_loop, inner_loop\n","\n","\n","\n","def epoch(mode, dataloader, net, optimizer, criterion, args, aug):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(args.device)\n","    criterion = criterion.to(args.device)\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(args.device)\n","        if aug:\n","            if args.dsa:\n","                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n","            else:\n","                pass\n","                #img = augment(img, args.dc_aug_param, device=args.device)\n","        lab = datum[1].long().to(args.device)\n","        n_b = lab.shape[0]\n","\n","        output = net(img)\n","        loss = criterion(output, lab)\n","        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","\n","        loss_avg += loss.item()*n_b\n","        acc_avg += acc\n","        num_exp += n_b\n","\n","        if mode == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","\n","\n","def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args):\n","    net = net.to(args.device)\n","    images_train = images_train.to(args.device)\n","    labels_train = labels_train.to(args.device)\n","    lr = float(args.lr_net)\n","    Epoch = int(args.epoch_eval_train)\n","    lr_schedule = [Epoch//2+1]\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","    criterion = nn.CrossEntropyLoss().to(args.device)\n","\n","    dst_train = TensorDataset(images_train, labels_train)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","    start = time.time()\n","    for ep in range(Epoch+1):\n","        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug = True)\n","        if ep in lr_schedule:\n","            lr *= 0.1\n","            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","\n","    time_train = time.time() - start\n","    loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, aug = False)\n","    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n","\n","    return net, acc_train, acc_test\n","\n","\n","\n","def augment(images, dc_aug_param, device):\n","    # This can be sped up in the future.\n","\n","    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n","        scale = dc_aug_param['scale']\n","        crop = dc_aug_param['crop']\n","        rotate = dc_aug_param['rotate']\n","        noise = dc_aug_param['noise']\n","        strategy = dc_aug_param['strategy']\n","\n","        shape = images.shape\n","        mean = []\n","        for c in range(shape[1]):\n","            mean.append(float(torch.mean(images[:,c])))\n","\n","        def cropfun(i):\n","            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n","            for c in range(shape[1]):\n","                im_[c] = mean[c]\n","            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n","            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n","            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n","\n","        def scalefun(i):\n","            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n","            mhw = max(h, w, shape[2], shape[3])\n","            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n","            r = int((mhw - h) / 2)\n","            c = int((mhw - w) / 2)\n","            im_[:, r:r + h, c:c + w] = tmp\n","            r = int((mhw - shape[2]) / 2)\n","            c = int((mhw - shape[3]) / 2)\n","            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n","\n","        def rotatefun(i):\n","            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n","            r = int((im_.shape[-2] - shape[-2]) / 2)\n","            c = int((im_.shape[-1] - shape[-1]) / 2)\n","            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n","\n","        def noisefun(i):\n","            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n","\n","\n","        augs = strategy.split('_')\n","\n","        for i in range(shape[0]):\n","            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n","            if choice == 'crop':\n","                cropfun(i)\n","            elif choice == 'scale':\n","                scalefun(i)\n","            elif choice == 'rotate':\n","                rotatefun(i)\n","            elif choice == 'noise':\n","                noisefun(i)\n","\n","    return images\n","\n","\n","\n","def get_daparam(dataset, model, model_eval, ipc):\n","    # We find that augmentation doesn't always benefit the performance.\n","    # So we do augmentation for some of the settings.\n","\n","    dc_aug_param = dict()\n","    dc_aug_param['crop'] = 4\n","    dc_aug_param['scale'] = 0.2\n","    dc_aug_param['rotate'] = 45\n","    dc_aug_param['noise'] = 0.001\n","    dc_aug_param['strategy'] = 'none'\n","\n","    if dataset == 'MNIST':\n","        dc_aug_param['strategy'] = 'crop_scale_rotate'\n","\n","    if model_eval in ['ConvNetBN']: # Data augmentation makes model training with Batch Norm layer easier.\n","        dc_aug_param['strategy'] = 'crop_noise'\n","\n","    return dc_aug_param\n","\n","\n","def get_eval_pool(eval_mode, model, model_eval):\n","    if eval_mode == 'M': # multiple architectures\n","        model_eval_pool = ['MLP', 'ConvNet', 'LeNet', 'AlexNet', 'VGG11', 'ResNet18']\n","    elif eval_mode == 'B':  # multiple architectures with BatchNorm for DM experiments\n","        model_eval_pool = ['ConvNetBN', 'ConvNetASwishBN', 'AlexNetBN', 'VGG11BN', 'ResNet18BN']\n","    elif eval_mode == 'W': # ablation study on network width\n","        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n","    elif eval_mode == 'D': # ablation study on network depth\n","        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n","    elif eval_mode == 'A': # ablation study on network activation function\n","        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL', 'ConvNetASwish']\n","    elif eval_mode == 'P': # ablation study on network pooling layer\n","        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n","    elif eval_mode == 'N': # ablation study on network normalization layer\n","        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n","    elif eval_mode == 'S': # itself\n","        if 'BN' in model:\n","            print('Attention: Here I will replace BN with IN in evaluation, as the synthetic set is too small to measure BN hyper-parameters.')\n","        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n","    elif eval_mode == 'SS':  # itself\n","        model_eval_pool = [model]\n","    else:\n","        model_eval_pool = [model_eval]\n","    return model_eval_pool\n","\n","\n","class ParamDiffAug():\n","    def __init__(self):\n","        self.aug_mode = 'S' #'multiple or single'\n","        self.prob_flip = 0.5\n","        self.ratio_scale = 1.2\n","        self.ratio_rotate = 15.0\n","        self.ratio_crop_pad = 0.125\n","        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n","        self.brightness = 1.0\n","        self.saturation = 2.0\n","        self.contrast = 0.5\n","\n","\n","def set_seed_DiffAug(param):\n","    if param.latestseed == -1:\n","        return\n","    else:\n","        torch.random.manual_seed(param.latestseed)\n","        param.latestseed += 1\n","\n","\n","def DiffAugment(x, strategy='', seed = -1, param = None):\n","    if strategy == 'None' or strategy == 'none' or strategy == '':\n","        return x\n","\n","    if seed == -1:\n","        param.Siamese = False\n","    else:\n","        param.Siamese = True\n","\n","    param.latestseed = seed\n","\n","    if strategy:\n","        if param.aug_mode == 'M': # original\n","            for p in strategy.split('_'):\n","                for f in AUGMENT_FNS[p]:\n","                    x = f(x, param)\n","        elif param.aug_mode == 'S':\n","            pbties = strategy.split('_')\n","            set_seed_DiffAug(param)\n","            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n","            for f in AUGMENT_FNS[p]:\n","                x = f(x, param)\n","        else:\n","            exit('unknown augmentation mode: %s'%param.aug_mode)\n","        x = x.contiguous()\n","    return x\n","\n","\n","# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n","def rand_scale(x, param):\n","    # x>1, max scale\n","    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n","    ratio = param.ratio_scale\n","    set_seed_DiffAug(param)\n","    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    set_seed_DiffAug(param)\n","    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    theta = [[[sx[i], 0,  0],\n","            [0,  sy[i], 0],] for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.Siamese: # Siamese augmentation:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape).to(x.device)\n","    x = F.grid_sample(x, grid)\n","    return x\n","\n","\n","def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n","    ratio = param.ratio_rotate\n","    set_seed_DiffAug(param)\n","    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n","    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n","        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.Siamese: # Siamese augmentation:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape).to(x.device)\n","    x = F.grid_sample(x, grid)\n","    return x\n","\n","\n","def rand_flip(x, param):\n","    prob = param.prob_flip\n","    set_seed_DiffAug(param)\n","    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n","    if param.Siamese: # Siamese augmentation:\n","        randf[:] = randf[0]\n","    return torch.where(randf < prob, x.flip(3), x)\n","\n","\n","def rand_brightness(x, param):\n","    ratio = param.brightness\n","    set_seed_DiffAug(param)\n","    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        randb[:] = randb[0]\n","    x = x + (randb - 0.5)*ratio\n","    return x\n","\n","\n","def rand_saturation(x, param):\n","    ratio = param.saturation\n","    x_mean = x.mean(dim=1, keepdim=True)\n","    set_seed_DiffAug(param)\n","    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        rands[:] = rands[0]\n","    x = (x - x_mean) * (rands * ratio) + x_mean\n","    return x\n","\n","\n","def rand_contrast(x, param):\n","    ratio = param.contrast\n","    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n","    set_seed_DiffAug(param)\n","    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        randc[:] = randc[0]\n","    x = (x - x_mean) * (randc + ratio) + x_mean\n","    return x\n","\n","\n","def rand_crop(x, param):\n","    # The image is padded on its surrounding and then cropped.\n","    ratio = param.ratio_crop_pad\n","    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        translation_x[:] = translation_x[0]\n","        translation_y[:] = translation_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n","    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n","    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n","    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n","    return x\n","\n","\n","def rand_cutout(x, param):\n","    ratio = param.ratio_cutout\n","    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        offset_x[:] = offset_x[0]\n","        offset_y[:] = offset_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n","    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n","    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n","    mask[grid_batch, grid_x, grid_y] = 0\n","    x = x * mask.unsqueeze(1)\n","    return x\n","\n","\n","AUGMENT_FNS = {\n","    'color': [rand_brightness, rand_saturation, rand_contrast],\n","    'crop': [rand_crop],\n","    'cutout': [rand_cutout],\n","    'flip': [rand_flip],\n","    'scale': [rand_scale],\n","    'rotate': [rand_rotate],\n","}"]},{"cell_type":"markdown","metadata":{"id":"PbCoX2FdkFx5"},"source":["### dataDAM.py"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1729642724305,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"DuDxIsufkD5z","outputId":"f24a4ea4-f7fa-4fca-dd33-838da8971773"},"outputs":[],"source":["\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","\n","import os\n","import time\n","import copy\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","\n","#from utils import get_dataset, get_network, get_eval_pool, evaluate_synset, get_time, DiffAugment, ParamDiffAug, get_attention\n","\n","def dataDAM():\n","    \n","    parser = argparse.ArgumentParser(description='DataDAM')\n","    parser.add_argument('--dataset', default='MNIST', type=str, help='dataset name')\n","    parser.add_argument('--data_path', default='../datasets', type=str, help='data path')\n","    parser.add_argument('--model', default='ConvNet', type=str, help='model name')\n","    parser.add_argument('--K', default=100, type=int, help='Number of random weight initializations')\n","    parser.add_argument('--T', default=10, type=int, help='Number of iterations')\n","    parser.add_argument('--eta_S', default=0.1, type=int, help='earning rate for the condensed samples')\n","    parser.add_argument('--eta_theta', default=0.01, type=int, help='earning rate for the model')\n","    parser.add_argument('--zeta_S', default=1, type=int, help='Number of optimization steps for condensed samples')\n","    parser.add_argument('--zeta_theta', default=50, type=int, help='Number of optimization steps for the model')\n","    parser.add_argument('--batch_real', default=256, type=int, help='Batch size for real data')\n","    parser.add_argument('--batch_train', default=256, type=int, help='Batch size for training networks')\n","    parser.add_argument('--ipc', default=10, type=int, help='Images per class')\n","    parser.add_argument('--init', default='real', type=str, help='Initialization of synthetic dataset')\n","    \n","    args = parser.parse_args()\n","    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    args.dis_metric = 'ours'\n","    \n","    # Create a list of tuples with argument names and their values\n","    arguments = [\n","        ('dataset', args.dataset),\n","        ('data_path', args.data_path),\n","        ('model', args.model),\n","        ('K', args.K),\n","        ('T', args.T),\n","        ('eta_S', args.eta_S),\n","        ('eta_theta', args.eta_theta),\n","        ('zeta_S', args.zeta_S),\n","        ('zeta_theta', args.zeta_theta),\n","        ('batch_real', args.batch_real),\n","        ('batch_train', args.batch_train),\n","        ('ipc', args.ipc),\n","        ('init', args.init),\n","        ('device', args.device)\n","    ]\n","\n","    # create pandas dataframe of args\n","    print(arguments)\n","    \n","    \n","    (\n","        channel,\n","        im_size,\n","        num_classes,\n","        class_names,\n","        mean,\n","        std,\n","        train_dataset,\n","        test_dataset,\n","        test_datasetLoader,\n","        train_datasetLoader,\n","        ) = get_dataset(args.dataset, args.data_path)\n","\n","    ''' organize the real dataset '''\n","    images_all = []\n","    labels_all = []\n","    indices_class = [[] for c in range(num_classes)]\n","    images_all = [torch.unsqueeze(train_dataset[i][0], dim=0) for i in range(len(train_dataset))]\n","    labels_all = [train_dataset[i][1] for i in range(len(train_dataset))]\n","    indices_class = [[] for c in range(num_classes)]\n","    for i, lab in enumerate(labels_all):\n","        indices_class[lab].append(i)\n","    images_all = torch.cat(images_all, dim=0).to(args.device)\n","    labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n","\n","    def get_images(c, n): # get random n images from class c\n","        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n","        return images_all[idx_shuffle]\n","    \n","    def error(real, syn, err_type=\"MSE\"):\n","        if(err_type == \"MSE\"):\n","            err = torch.sum((torch.mean(real, dim=0) - torch.mean(syn, dim=0))**2)\n","        elif(err_type == \"MSE_B\"):\n","            err = torch.sum((torch.mean(real.reshape(num_classes, args.batch_real, -1), dim=1).cpu() - torch.mean(syn.cpu().reshape(num_classes, args.ipc, -1), dim=1))**2)\n","        return err\n","    \n","    ''' initialize the synthetic data '''\n","    image_syn = torch.randn(\n","        size=(num_classes * args.ipc, channel, im_size[0], im_size[1]),\n","        dtype=torch.float,\n","        requires_grad=True,\n","        device=args.device,\n","    )\n","    label_syn = torch.tensor(\n","        [c for c in range(num_classes) for _ in range(args.ipc)],\n","        dtype=torch.long,\n","        device=args.device,\n","    )\n","    \n","    if (args.init == 'real'):\n","        for c in range(num_classes):\n","            image_syn.data[c * args.ipc: (c + 1) * args.ipc] = get_images(c, args.ipc).detach().data\n","    data_save = []\n","    \n","    data_save.append([\n","        image_syn.detach().cpu().numpy(),\n","        label_syn.detach().cpu().numpy(),\n","    ])\n","    ''' Optimizer for the synthetic data '''\n","    optimizer_img = torch.optim.SGD([image_syn], lr=args.eta_S, momentum=0.5)\n","    optimizer_img.zero_grad()\n","    criterion = torch.nn.CrossEntropyLoss().to(args.device)\n","    \n","    total_steps = args.K * args.T * args.zeta_theta\n","    \n","    for k in range(args.K):\n","        \n","        ''' initialize the model '''\n","        net = get_network(args.model, channel, num_classes, im_size).to(args.device)\n","        net.train()\n","        net_parameters = list(net.parameters())\n","        \n","        ''' Optimizer for the model '''\n","        optimizer_net = torch.optim.SGD(\n","            net_parameters,\n","            lr=args.eta_theta\n","        )\n","        optimizer_net.zero_grad()\n","        loss_avg = 0\n","        \n","        for it in range (args.T):\n","            loss = torch.tensor(0.0, device=args.device)\n","            \n","            ''' Update synthetic data samples '''\n","            img_real_all = []\n","            lab_real_all = []\n","            img_syn_all = []\n","            lab_syn_all = []\n","            for c in range (num_classes):\n","                img_real = get_images(c, args.batch_real)\n","                lab_real = torch.ones(img_real.shape[0], dtype=torch.long, device=args.device) * c\n","                \n","                img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n","                lab_syn = torch.ones((args.ipc,), device=args.device, dtype=torch.long) * c\n","                \n","                img_real_all.append(img_real)\n","                lab_real_all.append(lab_real)\n","                img_syn_all.append(img_syn)\n","                lab_syn_all.append(lab_syn)\n","\n","\n","            img_real_all = torch.cat(img_real_all, dim=0)\n","            lab_real_all = torch.cat(lab_real_all, dim=0)\n","            img_syn_all = torch.cat(img_syn_all, dim=0)\n","            lab_syn_all = torch.cat(lab_syn_all, dim=0)\n","            \n","            output_real_all = net(img_real_all)\n","            loss_real_all = criterion(output_real_all, lab_real_all)\n","                \n","            gw_real = torch.autograd.grad(loss_real_all, net_parameters)\n","            gw_real = list((_.detach().clone() for _ in gw_real))\n","                \n","            output_syn_all = net(img_syn_all)\n","            loss_syn_all = criterion(output_syn_all, lab_syn_all)\n","            gw_syn = torch.autograd.grad(loss_syn_all, net_parameters, create_graph=True)\n","                \n","            loss += match_loss(gw_syn, gw_real, args)\n","            optimizer_img.zero_grad()\n","            loss.backward()\n","            optimizer_img.step()\n","            \n","            ''' Prepare the synthetic data for the next iteration '''\n","            img_syn_train = copy.deepcopy(image_syn.detach())\n","            label_syn_train = copy.deepcopy(label_syn.detach())\n","            dst_syn_train = TensorDataset(img_syn_train, label_syn_train)\n","            syn_train_loader = torch.utils.data.DataLoader(\n","                dst_syn_train,\n","                batch_size=args.batch_train,\n","                shuffle=True,\n","                num_workers=0,\n","            )\n","            \n","            ''' Train model using synthetic data '''\n","            for z in range(args.zeta_theta):\n","                loss_avg, acc_avg = epoch(\n","                    'train',\n","                    syn_train_loader,\n","                    net,\n","                    optimizer_net,\n","                    criterion,\n","                    args,\n","                    aug=False,\n","                )\n","    data_save.append([\n","        image_syn.detach().cpu().numpy(),\n","        label_syn.detach().cpu().numpy(),\n","    ])\n","    torch.save(data_save, 'data.pt')"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('dataset', 'MNIST'), ('data_path', '../datasets'), ('model', 'ConvNet'), ('K', 5), ('T', 10), ('eta_S', 0.1), ('eta_theta', 0.01), ('zeta_S', 1), ('zeta_theta', 50), ('batch_real', 256), ('batch_train', 256), ('ipc', 10), ('init', 'real'), ('device', device(type='cuda'))]\n"]}],"source":["sys.argv = [\n","    'dataDAM',\n","    '--dataset', 'MNIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNet',\n","    '--K', '5'\n","]\n","dataDAM()"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(100, 1, 28, 28) (100,)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 2000x500 with 10 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 2000x500 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["data = torch.load('data.pt')\n","\n","orig_img = data[0][0]\n","orig_lab = data[0][1]\n","\n","\n","syn_img = data[1][0]\n","syn_lab = data[1][1]\n","\n","print(orig_img.shape, orig_lab.shape)\n","\n","# plot 10 images from the original dataset\n","n = 10\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(1, n, figsize=(20, 5))\n","for i in range(n):\n","    ax[i].imshow(orig_img[i][0], cmap='gray')\n","    ax[i].set_title(orig_lab[i])\n","    ax[i].axis('off')\n","plt.show()\n","\n","\n","# plot 10 images from the synthetic dataset\n","fig, ax = plt.subplots(1, n, figsize=(20, 5))\n","for i in range(n):\n","    ax[i].imshow(syn_img[i][0], cmap='gray')\n","    ax[i].set_title(syn_lab[i])\n","    ax[i].axis('off')\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"DKk2luqUkJ2D"},"source":["### Google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26461,"status":"ok","timestamp":1729642615066,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"3LbCnOvD4YFP","outputId":"810a895d-3bd8-4f7c-f6db-9d17c3975c1d"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src\n","\n","!pip install thop"]},{"cell_type":"markdown","metadata":{"id":"_U9GlicdkL_C"},"source":["### Init"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2071,"status":"ok","timestamp":1729642664063,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"YYOVF7fc3Tup","outputId":"0cc5765c-012a-4d3b-b000-df90e533ed64"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from tqdm.auto import tqdm\n","from torchvision.utils import save_image\n","from thop import profile\n","#from utils import get_network, get_dataset, TensorDataset\n","#import dataDAM\n","import sys\n","\n","figures_dir = '../report/figures/'\n","output_dir = '../output/'\n","\n","print(f\"PyTorch Version: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    device_name = torch.cuda.get_device_name(0)\n","    properties = torch.cuda.get_device_properties(0)\n","    compute_capability = f\"{properties.major}.{properties.minor}\"\n","    total_memory = properties.total_memory / 1024**3\n","\n","    print(f\"CUDA Device: {device_name}\")\n","    print(f\"CUDA Compute Capability: {compute_capability}\")\n","    print(f\"Total Memory: {total_memory:.2f} GB\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available\")\n","\n","\n","def epoch_S(mode, dataloader, net, optimizer, criterion, device, progress_bar):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(device)\n","    criterion = criterion.to(device)\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(device)\n","        lab = datum[1].long().to(device)\n","        n_b = lab.shape[0]\n","\n","        output = net(img)\n","        loss = criterion(output, lab)\n","        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","\n","        loss_avg += loss.item()*n_b\n","        acc_avg += acc\n","        num_exp += n_b\n","\n","        if mode == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        progress_bar.set_postfix(loss=loss.item() / (i + 1))\n","        progress_bar.update(1)\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","# function to get FLOPS for a given model\n","def get_flops(model, dataloader, device):\n","    for inputs, _ in dataloader:\n","        # Get a single image from the batch\n","        # add an extra batch dimension to the image, as the models expect a batch of\n","        # images as input, not a single image.\n","        single_image = inputs[0].unsqueeze(0).to(device)\n","        break\n","    flops = profile(model, inputs=(single_image, ), verbose=False)\n","    return flops\n","\n","# function to get syn dataset from the output file\n","def get_syn_dataset (sym_name):\n","    syn_dataset_file = output_dir + sym_name\n","    results = torch.load(syn_dataset_file, weights_only=True)\n","    syn_imgs = results['data'][0][0]\n","    syn_labels = results['data'][0][1]\n","\n","    syn_dataset = TensorDataset(syn_imgs, syn_labels)\n","    channel = syn_imgs.shape[1]\n","    num_classes = syn_labels.max().item() + 1\n","    im_size = (syn_imgs.shape[2], syn_imgs.shape[3])\n","\n","    dataloader = torch.utils.data.DataLoader(syn_dataset, batch_size=32, shuffle=True)\n","    return syn_dataset, channel, num_classes, im_size, dataloader"]},{"cell_type":"markdown","metadata":{"id":"L6gd4UsA3Tur"},"source":["## MNIST"]},{"cell_type":"markdown","metadata":{"id":"2HJ5YyuIjglO"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"executionInfo":{"elapsed":7415,"status":"ok","timestamp":1729626781373,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"Qe-20Rwx3Tus","outputId":"1acf1b1d-6d54-4478-c7c9-e338425063a0"},"outputs":[],"source":["# load MNIST dataset from utils\n","(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MNIST_dataset,\n","    test_MNIST_dataset,\n","    test_MNIST_dataloader,\n","    train_MNIST_dataloader,\n",") = get_dataset(\"MNIST\", \"../datasets\")\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(\n","        train_MNIST_dataset.data[train_MNIST_dataset.targets == i][0], cmap=\"gray\"\n","    )\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"MNIST_dataset.png\", dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"uHHNti603Tus"},"source":["### ConvNet3\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"elapsed":459,"status":"error","timestamp":1729626770640,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"bWNt-yCp3Tut","outputId":"6c481d55-3d84-4fa3-fa58-9846b6e4900b"},"outputs":[],"source":["ConvNet3 = get_network('ConvNetD3', channel, num_classes, im_size)\n","print(ConvNet3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["28e39ec029124058b79477593f9cc705","48cdad7737884a5fa27c5a4c29935f44","7c297763bc854e97b38ed34be0c19ac9","4abeefd064fd4e589265227df362bb20","b4710ac36fb046279ba50f6be15460fb","c9724c51375c4023abfedda82e68f88c","09906df43e64476994045e5e787fca25","7560d99a22bf49f4ae369217dcccc965","00705a21a0074b92bf4beecf33058617","dbabcd7224d04feaa15b1a9ee61c3d82","f012406ec0d3480193deed026785f7c4","6ac49717585e4ce7908721b6afd463db","0391213c99ca456c816261759c84dc4e","3f4232df43a64b108841cb8651354842","b4b08a8544a247898c35f95040951dc9","53d0cb62fee94dbebcb91af437647019","4cb5d61a7fa24b66bd3ba41ccec4482f","9e577742214a4136aee778b8675b04f1","7cf09cdffb314c9db9bf8865b2f50cce","387ff275594b46cea795f41e9a6fd2f1","9d19481a1a86466aa005df60e5bfab68","706ee0493c6c420fa9ae8c715e606c65"]},"executionInfo":{"elapsed":45369,"status":"ok","timestamp":1729623324965,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"wPRIq5kc3Tut","outputId":"f5fa4d57-8273-460c-b681-3b2271cb5891"},"outputs":[],"source":["n_epochs = 2\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet3.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = train_MNIST_dataloader\n","testLoader = test_MNIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet3, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet3, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3_train_acc = train_acc\n","ConvNet3_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet3, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":1077,"status":"ok","timestamp":1729623330717,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"CHCNfCrU3Tuu","outputId":"bdf1124e-dc22-4c65-cc6b-09adf644628b"},"outputs":[],"source":["# plot the training and test accuracy\n","plt.plot(train_acc, label=\"Train\")\n","plt.plot(test_acc, label=\"Test\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yxEStI5E3Tuu"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28938,"status":"ok","timestamp":1729642760913,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"HYONQKws3Tuv","outputId":"d2c4d6f0-b186-4655-cf74-e15eb82b9377"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'real',\n","    '--model', 'ConvNetD3',\n","    '--dataset', 'MNIST',\n","    '--output_file', 'MNIST_real_res.pt',       # Output file\n","    '--ipc', '10',                              # Images/class\n","    '--lr_img', '1.0',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '1',                          #\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '500',                        # T\n","]\n","\n","MNIST_real_res = dataDAM()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"executionInfo":{"elapsed":1073,"status":"ok","timestamp":1729642766499,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"vS5bJbmS5EYQ","outputId":"12783c4e-f313-4373-8756-c46b5b1b2325"},"outputs":[],"source":["\n","orig_MNIST_real_res = output_dir + 'orig_MNIST_real_res.pt'\n","results = torch.load(orig_MNIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","tmp = syn_imgs.permute(0, 2, 3, 1).squeeze()\n","# display syn_imgs[0]\n","plt.imshow(syn_imgs[0].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.tight_layout()\n","plt.show()\n","\n","MNIST_real_res = output_dir + 'MNIST_real_res.pt'\n","results = torch.load(MNIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","tmp2 = syn_imgs.permute(0, 2, 3, 1).squeeze()\n","\n","plt.imshow(syn_imgs[0].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4478,"status":"ok","timestamp":1729627091389,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"mioHw6Nq3Tuv","outputId":"dd817b3e-eacc-4095-ef86-e8dce3f706f5"},"outputs":[],"source":["# plot the results\n","MNIST_real_res = output_dir + 'MNIST_real_res.pt'\n","results = torch.load(MNIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","\n","plt.suptitle(\"MNIST Synthetic Images\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455593,"status":"ok","timestamp":1729625286168,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"1w8t6bK-3Tuw","outputId":"f72e4e51-e195-4ec0-ddb6-f8dc26f4a13d"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'noise',\n","    '--model', 'ConvNetD3',\n","    '--dataset', 'MNIST',\n","    '--output_file', 'MNIST_noise_res.pt',      # Output file\n","    '--ipc', '10',                              # Images/class\n","    '--lr_img', '0.1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '2000',                        # T\n","]\n","\n","MNIST_noise_res = dataDAM()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4510,"status":"ok","timestamp":1729625442229,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"cY5ZKVkW3Tuw","outputId":"2aee5ee7-7923-42f9-cc90-28dcd563639a"},"outputs":[],"source":["# plot the results\n","MNIST_noise_res = output_dir + 'MNIST_noise_res.pt'\n","results = torch.load(MNIST_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"MNIST Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkHV7ZlUu8SQ"},"outputs":[],"source":["# plot the results\n","MNIST_noise_res = output_dir + 'MNIST_noise_res.pt'\n","results = torch.load(MNIST_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"MNIST Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"e2VBde713Tux"},"source":["### ConvNet3 with Synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWJxZsx13Tux"},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MNIST_real_res.pt')\n","ConvNet3Syn = get_network('ConvNetD3', channel, num_classes, im_size)\n","n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet3Syn.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MNIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet3Syn, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet3Syn, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3Syn_train_acc = train_acc\n","ConvNet3Syn_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet3Syn, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaRsLgJW3Tux"},"outputs":[],"source":["plt.figure(figsize=(7, 5))  # Set figure size\n","\n","# Plotting with different line styles for train (dashed) and test (solid) data\n","plt.plot(ConvNet3_train_acc, label=\"Original Dataset [Train]\", linestyle='--', color='b')  # Dashed line for training\n","plt.plot(ConvNet3_test_acc, label=\"Original Dataset [Test]\", linestyle='-', color='b')    # Solid line for testing\n","plt.plot(ConvNet3Syn_train_acc, label=\"Synthetic Dataset [Train]\", linestyle='--', color='r')  # Dashed line for training\n","plt.plot(ConvNet3Syn_test_acc, label=\"Synthetic Dataset [Test]\", linestyle='-', color='r')    # Solid line for testing\n","\n","# Add grid\n","plt.grid(False)\n","\n","# Ensure x-axis displays only whole numbers, every 5 epochs\n","plt.xticks(np.arange(0, n_epochs+1, 5))\n","\n","\n","plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n","plt.title(\"ConvNet3 Model Accuracy for MNIST Dataset\", fontsize=14, fontweight='bold')\n","\n","# Add legend with a better location\n","plt.legend(loc='best', fontsize=10)\n","\n","# Add a tight layout to minimize unnecessary whitespace\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_4GzQObCu_6A"},"source":["## CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"AEBtSwUDvNrR"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192226,"status":"ok","timestamp":1729626504834,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"cwJxJ6_yu-rb","outputId":"895fe4ac-629a-4045-9dfe-8f05efe310b7"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'real',\n","    '--model', 'ConvNetD3',\n","    #'--dataset', 'MNIST',\n","    '--output_file', 'CIFAR10_noise_res.pt',      # Output file\n","    '--ipc', '10',                              # Images/class\n","    '--lr_img', '1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '50',                        # T\n","]\n","\n","CIFAR10_noise_res = dataDAM()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6537,"status":"ok","timestamp":1729626534006,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"rNaL9aD6vTh9","outputId":"0bf5ec63-1909-40fd-846e-a8e3fc733182"},"outputs":[],"source":["# plot the results\n","CIFAR10_noise_res = output_dir + 'CIFAR10_noise_res.pt'\n","results = torch.load(CIFAR10_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"CIFAR10 Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"VgyQFdNY3Tuy"},"source":["## MHIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4trkVu1P3Tuy"},"outputs":[],"source":["(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MHIST_dataset,\n","    test_MHIST_dataset,\n","    test_MHIST_dataloader,\n","    train_MHIST_dataloader,\n",") = get_dataset(\"MHIST\", \"../datasets\")\n","\n","indices = [100, 1560]\n","\n","# plot 2 images from the dataset\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    image, label = train_MHIST_dataset[indices[i]]\n","    # Transpose the image from [3, 224, 224] to [224, 224, 3] for plotting\n","    image = image.permute(1, 2, 0)\n","    ax.imshow(image)\n","    if label == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"MHIST_dataset.png\", dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"pX3MQTu93Tuy"},"source":["### ConvNet7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhMZfqCN3Tuy"},"outputs":[],"source":["ConvNet7 = get_network('ConvNetD7', channel, num_classes, im_size)\n","print(ConvNet7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ekz5CVpL3Tuz"},"outputs":[],"source":["n_epochs = 5\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet7.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = train_MHIST_dataloader\n","testLoader = test_MHIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet7, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet7, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","flops, _ = get_flops(ConvNet7, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"markdown","metadata":{"id":"JPHNre3z3Tuz"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eg396AJR3Tuz"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'real',\n","    '--model', 'ConvNetD7',\n","    '--dataset', 'MHIST',\n","    '--output_file', 'MHIST_real_res.pt',       # Output file\n","    '--ipc', '50',                              # Images/class\n","    '--lr_img', '0.1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '10',                        # T\n","]\n","\n","MHIST_real_res = dataDAM.main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FivmMeFX3Tuz"},"outputs":[],"source":["MHIST_real_res = output_dir + 'MHIST_real_res.pt'\n","results = torch.load(MHIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_labels = results['data'][0][1]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# 50 images per class (HP, SSA)\n","# create a grid of 50 images per class\n","# create 1 image per class\n","\n","# label 0 is HP, label 1 is SSA\n","HP_imgs = syn_imgs[syn_labels == 0]\n","SSA_imgs = syn_imgs[syn_labels == 1]\n","# plot only HP images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(HP_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"HP Synthesized Images\", fontsize=16)\n","plt.tight_layout()\n","\n","# plot only SSA images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(SSA_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"SSA Synthesized Images\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YmBPU773Tu0"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'noise',\n","    '--model', 'ConvNetD7',\n","    '--dataset', 'MHIST',\n","    '--output_file', 'MHIST_noise_res.pt',      # Output file\n","    '--ipc', '50',                              # Images/class\n","    '--lr_img', '0.1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '10',                        # T\n","]\n","\n","MHIST_noise_res = dataDAM.main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RhsasRM3Tu0"},"outputs":[],"source":["MHIST_noise_res = output_dir + 'MHIST_noise_res.pt'\n","results = torch.load(MHIST_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_labels = results['data'][0][1]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# 50 images per class (HP, SSA)\n","# create a grid of 50 images per class\n","# create 1 image per class\n","\n","# label 0 is HP, label 1 is SSA\n","HP_imgs = syn_imgs[syn_labels == 0]\n","SSA_imgs = syn_imgs[syn_labels == 1]\n","# plot only HP images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(HP_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"HP Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()\n","\n","# plot only SSA images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(SSA_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"SSA Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"UZqV01fd3Tu0"},"source":["### ConvNet7 with Synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_zc9k5J3Tu0"},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MHIST_real_res.pt')\n","ConvNet7Syn = get_network('ConvNetD7', channel, num_classes, im_size)\n","n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet7Syn.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MNIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet7Syn, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet7Syn, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3Syn_train_acc = train_acc\n","ConvNet3Syn_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet7Syn, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00705a21a0074b92bf4beecf33058617":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0391213c99ca456c816261759c84dc4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb5d61a7fa24b66bd3ba41ccec4482f","placeholder":"​","style":"IPY_MODEL_9e577742214a4136aee778b8675b04f1","value":"Epoch 2: 100%"}},"09906df43e64476994045e5e787fca25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28e39ec029124058b79477593f9cc705":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48cdad7737884a5fa27c5a4c29935f44","IPY_MODEL_7c297763bc854e97b38ed34be0c19ac9","IPY_MODEL_4abeefd064fd4e589265227df362bb20"],"layout":"IPY_MODEL_b4710ac36fb046279ba50f6be15460fb"}},"387ff275594b46cea795f41e9a6fd2f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f4232df43a64b108841cb8651354842":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cf09cdffb314c9db9bf8865b2f50cce","max":1095,"min":0,"orientation":"horizontal","style":"IPY_MODEL_387ff275594b46cea795f41e9a6fd2f1","value":1095}},"48cdad7737884a5fa27c5a4c29935f44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9724c51375c4023abfedda82e68f88c","placeholder":"​","style":"IPY_MODEL_09906df43e64476994045e5e787fca25","value":"Epoch 1: 100%"}},"4abeefd064fd4e589265227df362bb20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbabcd7224d04feaa15b1a9ee61c3d82","placeholder":"​","style":"IPY_MODEL_f012406ec0d3480193deed026785f7c4","value":" 1095/1095 [00:50&lt;00:00, 58.23it/s, loss=0.00165]"}},"4cb5d61a7fa24b66bd3ba41ccec4482f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53d0cb62fee94dbebcb91af437647019":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ac49717585e4ce7908721b6afd463db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0391213c99ca456c816261759c84dc4e","IPY_MODEL_3f4232df43a64b108841cb8651354842","IPY_MODEL_b4b08a8544a247898c35f95040951dc9"],"layout":"IPY_MODEL_53d0cb62fee94dbebcb91af437647019"}},"706ee0493c6c420fa9ae8c715e606c65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7560d99a22bf49f4ae369217dcccc965":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c297763bc854e97b38ed34be0c19ac9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7560d99a22bf49f4ae369217dcccc965","max":1095,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00705a21a0074b92bf4beecf33058617","value":1095}},"7cf09cdffb314c9db9bf8865b2f50cce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d19481a1a86466aa005df60e5bfab68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e577742214a4136aee778b8675b04f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4710ac36fb046279ba50f6be15460fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b08a8544a247898c35f95040951dc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d19481a1a86466aa005df60e5bfab68","placeholder":"​","style":"IPY_MODEL_706ee0493c6c420fa9ae8c715e606c65","value":" 1095/1095 [00:36&lt;00:00, 56.95it/s, loss=0.002]"}},"c9724c51375c4023abfedda82e68f88c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbabcd7224d04feaa15b1a9ee61c3d82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f012406ec0d3480193deed026785f7c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
