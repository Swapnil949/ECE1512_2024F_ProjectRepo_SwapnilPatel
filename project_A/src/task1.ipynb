{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.1\n",
      "CUDA Device: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "CUDA Compute Capability: 8.9\n",
      "Total Memory: 15.70 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    properties = torch.cuda.get_device_properties(0)\n",
    "    compute_capability = f\"{properties.major}.{properties.minor}\"\n",
    "    total_memory = properties.total_memory / 1024**3\n",
    "\n",
    "    print(f\"CUDA Device: {device_name}\")\n",
    "    print(f\"CUDA Compute Capability: {compute_capability}\")\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load MNIST dataset from utils\n",
    "from utils import get_dataset\n",
    "\n",
    "(\n",
    "    channel,\n",
    "    im_size,\n",
    "    num_classes,\n",
    "    class_names,\n",
    "    mean,\n",
    "    std,\n",
    "    train_MNIST_dataset,\n",
    "    test_MNIST_dataset,\n",
    "    test_MNIST_dataloader,\n",
    "    train_MNIST_dataloader,\n",
    ") = get_dataset(\"MNIST\", \"../datasets\")\n",
    "\n",
    "# visualize 10 classes of MNIST (2 by 5)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(\n",
    "        train_MNIST_dataset.data[train_MNIST_dataset.targets == i][0], cmap=\"gray\"\n",
    "    )\n",
    "    ax.set_title(f\"{i}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
      "    (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils import get_network\n",
    "\n",
    "ConvNet3 = get_network('ConvNetD3', channel, num_classes, im_size)\n",
    "print(ConvNet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_epochs = 2\\nlr = 0.01\\ncriterion = nn.CrossEntropyLoss().to(device)\\noptimizer = torch.optim.SGD(ConvNet3.parameters(), lr=lr)\\n\\nfor epoch in range(n_epochs):\\n    ConvNet3.train()\\n    running_loss = 0.0\\n    progress_bar = tqdm(\\n        enumerate(train_MNIST_dataloader, 0),\\n        total=len(train_MNIST_dataloader),\\n        desc=f\"Epoch {epoch+1}\",\\n    )\\n\\n    total_iterations = len(train_MNIST_dataloader)\\n    for i, data in enumerate(train_MNIST_dataloader, 0):\\n        inputs, labels = data\\n        inputs, labels = inputs.to(device), labels.to(device)\\n\\n        optimizer.zero_grad()\\n\\n        outputs = ConvNet3(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        percentage_complete = (i + 1) / total_iterations * 100\\n        progress_bar.set_postfix(loss=running_loss / (i + 1))\\n        progress_bar.update(1)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_epochs = 2\n",
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(ConvNet3.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ConvNet3.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_MNIST_dataloader, 0),\n",
    "        total=len(train_MNIST_dataloader),\n",
    "        desc=f\"Epoch {epoch+1}\",\n",
    "    )\n",
    "\n",
    "    total_iterations = len(train_MNIST_dataloader)\n",
    "    for i, data in enumerate(train_MNIST_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        percentage_complete = (i + 1) / total_iterations * 100\n",
    "        progress_bar.set_postfix(loss=running_loss / (i + 1))\n",
    "        progress_bar.update(1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import MHISTDataset\n",
    "from torchvision import transforms\n",
    "to_tensor = transforms.ToTensor()\n",
    "data_path = '../mhist_dataset/images-split'  \n",
    "\n",
    "train = MHISTDataset.MHISTDataset(data_path, train=True, transform=to_tensor)\n",
    "test = MHISTDataset.MHISTDataset(data_path, train=False, transform=None)\n",
    "\n",
    "images_train = []\n",
    "for i in range(len(train)):\n",
    "    img, label = train[i]\n",
    "    images_train.append(img)\n",
    "\n",
    "print(images_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_it_pool:  [0, 10]\n",
      "\n",
      "================== Exp 0 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'dataset': 'MHIST', 'model': 'ConvNetD3', 'ipc': 10, 'eval_mode': 'SS', 'num_exp': 1, 'num_eval': 1, 'epoch_eval_train': 1, 'Iteration': 10, 'lr_img': 1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'data_path': '../datasets', 'save_path': '../output/', 'task_balance': 0.01, 'method': 'attention_matching', 'device': 'cuda', 'dsa': False}\n",
      "Evaluation model pool:  ['ConvNetD3']\n",
      "class c = 0: 1545 real images\n",
      "class c = 1: 630 real images\n",
      "real images channel 0, mean = 0.7378, std = 0.1972\n",
      "real images channel 1, mean = 0.6486, std = 0.2437\n",
      "real images channel 2, mean = 0.7752, std = 0.1703\n",
      "initialize synthetic data from random real images\n",
      "[2024-10-21 23:34:18] training begins\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNetD3, model_eval = ConvNetD3, iteration = 0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 15.70 GiB of which 95.19 MiB is free. Process 887648 has 15.60 GiB memory in use. Of the allocated memory 15.19 GiB is allocated by PyTorch, and 135.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_AttentionMatching.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--init\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--num_eval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mmain_AttentionMatching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src/main_AttentionMatching.py:159\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m net_eval \u001b[38;5;241m=\u001b[39m get_network(model_eval, channel, num_classes, im_size)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# get a random model\u001b[39;00m\n\u001b[1;32m    158\u001b[0m image_syn_eval, label_syn_eval \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(image_syn\u001b[38;5;241m.\u001b[39mdetach()), copy\u001b[38;5;241m.\u001b[39mdeepcopy(label_syn\u001b[38;5;241m.\u001b[39mdetach()) \u001b[38;5;66;03m# avoid any unaware modification\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m mini_net, acc_train, acc_test \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_synset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_syn_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_syn_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m accs\u001b[38;5;241m.\u001b[39mappend(acc_test)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acc_test \u001b[38;5;241m>\u001b[39m best_5[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src/utils.py:442\u001b[0m, in \u001b[0;36mevaluate_synset\u001b[0;34m(it_eval, net, images_train, labels_train, testloader, args)\u001b[0m\n\u001b[1;32m    439\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m    441\u001b[0m time_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 442\u001b[0m loss_test, acc_test \u001b[38;5;241m=\u001b[39m \u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Evaluate_\u001b[39m\u001b[38;5;132;01m%02d\u001b[39;00m\u001b[38;5;124m: epoch = \u001b[39m\u001b[38;5;132;01m%04d\u001b[39;00m\u001b[38;5;124m train time = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m s train loss = \u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m train acc = \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, test acc = \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (get_time(), it_eval, Epoch, \u001b[38;5;28mint\u001b[39m(time_train), loss_train, acc_train, acc_test))\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net, acc_train, acc_test\n",
      "File \u001b[0;32m~/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src/utils.py:401\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(mode, dataloader, net, optimizer, criterion, args, aug)\u001b[0m\n\u001b[1;32m    398\u001b[0m lab \u001b[38;5;241m=\u001b[39m datum[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    399\u001b[0m n_b \u001b[38;5;241m=\u001b[39m lab\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 401\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, lab)\n\u001b[1;32m    403\u001b[0m acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mequal(np\u001b[38;5;241m.\u001b[39margmax(output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), lab\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src/networks.py:45\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 15.70 GiB of which 95.19 MiB is free. Process 887648 has 15.60 GiB memory in use. Of the allocated memory 15.19 GiB is allocated by PyTorch, and 135.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import main_AttentionMatching\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    'main_AttentionMatching.py',\n",
    "    '--init', 'real',\n",
    "    '--ipc', '10',\n",
    "    '--model', 'ConvNetD3',\n",
    "    '--dataset', 'MHIST',\n",
    "    '--num_eval', '1',\n",
    "]\n",
    "\n",
    "file = main_AttentionMatching.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results images\n",
    "\n",
    "# plot the results\n",
    "results = torch.load(file)\n",
    "syn_imgs = results['data'][0][0]\n",
    "syn_imgs = torch.clamp(syn_imgs, 0, 1)\n",
    "\n",
    "# clip the images to [0, 1]\n",
    " \n",
    "# 10 images per class\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    'main_AttentionMatching.py',\n",
    "    '--init', 'noise',\n",
    "    '--ipc', '10',\n",
    "    '--model', 'ConvNetD3',\n",
    "    #'--dataset', 'MNIST',\n",
    "    '--num_eval', '1',\n",
    "]\n",
    "\n",
    "file = main_AttentionMatching.main()\n",
    "# plot the results\n",
    "results = torch.load(file)\n",
    "syn_imgs = results['data'][0][0]\n",
    "syn_imgs = torch.clamp(syn_imgs, 0, 1)\n",
    "\n",
    "print(syn_imgs.shape) # = (100, 1, 28, 28)\n",
    "# \n",
    "# 10 images per class\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
