{"cells":[{"cell_type":"markdown","metadata":{"id":"hajAmAsD3Tuo"},"source":["## Project Initialization"]},{"cell_type":"markdown","metadata":{"id":"mQC18PADhBRf"},"source":["### Google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26504,"status":"ok","timestamp":1729707105607,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"mVZfEC5ahBRf","outputId":"290f89a1-afce-457f-9711-1df40f289726"},"outputs":[],"source":["try:\n","    # Check if running in Google Colab\n","    if 'google.colab' in str(get_ipython()):\n","        from google.colab import drive\n","        drive.mount('/content/drive')  # Mount Google Drive\n","        %cd /content/drive/MyDrive/Colab Notebooks/project_a/src\n","        !pip install thop\n","    else:\n","        print(\"Not running in Google Colab. Google Drive will not be mounted.\")\n","except NameError:\n","    print(\"Not running in an IPython environment.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25071,"status":"ok","timestamp":1729707130672,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"x6WTYgkKhBRh","outputId":"a3462a1b-7f14-4ae0-cf15-49e5547c254b"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from torchvision.utils import save_image\n","from thop import profile\n","import sys\n","from PIL import Image\n","\n","\n","figures_dir = '../report/figures/'\n","output_dir = '../output/'\n","\n","\n","print(f\"PyTorch Version: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    device_name = torch.cuda.get_device_name(0)\n","    properties = torch.cuda.get_device_properties(0)\n","    compute_capability = f\"{properties.major}.{properties.minor}\"\n","    total_memory = properties.total_memory / 1024**3\n","\n","    print(f\"CUDA Device: {device_name}\")\n","    print(f\"CUDA Compute Capability: {compute_capability}\")\n","    print(f\"Total Memory: {total_memory:.2f} GB\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x1k_-2JDjdVP"},"source":["### MHISTDataet.py"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1729707130672,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"uhhL6chvjaoW"},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","from PIL import Image\n","from torch.utils.data import Dataset\n","\n","class MHISTDataset(Dataset):\n","    def __init__(self, images_dir, annotations_file, train=True, transform=None):\n","        \"\"\"\n","        Args:\n","            images_dir (str): Directory containing all images.\n","            annotations_file (str): Path to the CSV file with annotations.\n","            train (bool): Flag indicating whether to load the training or test partition.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.images_dir = Path(images_dir)\n","        self.annotations = pd.read_csv(annotations_file)\n","        self.transform = transform\n","\n","        # Filter annotations based on the partition\n","        partition = 'train' if train else 'test'\n","        self.annotations = self.annotations[self.annotations['Partition'] == partition]\n","\n","        # Map folder names to class labels\n","        self.label_map = {\"HP\": 0, \"SSA\": 1}\n","\n","        # Gather image file paths and their corresponding labels\n","        self.image_paths = self.annotations['Image Name'].apply(lambda x: self.images_dir / x).tolist()\n","        self.labels = self.annotations['Majority Vote Label'].map(self.label_map).tolist()\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Get the image path and corresponding label\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path)\n","\n","        label = self.labels[idx]\n","        # Map label to class name, if 0 then HP, if 1 then SSA\n","        name = \"HP\" if label == 0 else \"SSA\"\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"B0rkzMf_jtH9"},"source":["### Networks.py"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1729707130672,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"s2hOogVujv9I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# Acknowledgement to\n","# https://github.com/kuangliu/pytorch-cifar,\n","# https://github.com/BIGBALLON/CIFAR-ZOO,\n","\n","\"\"\"\n","https://github.com/DataDistillation/attention_matching/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","\n","''' Swish activation '''\n","class Swish(nn.Module): # Swish(x) = x∗σ(x)\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, input):\n","        return input * torch.sigmoid(input)\n","\n","\n","''' MLP '''\n","class MLP(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(MLP, self).__init__()\n","        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n","        self.fc_2 = nn.Linear(128, 128)\n","        self.fc_3 = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        out = x.view(x.size(0), -1)\n","        out = F.relu(self.fc_1(out))\n","        out = F.relu(self.fc_2(out))\n","        out = self.fc_3(out)\n","        return out\n","\n","\n","\n","''' ConvNet '''\n","class ConvNet(nn.Module):\n","    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n","        super(ConvNet, self).__init__()\n","\n","        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n","        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n","        self.classifier = nn.Linear(num_feat, num_classes)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","    def _get_activation(self, net_act):\n","        if net_act == 'sigmoid':\n","            return nn.Sigmoid()\n","        elif net_act == 'relu':\n","            return nn.ReLU(inplace=True)\n","        elif net_act == 'leakyrelu':\n","            return nn.LeakyReLU(negative_slope=0.01)\n","        elif net_act == 'swish':\n","            return Swish()\n","        else:\n","            exit('unknown activation function: %s'%net_act)\n","\n","    def _get_pooling(self, net_pooling):\n","        if net_pooling == 'maxpooling':\n","            return nn.MaxPool2d(kernel_size=2, stride=2)\n","        elif net_pooling == 'avgpooling':\n","            return nn.AvgPool2d(kernel_size=2, stride=2)\n","        elif net_pooling == 'none':\n","            return None\n","        else:\n","            exit('unknown net_pooling: %s'%net_pooling)\n","\n","    def _get_normlayer(self, net_norm, shape_feat):\n","        # shape_feat = (c*h*w)\n","        if net_norm == 'batchnorm':\n","            return nn.BatchNorm2d(shape_feat[0], affine=True)\n","        elif net_norm == 'layernorm':\n","            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n","        elif net_norm == 'instancenorm':\n","            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n","        elif net_norm == 'groupnorm':\n","            return nn.GroupNorm(4, shape_feat[0], affine=True)\n","        elif net_norm == 'none':\n","            return None\n","        else:\n","            exit('unknown net_norm: %s'%net_norm)\n","\n","    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n","        layers = []\n","        in_channels = channel\n","        if im_size[0] == 28:\n","            im_size = (32, 32)\n","        shape_feat = [in_channels, im_size[0], im_size[1]]\n","        for d in range(net_depth):\n","            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n","            shape_feat[0] = net_width\n","            if net_norm != 'none':\n","                layers += [self._get_normlayer(net_norm, shape_feat)]\n","            layers += [self._get_activation(net_act)]\n","            in_channels = net_width\n","            if net_pooling != 'none':\n","                layers += [self._get_pooling(net_pooling)]\n","                shape_feat[1] //= 2\n","                shape_feat[2] //= 2\n","\n","        return nn.Sequential(*layers), shape_feat\n","\n","\n","\n","''' LeNet '''\n","class LeNet(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(LeNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(6, 16, kernel_size=5),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc_2 = nn.Linear(120, 84)\n","        self.fc_3 = nn.Linear(84, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc_1(x))\n","        x = F.relu(self.fc_2(x))\n","        x = self.fc_3(x)\n","        return x\n","\n","\n","\n","''' AlexNet '''\n","class AlexNet(nn.Module):\n","    def __init__(self, channel, num_classes, im_size):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","\n","        #self.fc = nn.Linear(192 * 4 * 4, num_classes)\n","        # Calculate the flattened size for the fully connected layer\n","        conv_output_size = self._get_conv_output_size(im_size, channel)\n","        \n","        # Fully connected layer\n","        self.fc = nn.Linear(conv_output_size, num_classes)  # Fully connected layer\n","    \n","    def _get_conv_output_size(self, im_size, channel):\n","        # Helper function to calculate the output size of the convolutional layers\n","        x = torch.rand(1, channel, im_size[0], im_size[1])  # Random input tensor with the given image size and channel\n","        x = self.features(x)\n","        return x.view(1, -1).size(1)\n","    \n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","\n","''' AlexNetBN '''\n","class AlexNetBN(nn.Module):\n","    def __init__(self, channel, num_classes, im_size):\n","        super(AlexNetBN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n","        \n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","\n","''' VGG '''\n","cfg_vgg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name, channel, num_classes, norm='instancenorm'):\n","        super(VGG, self).__init__()\n","        self.channel = channel\n","        self.features = self._make_layers(cfg_vgg[vgg_name], norm)\n","        self.classifier = nn.Linear(512 if vgg_name != 'VGGS' else 128, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def _make_layers(self, cfg, norm):\n","        layers = []\n","        in_channels = self.channel\n","        for ic, x in enumerate(cfg):\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=3 if self.channel==1 and ic==0 else 1),\n","                           nn.GroupNorm(x, x, affine=True) if norm=='instancenorm' else nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","\n","def VGG11(channel, num_classes):\n","    return VGG('VGG11', channel, num_classes)\n","def VGG11BN(channel, num_classes):\n","    return VGG('VGG11', channel, num_classes, norm='batchnorm')\n","def VGG13(channel, num_classes):\n","    return VGG('VGG13', channel, num_classes)\n","def VGG16(channel, num_classes):\n","    return VGG('VGG16', channel, num_classes)\n","def VGG19(channel, num_classes):\n","    return VGG('VGG19', channel, num_classes)\n","\n","\n","''' ResNet_AP '''\n","# The conv(stride=2) is replaced by conv(stride=1) + avgpool(kernel_size=2, stride=2)\n","\n","class BasicBlock_AP(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(BasicBlock_AP, self).__init__()\n","        self.norm = norm\n","        self.stride = stride\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n","                nn.AvgPool2d(kernel_size=2, stride=2), # modification\n","                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        if self.stride != 1: # modification\n","            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck_AP(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(Bottleneck_AP, self).__init__()\n","        self.norm = norm\n","        self.stride = stride\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n","                nn.AvgPool2d(kernel_size=2, stride=2),  # modification\n","                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        if self.stride != 1: # modification\n","            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet_AP(nn.Module):\n","    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n","        super(ResNet_AP, self).__init__()\n","        self.in_planes = 64\n","        self.norm = norm\n","\n","        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.classifier = nn.Linear(512 * block.expansion * 3 * 3 if channel==1 else 512 * block.expansion * 4 * 4, num_classes)  # modification\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride, self.norm))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","def ResNet18BN_AP(channel, num_classes):\n","    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n","\n","def ResNet18_AP(channel, num_classes):\n","    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes)\n","\n","\n","''' ResNet '''\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(BasicBlock, self).__init__()\n","        self.norm = norm\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(Bottleneck, self).__init__()\n","        self.norm = norm\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.norm = norm\n","\n","        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.classifier = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride, self.norm))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","\n","def ResNet18BN(channel, num_classes):\n","    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n","\n","def ResNet18(channel, num_classes):\n","    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes)\n","\n","def ResNet34(channel, num_classes):\n","    return ResNet(BasicBlock, [3,4,6,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet50(channel, num_classes):\n","    return ResNet(Bottleneck, [3,4,6,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet101(channel, num_classes):\n","    return ResNet(Bottleneck, [3,4,23,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet152(channel, num_classes):\n","    return ResNet(Bottleneck, [3,8,36,3], channel=channel, num_classes=num_classes)"]},{"cell_type":"markdown","metadata":{"id":"2FQSh2D_jiC-"},"source":["### Utils.py"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1729707130673,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"eKjR99HKjj46"},"outputs":[],"source":["import time\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torchvision import datasets, transforms\n","from scipy.ndimage import rotate as scipyrotate\n","import sys\n","#from networks import MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n","#import MHISTDataset\n","\n","\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","def get_attention(feature_set, param=0, exp=4, norm='l2'):\n","    if param==0:\n","        attention_map = torch.sum(torch.abs(feature_set), dim=1)\n","\n","    elif param ==1:\n","        attention_map =  torch.sum(torch.abs(feature_set)**exp, dim=1)\n","\n","    elif param == 2:\n","        attention_map =  torch.max(torch.abs(feature_set)**exp, dim=1)\n","\n","    if norm == 'l2':\n","        # Dimension: [B x (H*W)] -- Vectorized\n","        vectorized_attention_map =  attention_map.view(feature_set.size(0), -1)\n","        normalized_attention_maps = F.normalize(vectorized_attention_map, p=2.0)\n","\n","    elif norm == 'fro':\n","        # Dimension: [B x H x W] -- Un-Vectorized\n","        un_vectorized_attention_map =  attention_map\n","        # Dimension: [B]\n","        fro_norm = torch.sum(torch.sum(torch.abs(attention_map)**2, dim=1), dim=1)\n","        # Dimension: [B x H x W] -- Un-Vectorized)\n","        normalized_attention_maps = un_vectorized_attention_map / fro_norm.unsqueeze(dim=-1).unsqueeze(dim=-1)\n","    elif norm == 'l1':\n","        # Dimension: [B x (H*W)] -- Vectorized\n","        vectorized_attention_map =  attention_map.view(feature_set.size(0), -1)\n","        normalized_attention_maps = F.normalize(vectorized_attention_map, p=1.0)\n","\n","    elif norm =='none':\n","        normalized_attention_maps = attention_map\n","\n","    elif norm == 'none-vectorized':\n","        normalized_attention_maps =  attention_map.view(feature_set.size(0), -1)\n","\n","    return normalized_attention_maps\n","\n","def get_dataset(dataset, data_path):\n","    if dataset == 'MNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.1307]\n","        std = [0.3081]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","\n","    elif dataset == 'FashionMNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.2861]\n","        std = [0.3530]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'SVHN':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4377, 0.4438, 0.4728]\n","        std = [0.1980, 0.2010, 0.1970]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.SVHN(data_path, split='train', download=True, transform=transform)  # no augmentation\n","        dst_test = datasets.SVHN(data_path, split='test', download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","\n","    elif dataset == 'CIFAR10':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4914, 0.4822, 0.4465]\n","        std = [0.2023, 0.1994, 0.2010]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'CIFAR100':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 100\n","        mean = [0.5071, 0.4866, 0.4409]\n","        std = [0.2673, 0.2564, 0.2762]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'TinyImageNet':\n","        channel = 3\n","        im_size = (64, 64)\n","        num_classes = 200\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","        data = torch.load(os.path.join(data_path, 'tinyimagenet.pt'), map_location='cpu')\n","\n","        class_names = data['classes']\n","\n","        images_train = data['images_train']\n","        labels_train = data['labels_train']\n","        images_train = images_train.detach().float() / 255.0\n","        labels_train = labels_train.detach()\n","        for c in range(channel):\n","            images_train[:,c] = (images_train[:,c] - mean[c])/std[c]\n","        dst_train = TensorDataset(images_train, labels_train)  # no augmentation\n","\n","        images_val = data['images_val']\n","        labels_val = data['labels_val']\n","        images_val = images_val.detach().float() / 255.0\n","        labels_val = labels_val.detach()\n","\n","        for c in range(channel):\n","            images_val[:, c] = (images_val[:, c] - mean[c]) / std[c]\n","\n","        dst_test = TensorDataset(images_val, labels_val)  # no augmentation\n","\n","    elif dataset == 'MHIST':\n","        channel = 3\n","        im_size = (224, 224)\n","        num_classes = 2\n","        class_names = ['HP', 'SSA']\n","        images_dir = '../mhist_dataset/images/'\n","        annotation_file = '../mhist_dataset/annotations.csv'\n","        to_tensor = transforms.ToTensor()\n","\n","        dst_train = MHISTDataset(images_dir, annotation_file, train=True, transform=to_tensor)\n","        dst_test = MHISTDataset(images_dir, annotation_file, train=False, transform=to_tensor)\n","\n","        # calculate mean and std\n","        mean = [0.5, 0.5, 0.5]  # Adjust these values based on dataset statistics\n","        std = [0.1, 0.1, 0.1]  # Adjust these values based on dataset statistics\n","\n","    else:\n","        exit('unknown dataset: %s'%dataset)\n","\n","\n","    testloader = torch.utils.data.DataLoader(dst_test, batch_size=64, shuffle=False, num_workers=0)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=64, shuffle=True, num_workers=0)\n","    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, trainloader\n","\n","\n","\n","class TensorDataset(Dataset):\n","    def __init__(self, images, labels): # images: n x c x h x w tensor\n","        self.images = images.detach().float()\n","        self.labels = labels.detach()\n","\n","    def __getitem__(self, index):\n","        return self.images[index], self.labels[index]\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","\n","\n","def get_default_convnet_setting():\n","    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n","    return net_width, net_depth, net_act, net_norm, net_pooling\n","\n","\n","\n","def get_network(model, channel, num_classes, im_size=(32, 32)):\n","    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n","    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n","\n","    if model == 'MLP':\n","        net = MLP(channel=channel, num_classes=num_classes)\n","    elif model == 'ConvNet':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'LeNet':\n","        net = LeNet(channel=channel, num_classes=num_classes)\n","    elif model == 'AlexNet':\n","        net = AlexNet(channel=channel, num_classes=num_classes, im_size=im_size)\n","    elif model == 'AlexNetBN':\n","        net = AlexNetBN(channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11':\n","        net = VGG11( channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11BN':\n","        net = VGG11BN(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18':\n","        net = ResNet18(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN_AP':\n","        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN':\n","        net = ResNet18BN(channel=channel, num_classes=num_classes)\n","\n","    elif model == 'ConvNetD1':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD2':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD3':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD4':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD7':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=7, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW32':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW64':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW128':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW256':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetAS':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetAR':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetAL':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetASwish':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetASwishBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetNN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetLN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetIN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetGN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetNP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none', im_size=im_size)\n","    elif model == 'ConvNetMP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling', im_size=im_size)\n","    elif model == 'ConvNetAP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling', im_size=im_size)\n","\n","    else:\n","        net = None\n","        exit('unknown model: %s'%model)\n","\n","    gpu_num = torch.cuda.device_count()\n","    if gpu_num>0:\n","        device = 'cuda'\n","        if gpu_num>1:\n","            net = nn.DataParallel(net)\n","    else:\n","        device = 'cpu'\n","    net = net.to(device)\n","\n","    return net\n","\n","\n","\n","def get_time():\n","    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n","\n","\n","\n","def distance_wb(gwr, gws):\n","    shape = gwr.shape\n","    if len(shape) == 4: # conv, out*in*h*w\n","        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n","        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n","    elif len(shape) == 3:  # layernorm, C*h*w\n","        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n","        gws = gws.reshape(shape[0], shape[1] * shape[2])\n","    elif len(shape) == 2: # linear, out*in\n","        tmp = 'do nothing'\n","    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n","        gwr = gwr.reshape(1, shape[0])\n","        gws = gws.reshape(1, shape[0])\n","        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n","\n","    dis_weight = torch.sum(1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001))\n","    dis = dis_weight\n","    return dis\n","\n","\n","\n","def match_loss(gw_syn, gw_real, args):\n","    dis = torch.tensor(0.0).to(args.device)\n","\n","    if args.dis_metric == 'ours':\n","        for ig in range(len(gw_real)):\n","            gwr = gw_real[ig]\n","            gws = gw_syn[ig]\n","            dis += distance_wb(gwr, gws)\n","\n","    elif args.dis_metric == 'mse':\n","        gw_real_vec = []\n","        gw_syn_vec = []\n","        for ig in range(len(gw_real)):\n","            gw_real_vec.append(gw_real[ig].reshape((-1)))\n","            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n","        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n","        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n","        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n","\n","    elif args.dis_metric == 'cos':\n","        gw_real_vec = []\n","        gw_syn_vec = []\n","        for ig in range(len(gw_real)):\n","            gw_real_vec.append(gw_real[ig].reshape((-1)))\n","            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n","        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n","        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n","        dis = 1 - torch.sum(gw_real_vec * gw_syn_vec, dim=-1) / (torch.norm(gw_real_vec, dim=-1) * torch.norm(gw_syn_vec, dim=-1) + 0.000001)\n","\n","    else:\n","        exit('unknown distance function: %s'%args.dis_metric)\n","\n","    return dis\n","\n","\n","\n","def get_loops(ipc):\n","    # Get the two hyper-parameters of outer-loop and inner-loop.\n","    # The following values are empirically good.\n","    if ipc == 1:\n","        outer_loop, inner_loop = 1, 1\n","    elif ipc == 10:\n","        outer_loop, inner_loop = 10, 50\n","    elif ipc == 20:\n","        outer_loop, inner_loop = 20, 25\n","    elif ipc == 30:\n","        outer_loop, inner_loop = 30, 20\n","    elif ipc == 40:\n","        outer_loop, inner_loop = 40, 15\n","    elif ipc == 50:\n","        outer_loop, inner_loop = 50, 10\n","    else:\n","        outer_loop, inner_loop = 0, 0\n","        exit('loop hyper-parameters are not defined for %d ipc'%ipc)\n","    return outer_loop, inner_loop\n","\n","\n","\n","def epoch(mode, dataloader, net, optimizer, criterion, args, aug):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(args.device)\n","    criterion = criterion.to(args.device)\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(args.device)\n","        if aug:\n","            if args.dsa:\n","                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n","            else:\n","                pass\n","                #img = augment(img, args.dc_aug_param, device=args.device)\n","        lab = datum[1].long().to(args.device)\n","        n_b = lab.shape[0]\n","\n","        output = net(img)\n","        loss = criterion(output, lab)\n","        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","\n","        loss_avg += loss.item()*n_b\n","        acc_avg += acc\n","        num_exp += n_b\n","\n","        if mode == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","\n","\n","def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args):\n","    net = net.to(args.device)\n","    images_train = images_train.to(args.device)\n","    labels_train = labels_train.to(args.device)\n","    lr = float(args.lr_net)\n","    Epoch = int(args.epoch_eval_train)\n","    lr_schedule = [Epoch//2+1]\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","    criterion = nn.CrossEntropyLoss().to(args.device)\n","\n","    dst_train = TensorDataset(images_train, labels_train)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","    start = time.time()\n","    for ep in range(Epoch+1):\n","        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug = True)\n","        if ep in lr_schedule:\n","            lr *= 0.1\n","            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","\n","    time_train = time.time() - start\n","    loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, aug = False)\n","    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n","\n","    return net, acc_train, acc_test\n","\n","\n","\n","def augment(images, dc_aug_param, device):\n","    # This can be sped up in the future.\n","\n","    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n","        scale = dc_aug_param['scale']\n","        crop = dc_aug_param['crop']\n","        rotate = dc_aug_param['rotate']\n","        noise = dc_aug_param['noise']\n","        strategy = dc_aug_param['strategy']\n","\n","        shape = images.shape\n","        mean = []\n","        for c in range(shape[1]):\n","            mean.append(float(torch.mean(images[:,c])))\n","\n","        def cropfun(i):\n","            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n","            for c in range(shape[1]):\n","                im_[c] = mean[c]\n","            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n","            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n","            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n","\n","        def scalefun(i):\n","            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n","            mhw = max(h, w, shape[2], shape[3])\n","            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n","            r = int((mhw - h) / 2)\n","            c = int((mhw - w) / 2)\n","            im_[:, r:r + h, c:c + w] = tmp\n","            r = int((mhw - shape[2]) / 2)\n","            c = int((mhw - shape[3]) / 2)\n","            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n","\n","        def rotatefun(i):\n","            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n","            r = int((im_.shape[-2] - shape[-2]) / 2)\n","            c = int((im_.shape[-1] - shape[-1]) / 2)\n","            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n","\n","        def noisefun(i):\n","            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n","\n","\n","        augs = strategy.split('_')\n","\n","        for i in range(shape[0]):\n","            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n","            if choice == 'crop':\n","                cropfun(i)\n","            elif choice == 'scale':\n","                scalefun(i)\n","            elif choice == 'rotate':\n","                rotatefun(i)\n","            elif choice == 'noise':\n","                noisefun(i)\n","\n","    return images\n","\n","\n","\n","def get_daparam(dataset, model, model_eval, ipc):\n","    # We find that augmentation doesn't always benefit the performance.\n","    # So we do augmentation for some of the settings.\n","\n","    dc_aug_param = dict()\n","    dc_aug_param['crop'] = 4\n","    dc_aug_param['scale'] = 0.2\n","    dc_aug_param['rotate'] = 45\n","    dc_aug_param['noise'] = 0.001\n","    dc_aug_param['strategy'] = 'none'\n","\n","    if dataset == 'MNIST':\n","        dc_aug_param['strategy'] = 'crop_scale_rotate'\n","\n","    if model_eval in ['ConvNetBN']: # Data augmentation makes model training with Batch Norm layer easier.\n","        dc_aug_param['strategy'] = 'crop_noise'\n","\n","    return dc_aug_param\n","\n","\n","def get_eval_pool(eval_mode, model, model_eval):\n","    if eval_mode == 'M': # multiple architectures\n","        model_eval_pool = ['MLP', 'ConvNet', 'LeNet', 'AlexNet', 'VGG11', 'ResNet18']\n","    elif eval_mode == 'B':  # multiple architectures with BatchNorm for DM experiments\n","        model_eval_pool = ['ConvNetBN', 'ConvNetASwishBN', 'AlexNetBN', 'VGG11BN', 'ResNet18BN']\n","    elif eval_mode == 'W': # ablation study on network width\n","        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n","    elif eval_mode == 'D': # ablation study on network depth\n","        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n","    elif eval_mode == 'A': # ablation study on network activation function\n","        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL', 'ConvNetASwish']\n","    elif eval_mode == 'P': # ablation study on network pooling layer\n","        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n","    elif eval_mode == 'N': # ablation study on network normalization layer\n","        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n","    elif eval_mode == 'S': # itself\n","        if 'BN' in model:\n","            print('Attention: Here I will replace BN with IN in evaluation, as the synthetic set is too small to measure BN hyper-parameters.')\n","        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n","    elif eval_mode == 'SS':  # itself\n","        model_eval_pool = [model]\n","    else:\n","        model_eval_pool = [model_eval]\n","    return model_eval_pool\n","\n","\n","class ParamDiffAug():\n","    def __init__(self):\n","        self.aug_mode = 'S' #'multiple or single'\n","        self.prob_flip = 0.5\n","        self.ratio_scale = 1.2\n","        self.ratio_rotate = 15.0\n","        self.ratio_crop_pad = 0.125\n","        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n","        self.brightness = 1.0\n","        self.saturation = 2.0\n","        self.contrast = 0.5\n","\n","\n","def set_seed_DiffAug(param):\n","    if param.latestseed == -1:\n","        return\n","    else:\n","        torch.random.manual_seed(param.latestseed)\n","        param.latestseed += 1\n","\n","\n","def DiffAugment(x, strategy='', seed = -1, param = None):\n","    if strategy == 'None' or strategy == 'none' or strategy == '':\n","        return x\n","\n","    if seed == -1:\n","        param.Siamese = False\n","    else:\n","        param.Siamese = True\n","\n","    param.latestseed = seed\n","\n","    if strategy:\n","        if param.aug_mode == 'M': # original\n","            for p in strategy.split('_'):\n","                for f in AUGMENT_FNS[p]:\n","                    x = f(x, param)\n","        elif param.aug_mode == 'S':\n","            pbties = strategy.split('_')\n","            set_seed_DiffAug(param)\n","            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n","            for f in AUGMENT_FNS[p]:\n","                x = f(x, param)\n","        else:\n","            exit('unknown augmentation mode: %s'%param.aug_mode)\n","        x = x.contiguous()\n","    return x\n","\n","\n","# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n","def rand_scale(x, param):\n","    # x>1, max scale\n","    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n","    ratio = param.ratio_scale\n","    set_seed_DiffAug(param)\n","    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    set_seed_DiffAug(param)\n","    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    theta = [[[sx[i], 0,  0],\n","            [0,  sy[i], 0],] for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.Siamese: # Siamese augmentation:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape).to(x.device)\n","    x = F.grid_sample(x, grid)\n","    return x\n","\n","\n","def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n","    ratio = param.ratio_rotate\n","    set_seed_DiffAug(param)\n","    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n","    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n","        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.Siamese: # Siamese augmentation:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape).to(x.device)\n","    x = F.grid_sample(x, grid)\n","    return x\n","\n","\n","def rand_flip(x, param):\n","    prob = param.prob_flip\n","    set_seed_DiffAug(param)\n","    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n","    if param.Siamese: # Siamese augmentation:\n","        randf[:] = randf[0]\n","    return torch.where(randf < prob, x.flip(3), x)\n","\n","\n","def rand_brightness(x, param):\n","    ratio = param.brightness\n","    set_seed_DiffAug(param)\n","    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        randb[:] = randb[0]\n","    x = x + (randb - 0.5)*ratio\n","    return x\n","\n","\n","def rand_saturation(x, param):\n","    ratio = param.saturation\n","    x_mean = x.mean(dim=1, keepdim=True)\n","    set_seed_DiffAug(param)\n","    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        rands[:] = rands[0]\n","    x = (x - x_mean) * (rands * ratio) + x_mean\n","    return x\n","\n","\n","def rand_contrast(x, param):\n","    ratio = param.contrast\n","    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n","    set_seed_DiffAug(param)\n","    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        randc[:] = randc[0]\n","    x = (x - x_mean) * (randc + ratio) + x_mean\n","    return x\n","\n","\n","def rand_crop(x, param):\n","    # The image is padded on its surrounding and then cropped.\n","    ratio = param.ratio_crop_pad\n","    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        translation_x[:] = translation_x[0]\n","        translation_y[:] = translation_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n","    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n","    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n","    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n","    return x\n","\n","\n","def rand_cutout(x, param):\n","    ratio = param.ratio_cutout\n","    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        offset_x[:] = offset_x[0]\n","        offset_y[:] = offset_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n","    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n","    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n","    mask[grid_batch, grid_x, grid_y] = 0\n","    x = x * mask.unsqueeze(1)\n","    return x\n","\n","\n","AUGMENT_FNS = {\n","    'color': [rand_brightness, rand_saturation, rand_contrast],\n","    'crop': [rand_crop],\n","    'cutout': [rand_cutout],\n","    'flip': [rand_flip],\n","    'scale': [rand_scale],\n","    'rotate': [rand_rotate],\n","}\n","\n","\n","def epoch_S(mode, dataloader, net, optimizer, criterion, device, progress_bar, start_time):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(device)\n","    criterion = criterion.to(device)\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(device)\n","        lab = datum[1].long().to(device)\n","        n_b = lab.shape[0]\n","\n","        output = net(img)\n","        loss = criterion(output, lab)\n","        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","\n","        loss_avg += loss.item()*n_b\n","        acc_avg += acc\n","        num_exp += n_b\n","\n","        if mode == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        progress_bar.set_postfix(loss=loss.item() / (i_batch + 1))\n","        progress_bar.update(1)\n","        progress_bar.set_description(\n","                    f\"Elapsed: {time.time() - start_time:.2f}s | \"\n","        )\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","# function to get FLOPS for a given model\n","def get_flops(model, dataloader, device):\n","    for inputs, _ in dataloader:\n","        # Get a single image from the batch\n","        # add an extra batch dimension to the image, as the models expect a batch of\n","        # images as input, not a single image.\n","        single_image = inputs[0].unsqueeze(0).to(device)\n","        break\n","    flops = profile(model, inputs=(single_image, ), verbose=False)\n","    return flops\n","\n","# function to get syn dataset from the output file\n","def get_syn_dataset (sym_name):\n","    syn_dataset_file = output_dir + sym_name\n","    results = torch.load(syn_dataset_file, weights_only=True)\n","    syn_imgs = results[0][0]\n","    syn_labels = results[0][1]\n","\n","    syn_dataset = TensorDataset(syn_imgs, syn_labels)\n","    channel = syn_imgs.shape[1]\n","    num_classes = syn_labels.max().item() + 1\n","    im_size = (syn_imgs.shape[2], syn_imgs.shape[3])\n","\n","    dataloader = torch.utils.data.DataLoader(syn_dataset, batch_size=32, shuffle=True)\n","    return syn_dataset, channel, num_classes, im_size, dataloader"]},{"cell_type":"markdown","metadata":{"id":"PbCoX2FdkFx5"},"source":["### Attention Matching Algorithm"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1729707130674,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"DuDxIsufkD5z"},"outputs":[],"source":["\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","\n","import os\n","import time\n","import copy\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from tqdm.auto import tqdm\n","\n","#from utils import get_dataset, get_network, get_eval_pool, evaluate_synset, get_time, DiffAugment, ParamDiffAug, get_attention\n","\n","def attention_matching():\n","\n","    parser = argparse.ArgumentParser(description='attention_matching')\n","    parser.add_argument('--dataset', default='MNIST', type=str, help='dataset name')\n","    parser.add_argument('--data_path', default='../datasets', type=str, help='data path')\n","    parser.add_argument('--model', default='ConvNet', type=str, help='model name')\n","    parser.add_argument('--K', default=100, type=int, help='Number of random weight initializations')\n","    parser.add_argument('--T', default=10, type=int, help='Number of iterations')\n","    parser.add_argument('--eta_S', default=0.1, type=float, help='learning rate for the condensed samples')\n","    parser.add_argument('--eta_theta', default=0.01, type=float, help='learning rate for the model')\n","    parser.add_argument('--zeta_S', default=1, type=int, help='Number of optimization steps for condensed samples')\n","    parser.add_argument('--zeta_theta', default=50, type=int, help='Number of optimization steps for the model')\n","    parser.add_argument('--batch_real', default=256, type=int, help='Batch size for real data')\n","    parser.add_argument('--batch_train', default=256, type=int, help='Batch size for training networks')\n","    parser.add_argument('--ipc', default=10, type=int, help='Images per class')\n","    parser.add_argument('--init', default='real', type=str, help='Initialization of synthetic dataset')\n","    parser.add_argument('--task_balance', type=float, default=0.01, help='balance attention with output')\n","    parser.add_argument('--output_file', type=str, default='none', help='output file name')\n","\n","    args = parser.parse_args()\n","    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    args.dis_metric = 'ours'\n","    args.save_path = output_dir\n","    # Create a list of tuples with argument names and their values\n","\n","    # Print arguments\n","    print('\\n======== Arguments =========\\n')\n","    print(\"Arguments: \", args.__dict__)\n","    print('\\n============================\\n')\n","\n","    (\n","        channel,\n","        im_size,\n","        num_classes,\n","        class_names,\n","        mean,\n","        std,\n","        train_dataset,\n","        test_dataset,\n","        test_datasetLoader,\n","        train_datasetLoader,\n","        ) = get_dataset(args.dataset, args.data_path)\n","\n","    ''' organize the real dataset '''\n","    print('Organizing the real dataset...')\n","    images_all = []\n","    labels_all = []\n","    indices_class = [[] for c in range(num_classes)]\n","    images_all = [torch.unsqueeze(train_dataset[i][0], dim=0) for i in range(len(train_dataset))]\n","    labels_all = [train_dataset[i][1] for i in range(len(train_dataset))]\n","    indices_class = [[] for c in range(num_classes)]\n","    for i, lab in enumerate(labels_all):\n","        indices_class[lab].append(i)\n","    images_all = torch.cat(images_all, dim=0).to(args.device)\n","    labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n","\n","    def get_images(c, n): # get random n images from class c\n","        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n","        return images_all[idx_shuffle]\n","\n","    def error(real, syn, err_type=\"MSE\"):\n","        if(err_type == \"MSE\"):\n","            err = torch.sum((torch.mean(real, dim=0) - torch.mean(syn, dim=0))**2)\n","        elif(err_type == \"MSE_B\"):\n","            err = torch.sum((torch.mean(real.reshape(num_classes, args.batch_real, -1), dim=1).cpu() - torch.mean(syn.cpu().reshape(num_classes, args.ipc, -1), dim=1))**2)\n","        return err\n","\n","    ''' Defining the Hook Function to collect Activations '''\n","    activations = {}\n","    def getActivation(name):\n","        def hook_func(m, inp, op):\n","            activations[name] = op.clone()\n","        return hook_func\n","\n","    ''' Defining the Refresh Function to store Activations and reset Collection '''\n","    def refreshActivations(activations):\n","        model_set_activations = [] # Jagged Tensor Creation\n","        for i in activations.keys():\n","            model_set_activations.append(activations[i])\n","        activations = {}\n","        return activations, model_set_activations\n","\n","    ''' Defining the Delete Hook Function to collect Remove Hooks '''\n","    def delete_hooks(hooks):\n","        for i in hooks:\n","            i.remove()\n","        return\n","\n","    def attach_hooks(net):\n","        hooks = []\n","        base = net.module if torch.cuda.device_count() > 1 else net\n","        for module in (base.features.named_modules()):\n","            if isinstance(module[1], nn.ReLU):\n","                # Hook the Ouptus of a ReLU Layer\n","                hooks.append(base.features[int(module[0])].register_forward_hook(getActivation('ReLU_'+str(len(hooks)))))\n","        return hooks\n","\n","\n","    ''' initialize the synthetic data '''\n","    print('Initializing the synthetic data...')\n","    image_syn = torch.randn(\n","        size=(num_classes * args.ipc, channel, im_size[0], im_size[1]),\n","        dtype=torch.float,\n","        requires_grad=True,\n","        device=args.device,\n","    )\n","    label_syn = torch.tensor(\n","        [c for c in range(num_classes) for _ in range(args.ipc)],\n","        dtype=torch.long,\n","        device=args.device,\n","    )\n","\n","    if (args.init == 'real'):\n","        for c in range(num_classes):\n","            image_syn.data[c * args.ipc: (c + 1) * args.ipc] = get_images(c, args.ipc).detach().data\n","    data_save = []\n","\n","    data_save.append([\n","        image_syn.detach().cpu(),\n","        label_syn.detach().cpu(),\n","    ])\n","    ''' Optimizer for the synthetic data '''\n","    optimizer_img = torch.optim.SGD([image_syn], lr=args.eta_S, momentum=0.5)\n","    optimizer_img.zero_grad()\n","    criterion = torch.nn.CrossEntropyLoss().to(args.device)\n","\n","    total_steps = args.K * args.T * args.zeta_theta\n","    start_time = time.time()\n","    print(\"Starting the training process...\")\n","\n","    print(\"\\n\", flush=True)\n","    progress_bar = tqdm(\n","        total=total_steps,\n","        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","        unit = 'step',\n","    )\n","\n","    for k in range(args.K):\n","\n","        ''' initialize the model '''\n","        net = get_network(args.model, channel, num_classes, im_size).to(args.device)\n","        net.train()\n","        net_parameters = list(net.parameters())\n","\n","        ''' Optimizer for the model '''\n","        optimizer_net = torch.optim.SGD(\n","            net_parameters,\n","            lr=args.eta_theta\n","        )\n","        optimizer_net.zero_grad()\n","        loss_avg = 0\n","\n","        for it in range (args.T):\n","            loss = torch.tensor(0.0, device=args.device)\n","\n","            ''' Update synthetic data samples '''\n","            img_real_all = []\n","            lab_real_all = []\n","            img_syn_all = []\n","            lab_syn_all = []\n","            for c in range (num_classes):\n","                img_real = get_images(c, args.batch_real)\n","                lab_real = torch.ones(img_real.shape[0], dtype=torch.long, device=args.device) * c\n","\n","                img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n","                lab_syn = torch.ones((args.ipc,), device=args.device, dtype=torch.long) * c\n","\n","                img_real_all.append(img_real)\n","                lab_real_all.append(lab_real)\n","                img_syn_all.append(img_syn)\n","                lab_syn_all.append(lab_syn)\n","\n","\n","            img_real_all = torch.cat(img_real_all, dim=0)\n","            lab_real_all = torch.cat(lab_real_all, dim=0)\n","            img_syn_all = torch.cat(img_syn_all, dim=0)\n","            lab_syn_all = torch.cat(lab_syn_all, dim=0)\n","\n","            hooks = attach_hooks(net)\n","            output_real = net(img_real_all)\n","            activations, orig_model_set_activations = refreshActivations(activations)\n","\n","            output_syn = net(img_syn_all)\n","            activations, syn_model_set_activations = refreshActivations(activations)\n","            delete_hooks(hooks)\n","\n","            net_len = len(orig_model_set_activations)\n","\n","            for layer in range(net_len):\n","                real_attention = get_attention(orig_model_set_activations[layer].detach(), param=1, exp=1, norm='l2')\n","                syn_attention = get_attention(syn_model_set_activations[layer], param=1, exp=1, norm='l2')\n","\n","                tl =  100*error(real_attention, syn_attention, err_type=\"MSE_B\")\n","                loss+=tl\n","\n","            output_loss =  100*args.task_balance * error(output_real, output_syn, err_type=\"MSE_B\")\n","            loss += output_loss\n","            optimizer_img.zero_grad()\n","            loss.backward()\n","            optimizer_img.step()\n","            torch.cuda.empty_cache()\n","\n","\n","            ''' Prepare the synthetic data for the next iteration '''\n","            img_syn_train = copy.deepcopy(image_syn.detach())\n","            label_syn_train = copy.deepcopy(label_syn.detach())\n","            dst_syn_train = TensorDataset(img_syn_train, label_syn_train)\n","            syn_train_loader = torch.utils.data.DataLoader(\n","                dst_syn_train,\n","                batch_size=args.batch_train,\n","                shuffle=True,\n","                num_workers=0,\n","            )\n","\n","            ''' Train model using synthetic data '''\n","            for z in range(args.zeta_theta):\n","                loss_avg, acc_avg = epoch(\n","                    'train',\n","                    syn_train_loader,\n","                    net,\n","                    optimizer_net,\n","                    criterion,\n","                    args,\n","                    aug=False,\n","                )\n","                progress_bar.update(1)\n","                progress_bar.set_description(\n","                    f\"Elapsed: {time.time() - start_time:.2f}s | \"\n","                )\n","    data_save.append([\n","        image_syn.detach().cpu(),\n","        label_syn.detach().cpu(),\n","    ])\n","    if args.output_file == 'none':\n","        save_file_path = os.path.join(args.save_path, 'res_%s_%s_%s_%dipc_.pt'%(args.method, args.dataset, args.model, args.ipc))\n","    else:\n","        save_file_path = os.path.join(args.save_path, args.output_file)\n","    torch.save(data_save, save_file_path)\n","    return save_file_path"]},{"cell_type":"markdown","metadata":{"id":"L6gd4UsA3Tur"},"source":["## MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"executionInfo":{"elapsed":9452,"status":"ok","timestamp":1729707140118,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"Qe-20Rwx3Tus","outputId":"f10b9102-ef72-4a4b-a8be-cfdcaacdbfea"},"outputs":[],"source":["# load MNIST dataset from utils\n","(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MNIST_dataset,\n","    test_MNIST_dataset,\n","    test_MNIST_dataloader,\n","    train_MNIST_dataloader,\n",") = get_dataset(\"MNIST\", \"../datasets\")\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(\n","        train_MNIST_dataset.data[train_MNIST_dataset.targets == i][0], cmap=\"gray\"\n","    )\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"MNIST_dataset.png\", dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"uHHNti603Tus"},"source":["### ConvNet3\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449,"status":"ok","timestamp":1729707140562,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"bWNt-yCp3Tut","outputId":"daa17282-eea8-4ef0-9565-a16b33668220"},"outputs":[],"source":["ConvNet3 = get_network('ConvNetD3', channel, num_classes, im_size)\n","print(ConvNet3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["9b9d7e8d1aae46819c74a7aad620dfce","0b2edab164974ef6be5dd4082056bb77","291438b4b4ad41ccbc27ad7409377a28","b470dd1875e24c419cd73dba1dade827","d277fe13863a4a098ba85d2d9c62054f","964cf247db3e43e48a18293b731f73a0","ad49a92464fa46fc8b289d6bd5b8c74f","4d5675c156ab478da20e7ebc451c53cd","3e590faafef041bfa7f38eeffb06be9b","007faccb68424bb0b8bf5ee149d53083","4d20dcb6de7843d0a947ec6013568213"]},"executionInfo":{"elapsed":709205,"status":"ok","timestamp":1729707849762,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"wPRIq5kc3Tut","outputId":"7880ef6c-79ef-452e-dada-99f876260a30"},"outputs":[],"source":["n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet3.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = train_MNIST_dataloader\n","testLoader = test_MNIST_dataloader\n","\n","progress_bar = tqdm(\n","    total=n_epochs*len(trainLoader) + n_epochs*len(testLoader),\n","    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","    unit = 'step',\n",")\n","start_time = time.time()\n","for ep in range (n_epochs):\n","    train_loss_avg, train_acc_avg = epoch_S(\n","        'train',\n","        trainLoader,\n","        ConvNet3,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","        \n","    test_loss_avg, test_acc_avg = epoch_S(\n","        'test',\n","        testLoader,\n","        ConvNet3,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3_train_acc = train_acc\n","ConvNet3_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet3, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))\n","\n","## Runtime      255.49 s\n","## FLOPS        48,429,056"]},{"cell_type":"markdown","metadata":{"id":"yxEStI5E3Tuu"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["5305717294404301a96716966f4a2608","86148bf00e29414c84c0aee1f3a64ef1","560a36a23eea4933a5f1d42683149e3d","69f185fd47c74721b643f1488d3b9086","ed4cb5cb8f3646e497bde38ca8a2c5bb","7d0de3fffc304ee29f612f540e0e8230","396a8e7bc76f47e5be1e6c5843474abf","be0424c2956a4b01a75cab84ad7f2f1d","dd20bd887f0c464d94497c62a89b0819","fe5018b6ae5d4740bc16fa8e58854dfb","9cb02825446d4822bec51cf69f93b30c"]},"executionInfo":{"elapsed":48822,"status":"ok","timestamp":1729707898570,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"HYONQKws3Tuv","outputId":"e888de7d-b5a2-43ec-f1c4-5a1808d096c0"},"outputs":[],"source":["output_file = output_dir + 'MNIST_real_res.pt'\n","sys.argv = [\n","    'attention_matching',\n","    '--dataset', 'MNIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNetD3',\n","    '--K', '100',\n","    '--init', 'real',\n","    '--batch_real', '256',\n","    '--batch_train', '256',\n","    '--output_file', output_file\n","]\n","MNIST_real_res = attention_matching()\n","\n","\"\"\"\n","sys.argv = [\n","    'attention_matching',\n","    '--dataset', 'MNIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNetD3',\n","    '--K', '100',\n","    '--init', 'real',\n","    '--batch_real', '256',\n","    '--batch_train', '256',\n","    '--output_file', output_file\n","]\n","## Runtime      331.26 s\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2370,"status":"ok","timestamp":1729707900934,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"vS5bJbmS5EYQ","outputId":"ef9be683-8878-4f25-a619-94622501d6a8"},"outputs":[],"source":["data = torch.load(MNIST_real_res)\n","real_imgs = data[0][0]\n","real_labels = data[0][1]\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(real_imgs[real_labels == i][0].cpu().numpy().squeeze(), cmap=\"gray\")\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MNIST Dataset (with real dataset) start\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_real_sample.png\", dpi=300)\n","plt.show()\n","\n","\n","syn_imgs = data[1][0]\n","syn_labels = data[1][1]\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[syn_labels == i][0].cpu().numpy().squeeze(), cmap=\"gray\")\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MNIST Dataset (with real dataset) final\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_real_syn.png\", dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3854,"status":"ok","timestamp":1729707904783,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"y2P5ZHv_hBRx","outputId":"950cb3e2-7ec0-4add-9318-367b768cf2f1"},"outputs":[],"source":["# plot all the synthetic images\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].cpu().numpy().squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","\n","plt.suptitle(\"Synthetic MNIST Dataset (with real dataset)\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_real_syn_all.png\", dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["d9c2e088100a4fed92d3654f8fcb02f9","a3fba991a73c4cb98bbba3c06cc82d9e","f51561f4bc514a4187bef9f04a6b097c","419d2d24820242e0a753da1159521d83","b1118fa353a04c279cf470e063d524c8","1f3404ac31974d3294e29f6992c8d3d4","7bfa685359604986a16475ac81c84aa5","5fc4aa3b2b1343179a28969c310ec1d4","69bb329b7cbe41b981300396346a6b07","5b42fd897d6148d784fd95c3c29df818","17b559d49d584e9c8a189aee5a6123b9"]},"executionInfo":{"elapsed":47569,"status":"ok","timestamp":1729707952339,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"1w8t6bK-3Tuw","outputId":"f1cce021-3bdc-4fd1-bb1b-725f617be342"},"outputs":[],"source":["output_file = output_dir + 'MNIST_noise_res.pt'\n","sys.argv = [\n","    'attention_matching',\n","    '--dataset', 'MNIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNetD3',\n","    '--K', '100',\n","    '--init', 'noise',\n","    '--batch_real', '256',\n","    '--batch_train', '256',\n","    '--output_file', output_file\n","]\n","MNIST_noise_res = attention_matching()\n","\n","\"\"\"\n","sys.argv = [\n","    'attention_matching',\n","    '--dataset', 'MNIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNetD3',\n","    '--K', '100',\n","    '--init', 'noise',\n","    '--batch_real', '256',\n","    '--batch_train', '256',\n","    '--output_file', output_file\n","]\n","## Runtime      323.75 s\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2330,"status":"ok","timestamp":1729707954656,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"tuI9XvaqhBRy","outputId":"83a9c941-2f65-4cd5-e2e7-1b51e6da2364"},"outputs":[],"source":["data = torch.load(MNIST_noise_res)\n","real_imgs = data[0][0]\n","real_labels = data[0][1]\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(real_imgs[real_labels == i][0].cpu().numpy().squeeze(), cmap=\"gray\")\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MNIST Dataset (with gaussian noise) start\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_noise_sample.png\", dpi=300)\n","plt.show()\n","\n","\n","syn_imgs = data[1][0]\n","syn_labels = data[1][1]\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[syn_labels == i][0].cpu().numpy().squeeze(), cmap=\"gray\")\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MNIST Dataset (with gaussian noise) final\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_noise_syn.png\", dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4851,"status":"ok","timestamp":1729707959502,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"xMJX2xW8hBRz","outputId":"3d6da16b-23fe-453e-ee56-79fc61a01f6b"},"outputs":[],"source":["# plot all the synthetic images\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].cpu().numpy().squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","\n","plt.suptitle(\"Synthetic MNIST Dataset (with gaussian noise)\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_noise_syn_all.png\", dpi=300)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e2VBde713Tux"},"source":["### ConvNet3 with Synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["787b79cde0da489fb798b654c36a8616","cfa276575c024041813710f15121bae2","537e599a401c415f848b2d127ea111b0","6e740eff020c4119906e827f79f0b04f","82412ef01eb44e55b95734ebcc9ca280","31b6f6ae4b864bd6b894ce50841d0a44","44aecc3fe647478ab984b68c6f5d02b9","8bcfb15ccd8c41e4adbcf69a0e7b634d","4875f2c1163941cdb38e9917b1cff5cc","8e7b2dfd601c41378a8ed310df005fbb","b22a356ad777430da1624c2d78b5629e"]},"executionInfo":{"elapsed":85033,"status":"ok","timestamp":1729708044513,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"vWJxZsx13Tux","outputId":"8dbdb711-1737-4b25-b94e-1b1f6c201cf1"},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MNIST_real_res.pt')\n","ConvNet3Syn = get_network('ConvNetD3', channel, num_classes, im_size)\n","n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet3Syn.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MNIST_dataloader\n","\n","progress_bar = tqdm(\n","    total=n_epochs*len(trainLoader) + n_epochs*len(testLoader),\n","    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","    unit = 'step',\n",")\n","start_time = time.time()\n","for ep in range (n_epochs):\n","    train_loss_avg, train_acc_avg = epoch_S(\n","        'train',\n","        trainLoader,\n","        ConvNet3Syn,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","        \n","    test_loss_avg, test_acc_avg = epoch_S(\n","        'test',\n","        testLoader,\n","        ConvNet3Syn,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3Syn_train_acc = train_acc\n","ConvNet3Syn_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet3Syn, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))\n","\n","## Runtime      32.27 s\n","## FLOPS        48,429,056"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1729708044514,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"aaRsLgJW3Tux","outputId":"1de41b76-a0d1-40a8-c0e9-ee1c26b59fa2"},"outputs":[],"source":["plt.figure(figsize=(7, 5))  # Set figure size\n","\n","# Plotting with different line styles for train (dashed) and test (solid) data\n","plt.plot(ConvNet3_train_acc, label=\"Original Dataset [Train]\", linestyle='--', color='b')  # Dashed line for training\n","plt.plot(ConvNet3_test_acc, label=\"Original Dataset [Test]\", linestyle='-', color='b')    # Solid line for testing\n","plt.plot(ConvNet3Syn_train_acc, label=\"Synthetic Dataset [Train]\", linestyle='--', color='r')  # Dashed line for training\n","plt.plot(ConvNet3Syn_test_acc, label=\"Synthetic Dataset [Test]\", linestyle='-', color='r')    # Solid line for testing\n","\n","# Add grid\n","plt.grid(False)\n","\n","# Ensure x-axis displays only whole numbers, every 5 epochs\n","plt.xticks(np.arange(0, n_epochs+1, 5))\n","\n","\n","plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n","plt.title(\"ConvNet3 Model Accuracy for MNIST Dataset\", fontsize=14, fontweight='bold')\n","\n","# Add legend with a better location\n","plt.legend(loc='best', fontsize=10)\n","\n","# Add a tight layout to minimize unnecessary whitespace\n","plt.tight_layout()\n","\n","plt.savefig(figures_dir + \"mnist_syn_acc.png\", dpi=300)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Cross-architecture Generalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MNIST_dataset,\n","    test_MNIST_dataset,\n","    test_MNIST_dataloader,\n","    train_MNIST_dataloader,\n",") = get_dataset(\"MNIST\", \"../datasets\")\n","\n","alex_net = get_network('AlexNet', channel, num_classes, im_size)\n","\n","print(alex_net)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MNIST_real_res.pt')\n","n_epochs = 100\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(alex_net.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MNIST_dataloader\n","\n","progress_bar = tqdm(\n","    total=n_epochs*len(trainLoader) + n_epochs*len(testLoader),\n","    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","    unit = 'step',\n",")\n","start_time = time.time()\n","for ep in range (n_epochs):\n","    train_loss_avg, train_acc_avg = epoch_S(\n","        'train',\n","        trainLoader,\n","        alex_net,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","\n","    test_loss_avg, test_acc_avg = epoch_S(\n","        'test',\n","        testLoader,\n","        alex_net,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","alex_net_train_acc = train_acc\n","alex_net_test_acc = test_acc\n","\n","flops, _ = get_flops(alex_net, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))\n","\n","## Runtime      170.02 s\n","## FLops        238,450,688"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(7, 5))  # Set figure size\n","\n","# plot AlexNet accuracy results\n","plt.plot(alex_net_train_acc, label=\"Train Acc.\", linestyle='--', color='b')  # Dashed line for training\n","plt.plot(alex_net_test_acc, label=\"Test Acc.\", linestyle='-', color='b')    # Solid line for testing\n","\n","plt.grid(False)\n","plt.xticks(np.arange(0, n_epochs+1, 5))\n","plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n","plt.title(\"AlexNet Model Accuracy for Synthetic MNIST Dataset\", fontsize=14, fontweight='bold')\n","plt.legend(loc='best', fontsize=10)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mnist_alex_acc.png\", dpi=300)\n"]},{"cell_type":"markdown","metadata":{"id":"VgyQFdNY3Tuy"},"source":["## MHIST"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"executionInfo":{"elapsed":12113,"status":"ok","timestamp":1729708276427,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"4trkVu1P3Tuy","outputId":"590c9d6e-d300-4e28-8a1e-cb1b7f4491e8"},"outputs":[],"source":["(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MHIST_dataset,\n","    test_MHIST_dataset,\n","    test_MHIST_dataloader,\n","    train_MHIST_dataloader,\n",") = get_dataset(\"MHIST\", \"../datasets\")\n","\n","indices = [100, 1800]\n","\n","# plot 2 images from the dataset\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    image, label = train_MHIST_dataset[indices[i]]\n","    # Transpose the image from [3, 224, 224] to [224, 224, 3] for plotting\n","    image = image.permute(1, 2, 0)\n","    ax.imshow(image)\n","    if label == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"MHIST_dataset.png\", dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"pX3MQTu93Tuy"},"source":["### ConvNet7"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1729708295987,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"xhMZfqCN3Tuy","outputId":"7c4d8d6f-8b4a-4d7b-998c-2bd1932eccbb"},"outputs":[],"source":["ConvNet7 = get_network('ConvNetD7', channel, num_classes, im_size)\n","print(ConvNet7)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397,"referenced_widgets":["3972a63eb61843fe87fb42a3a8554902","a652957f977c4db580ecf8bec6494f14","b610910acc4f414eb5fbb8ea638cf65a","8e90676fbc594d40b0af35bd9b597fcd","5a1ce7000e494f43b204e2f8b709ff2a","ffe9ba3316b54e12a5b59650b6c16cd1","7dfdc94f5bbc4894a098701e3b09057b","7ec22626dc8c4a3ebbd9e5d6003cdf40","60c95665be2f46ddb6a676951c57d78c","3210d11e50bc498480c08134020f5bc5","dd646f11219b4cd7b9fbbe09730c353c"]},"executionInfo":{"elapsed":250966,"status":"error","timestamp":1729708549079,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"Ekz5CVpL3Tuz","outputId":"d79fb498-6925-4006-e648-3a586a01e538"},"outputs":[],"source":["n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet7.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = train_MHIST_dataloader\n","testLoader = test_MHIST_dataloader\n","\n","progress_bar = tqdm(\n","    total=n_epochs*len(trainLoader) + n_epochs*len(testLoader),\n","    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","    unit = 'step',\n",")\n","start_time = time.time()\n","for ep in range (n_epochs):\n","    train_loss_avg, train_acc_avg = epoch_S(\n","        'train',\n","        trainLoader,\n","        ConvNet7,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","        \n","    test_loss_avg, test_acc_avg = epoch_S(\n","        'test',\n","        testLoader,\n","        ConvNet7,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","flops, _ = get_flops(ConvNet7, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))\n","\n","# Runtime      466.05 s\n","# FLOPS        2,640,717,952 "]},{"cell_type":"markdown","metadata":{"id":"JPHNre3z3Tuz"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eg396AJR3Tuz"},"outputs":[],"source":["output_file = output_dir + 'MHIST_real_res.pt'\n","sys.argv = [\n","    'attention_matching',\n","    '--dataset', 'MHIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNetD7',\n","    '--K', '200',\n","    '--ipc', '50',\n","    '--init', 'real',\n","    '--batch_real', '4',           # Gpu ram limited\n","    '--batch_train', '4',          # Gpu ram limted\n","    '--output_file', output_file\n","]\n","MNIST_real_res = attention_matching()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FivmMeFX3Tuz"},"outputs":[],"source":["MHIST_real_res = output_dir + 'MHIST_real_res.pt'\n","results = torch.load(MHIST_real_res)\n","real_imgs = results[0][0]\n","real_labels = results[0][1]\n","\n","# visualize 2 classes of MHIST (1 by 2)\n","# 0 = HP, 1 = SSA\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    img = real_imgs[real_labels == i][0].permute(1, 2, 0).squeeze().numpy()\n","    img = np.clip(img, 0, 1)  # Ensure pixel values are in the range [0, 1]\n","    ax.imshow(img)\n","    if i == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MHIST Dataset (with real dataset) start\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mhist_real_sample.png\", dpi=300)\n","plt.show()\n","\n","syn_imgs = results[1][0]\n","syn_labels = results[1][1]\n","\n","# visualize 2 classes of MHIST (1 by 2)\n","# 0 = HP, 1 = SSA\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    img = syn_imgs[syn_labels == i][0].permute(1, 2, 0).squeeze().numpy()\n","    img = np.clip(img, 0, 1)  # Ensure pixel values are in the range [0, 1]\n","    ax.imshow(img)\n","    if i == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MHIST Dataset (with real dataset) final\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mhist_real_syn.png\", dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot 10 images from 2 classes of MHIST (5 by 2)\n","# 0 = HP, 1 = SSA\n","fig, axes = plt.subplots(5, 2, figsize=(6, 15))\n","for i in range(2):  # For each class\n","    for j in range(5):  # For 5 images per class\n","        img = syn_imgs[syn_labels == i][j].permute(1, 2, 0).squeeze().numpy()\n","        img = np.clip(img, 0, 1)\n","        axes[j, i].imshow(img)\n","        axes[j, i].axis(\"off\")\n","        if j == 0:\n","            axes[j, i].set_title(\"HP\" if i == 0 else \"SSA\")\n","    \n","plt.suptitle(\"Synthetic MHIST Dataset (with real dataset)\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mhist_real_syn_all.png\", dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YmBPU773Tu0"},"outputs":[],"source":["output_file = output_dir + 'MHIST_noise_res.pt'\n","sys.argv = [\n","    'attention_matching',\n","    '--dataset', 'MHIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNetD7',\n","    '--K', '200',\n","    '--ipc', '50',\n","    '--init', 'noise',\n","    '--batch_real', '128',           # Gpu ram limited\n","    '--batch_train', '128',          # Gpu ram limted\n","    '--output_file', output_file\n","]\n","MHIST_noise_res = attention_matching()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RhsasRM3Tu0"},"outputs":[],"source":["MHIST_noise_res = output_dir + 'MHIST_noise_res.pt'\n","results = torch.load(MHIST_noise_res)\n","real_imgs = results[0][0]\n","real_labels = results[0][1]\n","\n","# visualize 2 classes of MHIST (1 by 2)\n","# 0 = HP, 1 = SSA\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    img = real_imgs[real_labels == i][0].permute(1, 2, 0).squeeze().numpy()\n","    img = np.clip(img, 0, 1)  # Ensure pixel values are in the range [0, 1]\n","    ax.imshow(img)\n","    if i == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MHIST Dataset (with gaussian noise) start\", fontsize=16)\n","plt.tight_layout()\n","plt.show()\n","\n","syn_imgs = results[1][0]\n","syn_labels = results[1][1]\n","\n","# visualize 2 classes of MHIST (1 by 2)\n","# 0 = HP, 1 = SSA\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    img = syn_imgs[syn_labels == i][0].permute(1, 2, 0).squeeze().numpy()\n","    img = np.clip(img, 0, 1)  # Ensure pixel values are in the range [0, 1]\n","    ax.imshow(img)\n","    if i == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Synthetic MHIST Dataset (with gaussian noise) final\", fontsize=16)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot 10 images from 2 classes of MHIST (5 by 2)\n","# 0 = HP, 1 = SSA\n","fig, axes = plt.subplots(5, 2, figsize=(6, 15))\n","for i in range(2):  # For each class\n","    for j in range(5):  # For 5 images per class\n","        img = syn_imgs[syn_labels == i][j].permute(1, 2, 0).squeeze().numpy()\n","        img = np.clip(img, 0, 1)\n","        axes[j, i].imshow(img)\n","        axes[j, i].axis(\"off\")\n","        if j == 0:\n","            axes[j, i].set_title(\"HP\" if i == 0 else \"SSA\")\n","\n","plt.suptitle(\"Synthetic MHIST Dataset (with gaussian noise)\", fontsize=16)\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mhist_noise_syn_all.png\", dpi=300)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UZqV01fd3Tu0"},"source":["### ConvNet7 with Synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_zc9k5J3Tu0"},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MHIST_real_res.pt')\n","ConvNet7Syn = get_network('ConvNetD7', channel, num_classes, im_size)\n","n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet7Syn.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MHIST_dataloader\n","\n","progress_bar = tqdm(\n","    total=n_epochs*len(trainLoader) + n_epochs*len(testLoader),\n","    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","    unit = 'step',\n",")\n","start_time = time.time()\n","for ep in range (n_epochs):\n","    train_loss_avg, train_acc_avg = epoch_S(\n","        'train',\n","        trainLoader,\n","        ConvNet7Syn,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","        \n","    test_loss_avg, test_acc_avg = epoch_S(\n","        'test',\n","        testLoader,\n","        ConvNet7Syn,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet7Syn_train_acc = train_acc\n","ConvNet7Syn_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet7Syn, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(7, 5))  # Set figure size\n","\n","# Plotting with different line styles for train (dashed) and test (solid) data\n","plt.plot(ConvNet7_train_acc, label=\"Original Dataset [Train]\", linestyle='--', color='b')  # Dashed line for training\n","plt.plot(ConvNet7_test_acc, label=\"Original Dataset [Test]\", linestyle='-', color='b')    # Solid line for testing\n","plt.plot(ConvNet7Syn_train_acc, label=\"Synthetic Dataset [Train]\", linestyle='--', color='r')  # Dashed line for training\n","plt.plot(ConvNet7Syn_test_acc, label=\"Synthetic Dataset [Test]\", linestyle='-', color='r')    # Solid line for testing\n","\n","plt.grid(False)\n","\n","# Ensure x-axis displays only whole numbers, every 5 epochs\n","plt.xticks(np.arange(0, n_epochs+1, 5))\n","\n","plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n","plt.title(\"ConvNet7 Model Accuracy for MHIST Dataset\", fontsize=14, fontweight='bold')\n","\n","# Add legend with a better location\n","plt.legend(loc='best', fontsize=10)\n","\n","# Add a tight layout to minimize unnecessary whitespace\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"mhist_syn_acc.png\", dpi=300)\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Cross-architecture Generalization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MHIST_dataset,\n","    test_MHIST_dataset,\n","    test_MHIST_dataloader,\n","    train_MHIST_dataloader,\n",") = get_dataset(\"MHIST\", \"../datasets\")\n","\n","alex_net = get_network('AlexNet', channel, num_classes, im_size)\n","print(alex_net)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MHIST_real_res.pt')\n","n_epochs = 50\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(alex_net.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MHIST_dataloader\n","\n","progress_bar = tqdm(\n","    total=n_epochs*len(trainLoader) + n_epochs*len(testLoader),\n","    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',\n","    unit = 'step',\n",")\n","start_time = time.time()\n","for ep in range (n_epochs):\n","    train_loss_avg, train_acc_avg = epoch_S(\n","        'train',\n","        trainLoader,\n","        alex_net,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","\n","    test_loss_avg, test_acc_avg = epoch_S(\n","        'test',\n","        testLoader,\n","        alex_net,\n","        optimizer,\n","        criterion,\n","        device,\n","        progress_bar,\n","        start_time,\n","    )\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","alex_net_train_acc = train_acc\n","alex_net_test_acc = test_acc\n","\n","flops, _ = get_flops(alex_net, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(7, 5))  # Set figure size\n","\n","# Plotting with different line styles for train (dashed) and test (solid) data\n","plt.plot(ConvNet7Syn_train_acc, label=\"ConvNet7 [Train]\", linestyle='--', color='b')  # Dashed line for training\n","plt.plot(ConvNet7Syn_test_acc, label=\"ConvNet7 [Test]\", linestyle='-', color='b')    # Solid line for testing\n","plt.plot(alex_net_train_acc, label=\"AlexNet [Train]\", linestyle='--', color='g')  # Dashed line for training\n","plt.plot(alex_net_test_acc, label=\"AlexNet [Test]\", linestyle='-', color='g')    # Solid line for testing\n","plt.grid(False)\n","plt.xticks(np.arange(0, n_epochs+1, 5))\n","plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n","plt.title(\"Model Accuracy Comparison for Synthetic MHIST Dataset\", fontsize=14, fontweight='bold')\n","plt.legend(loc='best', fontsize=10)\n","plt.savefig(figures_dir + \"mhist_syn_acc_comp.png\", dpi=300)\n","plt.tight_layout()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"007faccb68424bb0b8bf5ee149d53083":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b2edab164974ef6be5dd4082056bb77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_964cf247db3e43e48a18293b731f73a0","placeholder":"​","style":"IPY_MODEL_ad49a92464fa46fc8b289d6bd5b8c74f","value":"100%"}},"17b559d49d584e9c8a189aee5a6123b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f3404ac31974d3294e29f6992c8d3d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"291438b4b4ad41ccbc27ad7409377a28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d5675c156ab478da20e7ebc451c53cd","max":21900,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e590faafef041bfa7f38eeffb06be9b","value":21900}},"31b6f6ae4b864bd6b894ce50841d0a44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3210d11e50bc498480c08134020f5bc5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"396a8e7bc76f47e5be1e6c5843474abf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3972a63eb61843fe87fb42a3a8554902":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a652957f977c4db580ecf8bec6494f14","IPY_MODEL_b610910acc4f414eb5fbb8ea638cf65a","IPY_MODEL_8e90676fbc594d40b0af35bd9b597fcd"],"layout":"IPY_MODEL_5a1ce7000e494f43b204e2f8b709ff2a"}},"3e590faafef041bfa7f38eeffb06be9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"419d2d24820242e0a753da1159521d83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b42fd897d6148d784fd95c3c29df818","placeholder":"​","style":"IPY_MODEL_17b559d49d584e9c8a189aee5a6123b9","value":" 1000/1000 [00:20&lt;00:00, 54.05step/s]"}},"44aecc3fe647478ab984b68c6f5d02b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4875f2c1163941cdb38e9917b1cff5cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d20dcb6de7843d0a947ec6013568213":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d5675c156ab478da20e7ebc451c53cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5305717294404301a96716966f4a2608":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86148bf00e29414c84c0aee1f3a64ef1","IPY_MODEL_560a36a23eea4933a5f1d42683149e3d","IPY_MODEL_69f185fd47c74721b643f1488d3b9086"],"layout":"IPY_MODEL_ed4cb5cb8f3646e497bde38ca8a2c5bb"}},"537e599a401c415f848b2d127ea111b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bcfb15ccd8c41e4adbcf69a0e7b634d","max":3220,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4875f2c1163941cdb38e9917b1cff5cc","value":3220}},"560a36a23eea4933a5f1d42683149e3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be0424c2956a4b01a75cab84ad7f2f1d","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd20bd887f0c464d94497c62a89b0819","value":1000}},"5a1ce7000e494f43b204e2f8b709ff2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b42fd897d6148d784fd95c3c29df818":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fc4aa3b2b1343179a28969c310ec1d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c95665be2f46ddb6a676951c57d78c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69bb329b7cbe41b981300396346a6b07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69f185fd47c74721b643f1488d3b9086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe5018b6ae5d4740bc16fa8e58854dfb","placeholder":"​","style":"IPY_MODEL_9cb02825446d4822bec51cf69f93b30c","value":" 1000/1000 [00:21&lt;00:00, 56.82step/s]"}},"6e740eff020c4119906e827f79f0b04f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e7b2dfd601c41378a8ed310df005fbb","placeholder":"​","style":"IPY_MODEL_b22a356ad777430da1624c2d78b5629e","value":" 3220/3220 [01:42&lt;00:00, 42.34epoch/s]"}},"787b79cde0da489fb798b654c36a8616":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfa276575c024041813710f15121bae2","IPY_MODEL_537e599a401c415f848b2d127ea111b0","IPY_MODEL_6e740eff020c4119906e827f79f0b04f"],"layout":"IPY_MODEL_82412ef01eb44e55b95734ebcc9ca280"}},"7bfa685359604986a16475ac81c84aa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d0de3fffc304ee29f612f540e0e8230":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dfdc94f5bbc4894a098701e3b09057b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ec22626dc8c4a3ebbd9e5d6003cdf40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82412ef01eb44e55b95734ebcc9ca280":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86148bf00e29414c84c0aee1f3a64ef1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d0de3fffc304ee29f612f540e0e8230","placeholder":"​","style":"IPY_MODEL_396a8e7bc76f47e5be1e6c5843474abf","value":"Elapsed: 21.32s | : 100%"}},"8bcfb15ccd8c41e4adbcf69a0e7b634d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e7b2dfd601c41378a8ed310df005fbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e90676fbc594d40b0af35bd9b597fcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3210d11e50bc498480c08134020f5bc5","placeholder":"​","style":"IPY_MODEL_dd646f11219b4cd7b9fbbe09730c353c","value":" 9/50 [04:06&lt;19:02, 27.86s/it, loss=0.276]"}},"964cf247db3e43e48a18293b731f73a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b9d7e8d1aae46819c74a7aad620dfce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b2edab164974ef6be5dd4082056bb77","IPY_MODEL_291438b4b4ad41ccbc27ad7409377a28","IPY_MODEL_b470dd1875e24c419cd73dba1dade827"],"layout":"IPY_MODEL_d277fe13863a4a098ba85d2d9c62054f"}},"9cb02825446d4822bec51cf69f93b30c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3fba991a73c4cb98bbba3c06cc82d9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f3404ac31974d3294e29f6992c8d3d4","placeholder":"​","style":"IPY_MODEL_7bfa685359604986a16475ac81c84aa5","value":"Elapsed: 20.13s | : 100%"}},"a652957f977c4db580ecf8bec6494f14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffe9ba3316b54e12a5b59650b6c16cd1","placeholder":"​","style":"IPY_MODEL_7dfdc94f5bbc4894a098701e3b09057b","value":"Epoch 1:  18%"}},"ad49a92464fa46fc8b289d6bd5b8c74f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1118fa353a04c279cf470e063d524c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b22a356ad777430da1624c2d78b5629e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b470dd1875e24c419cd73dba1dade827":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_007faccb68424bb0b8bf5ee149d53083","placeholder":"​","style":"IPY_MODEL_4d20dcb6de7843d0a947ec6013568213","value":" 21900/21900 [12:00&lt;00:00, 39.75epoch/s]"}},"b610910acc4f414eb5fbb8ea638cf65a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ec22626dc8c4a3ebbd9e5d6003cdf40","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60c95665be2f46ddb6a676951c57d78c","value":9}},"be0424c2956a4b01a75cab84ad7f2f1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfa276575c024041813710f15121bae2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31b6f6ae4b864bd6b894ce50841d0a44","placeholder":"​","style":"IPY_MODEL_44aecc3fe647478ab984b68c6f5d02b9","value":"100%"}},"d277fe13863a4a098ba85d2d9c62054f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9c2e088100a4fed92d3654f8fcb02f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3fba991a73c4cb98bbba3c06cc82d9e","IPY_MODEL_f51561f4bc514a4187bef9f04a6b097c","IPY_MODEL_419d2d24820242e0a753da1159521d83"],"layout":"IPY_MODEL_b1118fa353a04c279cf470e063d524c8"}},"dd20bd887f0c464d94497c62a89b0819":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd646f11219b4cd7b9fbbe09730c353c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed4cb5cb8f3646e497bde38ca8a2c5bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f51561f4bc514a4187bef9f04a6b097c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fc4aa3b2b1343179a28969c310ec1d4","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69bb329b7cbe41b981300396346a6b07","value":1000}},"fe5018b6ae5d4740bc16fa8e58854dfb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffe9ba3316b54e12a5b59650b6c16cd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
