{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.4.1+cu121\n",
      "CUDA Device: NVIDIA GeForce RTX 2060\n",
      "CUDA Compute Capability: 7.5\n",
      "Total Memory: 5.61 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    properties = torch.cuda.get_device_properties(0)\n",
    "    compute_capability = f\"{properties.major}.{properties.minor}\"\n",
    "    total_memory = properties.total_memory / 1024**3\n",
    "\n",
    "    print(f\"CUDA Device: {device_name}\")\n",
    "    print(f\"CUDA Compute Capability: {compute_capability}\")\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load MNIST dataset from utils\n",
    "from utils import get_dataset\n",
    "\n",
    "(\n",
    "    channel,\n",
    "    im_size,\n",
    "    num_classes,\n",
    "    class_names,\n",
    "    mean,\n",
    "    std,\n",
    "    train_MNIST_dataset,\n",
    "    test_MNIST_dataset,\n",
    "    test_MNIST_dataloader,\n",
    "    train_MNIST_dataloader,\n",
    ") = get_dataset(\"MNIST\", \"../datasets\")\n",
    "\n",
    "# visualize 10 classes of MNIST (2 by 5)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(\n",
    "        train_MNIST_dataset.data[train_MNIST_dataset.targets == i][0], cmap=\"gray\"\n",
    "    )\n",
    "    ax.set_title(f\"{i}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
      "    (1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils import get_network\n",
    "\n",
    "ConvNet3 = get_network('ConvNet', channel, num_classes, im_size)\n",
    "print(ConvNet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b84a2391a3400b833eed6cbf8d998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef7eeb0765f4c2cb4430370a164f5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(ConvNet3.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ConvNet3.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_MNIST_dataloader, 0),\n",
    "        total=len(train_MNIST_dataloader),\n",
    "        desc=f\"Epoch {epoch+1}\",\n",
    "    )\n",
    "\n",
    "    total_iterations = len(train_MNIST_dataloader)\n",
    "    for i, data in enumerate(train_MNIST_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ConvNet3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        percentage_complete = (i + 1) / total_iterations * 100\n",
    "        progress_bar.set_postfix(loss=running_loss / (i + 1))\n",
    "        progress_bar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report classification accuracy for test dataset\n",
    "ConvNet3.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_MNIST_dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ConvNet3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Distillation using Attention matching Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Attention Matching Algorithm hyperparameters \"\"\"\n",
    "imgs_per_class = 10\n",
    "minibatch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 5923 real images\n",
      "class 1: 6742 real images\n",
      "class 2: 5958 real images\n",
      "class 3: 6131 real images\n",
      "class 4: 5842 real images\n",
      "class 5: 5421 real images\n",
      "class 6: 5918 real images\n",
      "class 7: 6265 real images\n",
      "class 8: 5851 real images\n",
      "class 9: 5949 real images\n",
      "real images channel 0, mean = -0.0001, std = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# learn synthetic dataset using Attention matching algorithm\n",
    "\n",
    "\"\"\" organize the real dataset \"\"\"\n",
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [\n",
    "    torch.unsqueeze(train_MNIST_dataset[i][0], dim=0)\n",
    "    for i in range(len(train_MNIST_dataset))\n",
    "]\n",
    "labels_all = [train_MNIST_dataset[i][1] for i in range(len(train_MNIST_dataset))]\n",
    "\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "for c in range(num_classes):\n",
    "    print(\"class %d: %d real images\" % (c, len(indices_class[c])))\n",
    "\n",
    "\n",
    "def get_images(c, n):  # get random n images from class c\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    return images_all[idx_shuffle]\n",
    "\n",
    "\n",
    "for ch in range(channel):\n",
    "    print(\n",
    "        \"real images channel %d, mean = %.4f, std = %.4f\"\n",
    "        % (ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142754/3391903044.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  labels_syn = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Initialize the synthetic dataset \"\"\"\n",
    "images_syn = torch.randn(\n",
    "    size=(num_classes*imgs_per_class, channel, im_size[0], im_size[1]),\n",
    "    dtype=torch.float,\n",
    "    requires_grad=True,\n",
    "    device=device,\n",
    ")\n",
    "labels_syn = torch.tensor(\n",
    "    [np.ones(imgs_per_class) * c for c in range(num_classes)],\n",
    "    dtype=torch.long,\n",
    "    requires_grad=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Initialize synthetic dataset using real images\n",
    "for c in range(num_classes):\n",
    "    real_images = get_images(c, imgs_per_class)\n",
    "    images_syn.data[c*imgs_per_class:(c+1)*imgs_per_class] = real_images.data\n",
    "\n",
    "# print details of synthetic dataset, size and all\n",
    "print(images_syn.size())\n",
    "print(labels_syn.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def getActivation(name):\n",
    "    def hook_func(m, inp, op):\n",
    "        activations[name] = op.clone()\n",
    "    return hook_func\n",
    "\n",
    "''' Defining the Refresh Function to store Activations and reset Collection '''\n",
    "def refreshActivations(activations):\n",
    "    model_set_activations = [] # Jagged Tensor Creation\n",
    "    for i in activations.keys():\n",
    "        model_set_activations.append(activations[i])\n",
    "    activations = {}\n",
    "    return activations, model_set_activations\n",
    "\n",
    "''' Defining the Delete Hook Function to collect Remove Hooks '''\n",
    "def delete_hooks(hooks):\n",
    "    for i in hooks:\n",
    "        i.remove()\n",
    "    return\n",
    "        \n",
    "def attach_hooks(net):\n",
    "    hooks = []\n",
    "    base = net.module if torch.cuda.device_count() > 1 else net\n",
    "    for module in (base.features.named_modules()):\n",
    "        if isinstance(module[1], nn.ReLU):\n",
    "            # Hook the Ouptus of a ReLU Layer\n",
    "            hooks.append(base.features[int(module[0])].register_forward_hook(getActivation('ReLU_'+str(len(hooks)))))\n",
    "    return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 5.61 GiB of which 253.81 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 4.90 GiB is allocated by PyTorch, and 151.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m images_syn_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(images_syn_all, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m hooks \u001b[38;5;241m=\u001b[39m attach_hooks(ConvNet3)\n\u001b[0;32m---> 24\u001b[0m output_real \u001b[38;5;241m=\u001b[39m \u001b[43mConvNet3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_real_all\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     25\u001b[0m activations, original_model_set_activations \u001b[38;5;241m=\u001b[39m refreshActivations(activations)\n\u001b[1;32m     28\u001b[0m output_syn \u001b[38;5;241m=\u001b[39m ConvNet3(images_syn_all)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src/networks.py:45\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(out)\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/ECE1512_2024F_ProjectRepo_SwapnilPatel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1616\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1616\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mgetActivation.<locals>.hook_func\u001b[0;34m(m, inp, op)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhook_func\u001b[39m(m, inp, op):\n\u001b[0;32m----> 4\u001b[0m     activations[name] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 5.61 GiB of which 253.81 MiB is free. Including non-PyTorch memory, this process has 5.16 GiB memory in use. Of the allocated memory 4.90 GiB is allocated by PyTorch, and 151.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the synthetic dataset using Attention Matching Algorithm \"\"\"\n",
    "\n",
    "optimizer_img = torch.optim.SGD([images_syn], lr=0.1, momentum=0.9)\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "images_real_all = []\n",
    "images_syn_all = []\n",
    "\n",
    "for c in range(num_classes):\n",
    "    img_real = get_images(c, minibatch_size)\n",
    "    img_sync = images_syn[c * imgs_per_class : (c + 1) * imgs_per_class].reshape(\n",
    "        (imgs_per_class, channel, im_size[0], im_size[1])\n",
    "    )\n",
    "\n",
    "    images_real_all.append(img_real)\n",
    "    images_syn_all.append(img_sync)\n",
    "\n",
    "images_real_all = torch.cat(images_real_all, dim=0)\n",
    "images_syn_all = torch.cat(images_syn_all, dim=0)\n",
    "\n",
    "hooks = attach_hooks(ConvNet3)\n",
    "output_real = ConvNet3(images_real_all).detach()\n",
    "activations, original_model_set_activations = refreshActivations(activations)\n",
    "\n",
    "\n",
    "output_syn = ConvNet3(images_syn_all).detach()\n",
    "activations, synthetic_model_set_activations = refreshActivations(activations)\n",
    "delete_hooks(hooks)\n",
    "\n",
    "print(\"output_real\", output_real.size())\n",
    "print(\"output_syn\", output_syn.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MHIST dataset from MHISTDataset class\n",
    "import MHISTDataset\n",
    "\n",
    "train_MHIST_dataset = MHISTDataset.MHISTDataset('../mhist_dataset/images-split/train', transform=None)\n",
    "test_MNIST_dataset = MHISTDataset.MHISTDataset('../mhist_dataset/images-split/test', transform=None)\n",
    "\n",
    "# find train_MHIST_dataset with \"HP\" label, train_MHHIST_dataset[i][2]\n",
    "HP_indices = [i for i in range(len(train_MHIST_dataset)) if train_MHIST_dataset[i][2] == 'HP']\n",
    "SSA_indices = [i for i in range(len(train_MHIST_dataset)) if train_MHIST_dataset[i][2] == 'SSA']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "axes[0].imshow(train_MHIST_dataset[HP_indices[0]][0])\n",
    "axes[0].set_title(\"HP\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(train_MHIST_dataset[SSA_indices[0]][0])\n",
    "axes[1].set_title(\"SSA\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
