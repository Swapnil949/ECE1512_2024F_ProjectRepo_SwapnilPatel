{"cells":[{"cell_type":"markdown","metadata":{"id":"hajAmAsD3Tuo"},"source":["## Project Initialization"]},{"cell_type":"markdown","metadata":{"id":"x1k_-2JDjdVP"},"source":["### MHISTDataet.py"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3723,"status":"ok","timestamp":1729642574507,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"uhhL6chvjaoW"},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","from PIL import Image\n","from torch.utils.data import Dataset\n","\n","\n","class MHISTDataset(Dataset):\n","    def __init__(self, split_dir, train=True, transform=None):\n","        \"\"\"\n","        Args:\n","            split_dir (str): Directory containing the split images (e.g., 'images-split/train' or 'images-split/test').\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.split_dir = Path(split_dir)\n","        if train:\n","            self.split_dir = self.split_dir / 'train'\n","        else:\n","            self.split_dir = self.split_dir / 'test'\n","\n","        self.transform = transform\n","\n","        # Map folder names to class labels\n","        self.label_map = {\"HP\": 0, \"SSA\": 1}\n","\n","        # Gather image file paths and their corresponding labels\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for label_name, label_value in self.label_map.items():\n","            class_dir = self.split_dir / label_name\n","            if class_dir.exists():\n","                for img_path in class_dir.rglob('*.*'):  # Recursively list all image files\n","                    self.image_paths.append(img_path)\n","                    self.labels.append(label_value)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Get the image path and corresponding label\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path)\n","\n","        label = self.labels[idx]\n","        # map label to class name, if 0 then HP, if 1 then SSA\n","\n","        name = \"HP\" if label == 0 else \"SSA\"\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"B0rkzMf_jtH9"},"source":["### Networks.py"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1729642574507,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"s2hOogVujv9I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# Acknowledgement to\n","# https://github.com/kuangliu/pytorch-cifar,\n","# https://github.com/BIGBALLON/CIFAR-ZOO,\n","\n","\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","\n","''' Swish activation '''\n","class Swish(nn.Module): # Swish(x) = x∗σ(x)\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, input):\n","        return input * torch.sigmoid(input)\n","\n","\n","''' MLP '''\n","class MLP(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(MLP, self).__init__()\n","        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n","        self.fc_2 = nn.Linear(128, 128)\n","        self.fc_3 = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        out = x.view(x.size(0), -1)\n","        out = F.relu(self.fc_1(out))\n","        out = F.relu(self.fc_2(out))\n","        out = self.fc_3(out)\n","        return out\n","\n","\n","\n","''' ConvNet '''\n","class ConvNet(nn.Module):\n","    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n","        super(ConvNet, self).__init__()\n","\n","        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n","        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n","        self.classifier = nn.Linear(num_feat, num_classes)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","    def _get_activation(self, net_act):\n","        if net_act == 'sigmoid':\n","            return nn.Sigmoid()\n","        elif net_act == 'relu':\n","            return nn.ReLU(inplace=True)\n","        elif net_act == 'leakyrelu':\n","            return nn.LeakyReLU(negative_slope=0.01)\n","        elif net_act == 'swish':\n","            return Swish()\n","        else:\n","            exit('unknown activation function: %s'%net_act)\n","\n","    def _get_pooling(self, net_pooling):\n","        if net_pooling == 'maxpooling':\n","            return nn.MaxPool2d(kernel_size=2, stride=2)\n","        elif net_pooling == 'avgpooling':\n","            return nn.AvgPool2d(kernel_size=2, stride=2)\n","        elif net_pooling == 'none':\n","            return None\n","        else:\n","            exit('unknown net_pooling: %s'%net_pooling)\n","\n","    def _get_normlayer(self, net_norm, shape_feat):\n","        # shape_feat = (c*h*w)\n","        if net_norm == 'batchnorm':\n","            return nn.BatchNorm2d(shape_feat[0], affine=True)\n","        elif net_norm == 'layernorm':\n","            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n","        elif net_norm == 'instancenorm':\n","            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n","        elif net_norm == 'groupnorm':\n","            return nn.GroupNorm(4, shape_feat[0], affine=True)\n","        elif net_norm == 'none':\n","            return None\n","        else:\n","            exit('unknown net_norm: %s'%net_norm)\n","\n","    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n","        layers = []\n","        in_channels = channel\n","        if im_size[0] == 28:\n","            im_size = (32, 32)\n","        shape_feat = [in_channels, im_size[0], im_size[1]]\n","        for d in range(net_depth):\n","            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n","            shape_feat[0] = net_width\n","            if net_norm != 'none':\n","                layers += [self._get_normlayer(net_norm, shape_feat)]\n","            layers += [self._get_activation(net_act)]\n","            in_channels = net_width\n","            if net_pooling != 'none':\n","                layers += [self._get_pooling(net_pooling)]\n","                shape_feat[1] //= 2\n","                shape_feat[2] //= 2\n","\n","        return nn.Sequential(*layers), shape_feat\n","\n","\n","\n","''' LeNet '''\n","class LeNet(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(LeNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(6, 16, kernel_size=5),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc_2 = nn.Linear(120, 84)\n","        self.fc_3 = nn.Linear(84, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc_1(x))\n","        x = F.relu(self.fc_2(x))\n","        x = self.fc_3(x)\n","        return x\n","\n","\n","\n","''' AlexNet '''\n","class AlexNet(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","\n","''' AlexNetBN '''\n","class AlexNetBN(nn.Module):\n","    def __init__(self, channel, num_classes):\n","        super(AlexNetBN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","\n","''' VGG '''\n","cfg_vgg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name, channel, num_classes, norm='instancenorm'):\n","        super(VGG, self).__init__()\n","        self.channel = channel\n","        self.features = self._make_layers(cfg_vgg[vgg_name], norm)\n","        self.classifier = nn.Linear(512 if vgg_name != 'VGGS' else 128, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def embed(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def _make_layers(self, cfg, norm):\n","        layers = []\n","        in_channels = self.channel\n","        for ic, x in enumerate(cfg):\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=3 if self.channel==1 and ic==0 else 1),\n","                           nn.GroupNorm(x, x, affine=True) if norm=='instancenorm' else nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","\n","def VGG11(channel, num_classes):\n","    return VGG('VGG11', channel, num_classes)\n","def VGG11BN(channel, num_classes):\n","    return VGG('VGG11', channel, num_classes, norm='batchnorm')\n","def VGG13(channel, num_classes):\n","    return VGG('VGG13', channel, num_classes)\n","def VGG16(channel, num_classes):\n","    return VGG('VGG16', channel, num_classes)\n","def VGG19(channel, num_classes):\n","    return VGG('VGG19', channel, num_classes)\n","\n","\n","''' ResNet_AP '''\n","# The conv(stride=2) is replaced by conv(stride=1) + avgpool(kernel_size=2, stride=2)\n","\n","class BasicBlock_AP(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(BasicBlock_AP, self).__init__()\n","        self.norm = norm\n","        self.stride = stride\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n","                nn.AvgPool2d(kernel_size=2, stride=2), # modification\n","                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        if self.stride != 1: # modification\n","            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck_AP(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(Bottleneck_AP, self).__init__()\n","        self.norm = norm\n","        self.stride = stride\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # modification\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=1, bias=False),\n","                nn.AvgPool2d(kernel_size=2, stride=2),  # modification\n","                nn.GroupNorm(self.expansion * planes, self.expansion * planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        if self.stride != 1: # modification\n","            out = F.avg_pool2d(out, kernel_size=2, stride=2)\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet_AP(nn.Module):\n","    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n","        super(ResNet_AP, self).__init__()\n","        self.in_planes = 64\n","        self.norm = norm\n","\n","        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.classifier = nn.Linear(512 * block.expansion * 3 * 3 if channel==1 else 512 * block.expansion * 4 * 4, num_classes)  # modification\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride, self.norm))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, kernel_size=1, stride=1) # modification\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","def ResNet18BN_AP(channel, num_classes):\n","    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n","\n","def ResNet18_AP(channel, num_classes):\n","    return ResNet_AP(BasicBlock_AP, [2,2,2,2], channel=channel, num_classes=num_classes)\n","\n","\n","''' ResNet '''\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(BasicBlock, self).__init__()\n","        self.norm = norm\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n","        super(Bottleneck, self).__init__()\n","        self.norm = norm\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.norm = norm\n","\n","        self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.classifier = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride, self.norm))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def embed(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        return out\n","\n","\n","def ResNet18BN(channel, num_classes):\n","    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes, norm='batchnorm')\n","\n","def ResNet18(channel, num_classes):\n","    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes)\n","\n","def ResNet34(channel, num_classes):\n","    return ResNet(BasicBlock, [3,4,6,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet50(channel, num_classes):\n","    return ResNet(Bottleneck, [3,4,6,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet101(channel, num_classes):\n","    return ResNet(Bottleneck, [3,4,23,3], channel=channel, num_classes=num_classes)\n","\n","def ResNet152(channel, num_classes):\n","    return ResNet(Bottleneck, [3,8,36,3], channel=channel, num_classes=num_classes)"]},{"cell_type":"markdown","metadata":{"id":"2FQSh2D_jiC-"},"source":["### Utils.py"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4706,"status":"ok","timestamp":1729642585107,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"eKjR99HKjj46"},"outputs":[],"source":["import time\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torchvision import datasets, transforms\n","from scipy.ndimage import rotate as scipyrotate\n","import sys\n","#from networks import MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n","#import MHISTDataset\n","\n","\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","def get_attention(feature_set, param=0, exp=4, norm='l2'):\n","    if param==0:\n","        attention_map = torch.sum(torch.abs(feature_set), dim=1)\n","\n","    elif param ==1:\n","        attention_map =  torch.sum(torch.abs(feature_set)**exp, dim=1)\n","\n","    elif param == 2:\n","        attention_map =  torch.max(torch.abs(feature_set)**exp, dim=1)\n","\n","    if norm == 'l2':\n","        # Dimension: [B x (H*W)] -- Vectorized\n","        vectorized_attention_map =  attention_map.view(feature_set.size(0), -1)\n","        normalized_attention_maps = F.normalize(vectorized_attention_map, p=2.0)\n","\n","    elif norm == 'fro':\n","        # Dimension: [B x H x W] -- Un-Vectorized\n","        un_vectorized_attention_map =  attention_map\n","        # Dimension: [B]\n","        fro_norm = torch.sum(torch.sum(torch.abs(attention_map)**2, dim=1), dim=1)\n","        # Dimension: [B x H x W] -- Un-Vectorized)\n","        normalized_attention_maps = un_vectorized_attention_map / fro_norm.unsqueeze(dim=-1).unsqueeze(dim=-1)\n","    elif norm == 'l1':\n","        # Dimension: [B x (H*W)] -- Vectorized\n","        vectorized_attention_map =  attention_map.view(feature_set.size(0), -1)\n","        normalized_attention_maps = F.normalize(vectorized_attention_map, p=1.0)\n","\n","    elif norm =='none':\n","        normalized_attention_maps = attention_map\n","\n","    elif norm == 'none-vectorized':\n","        normalized_attention_maps =  attention_map.view(feature_set.size(0), -1)\n","\n","    return normalized_attention_maps\n","\n","def get_dataset(dataset, data_path):\n","    if dataset == 'MNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.1307]\n","        std = [0.3081]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","\n","    elif dataset == 'FashionMNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.2861]\n","        std = [0.3530]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'SVHN':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4377, 0.4438, 0.4728]\n","        std = [0.1980, 0.2010, 0.1970]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.SVHN(data_path, split='train', download=True, transform=transform)  # no augmentation\n","        dst_test = datasets.SVHN(data_path, split='test', download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","\n","    elif dataset == 'CIFAR10':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4914, 0.4822, 0.4465]\n","        std = [0.2023, 0.1994, 0.2010]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'CIFAR100':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 100\n","        mean = [0.5071, 0.4866, 0.4409]\n","        std = [0.2673, 0.2564, 0.2762]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","\n","    elif dataset == 'TinyImageNet':\n","        channel = 3\n","        im_size = (64, 64)\n","        num_classes = 200\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","        data = torch.load(os.path.join(data_path, 'tinyimagenet.pt'), map_location='cpu')\n","\n","        class_names = data['classes']\n","\n","        images_train = data['images_train']\n","        labels_train = data['labels_train']\n","        images_train = images_train.detach().float() / 255.0\n","        labels_train = labels_train.detach()\n","        for c in range(channel):\n","            images_train[:,c] = (images_train[:,c] - mean[c])/std[c]\n","        dst_train = TensorDataset(images_train, labels_train)  # no augmentation\n","\n","        images_val = data['images_val']\n","        labels_val = data['labels_val']\n","        images_val = images_val.detach().float() / 255.0\n","        labels_val = labels_val.detach()\n","\n","        for c in range(channel):\n","            images_val[:, c] = (images_val[:, c] - mean[c]) / std[c]\n","\n","        dst_test = TensorDataset(images_val, labels_val)  # no augmentation\n","\n","    elif dataset == 'MHIST':\n","        channel = 3\n","        im_size = (224, 224)\n","        num_classes = 2\n","        class_names = ['HP', 'SSA']\n","        data_path = '../mhist_dataset/images-split'\n","        to_tensor = transforms.ToTensor()\n","\n","        dst_train = MHISTDataset.MHISTDataset(data_path, train=True, transform=to_tensor)\n","        dst_test = MHISTDataset.MHISTDataset(data_path, train=False, transform=to_tensor)\n","\n","        # calculate mean and std\n","        mean = [0.5, 0.5, 0.5]  # Adjust these values based on dataset statistics\n","        std = [0.1, 0.1, 0.1]  # Adjust these values based on dataset statistics\n","\n","    else:\n","        exit('unknown dataset: %s'%dataset)\n","\n","\n","    testloader = torch.utils.data.DataLoader(dst_test, batch_size=64, shuffle=False, num_workers=0)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=64, shuffle=True, num_workers=0)\n","    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, trainloader\n","\n","\n","\n","class TensorDataset(Dataset):\n","    def __init__(self, images, labels): # images: n x c x h x w tensor\n","        self.images = images.detach().float()\n","        self.labels = labels.detach()\n","\n","    def __getitem__(self, index):\n","        return self.images[index], self.labels[index]\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","\n","\n","def get_default_convnet_setting():\n","    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n","    return net_width, net_depth, net_act, net_norm, net_pooling\n","\n","\n","\n","def get_network(model, channel, num_classes, im_size=(32, 32)):\n","    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n","    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n","\n","    if model == 'MLP':\n","        net = MLP(channel=channel, num_classes=num_classes)\n","    elif model == 'ConvNet':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'LeNet':\n","        net = LeNet(channel=channel, num_classes=num_classes)\n","    elif model == 'AlexNet':\n","        net = AlexNet(channel=channel, num_classes=num_classes)\n","    elif model == 'AlexNetBN':\n","        net = AlexNetBN(channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11':\n","        net = VGG11( channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11BN':\n","        net = VGG11BN(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18':\n","        net = ResNet18(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN_AP':\n","        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN':\n","        net = ResNet18BN(channel=channel, num_classes=num_classes)\n","\n","    elif model == 'ConvNetD1':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD2':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD3':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD4':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD7':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=7, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW32':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW64':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW128':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetW256':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetAS':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetAR':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetAL':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetASwish':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetASwishBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetNN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetLN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetIN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetGN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling, im_size=im_size)\n","\n","    elif model == 'ConvNetNP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none', im_size=im_size)\n","    elif model == 'ConvNetMP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling', im_size=im_size)\n","    elif model == 'ConvNetAP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling', im_size=im_size)\n","\n","    else:\n","        net = None\n","        exit('unknown model: %s'%model)\n","\n","    gpu_num = torch.cuda.device_count()\n","    if gpu_num>0:\n","        device = 'cuda'\n","        if gpu_num>1:\n","            net = nn.DataParallel(net)\n","    else:\n","        device = 'cpu'\n","    net = net.to(device)\n","\n","    return net\n","\n","\n","\n","def get_time():\n","    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n","\n","\n","\n","def distance_wb(gwr, gws):\n","    shape = gwr.shape\n","    if len(shape) == 4: # conv, out*in*h*w\n","        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n","        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n","    elif len(shape) == 3:  # layernorm, C*h*w\n","        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n","        gws = gws.reshape(shape[0], shape[1] * shape[2])\n","    elif len(shape) == 2: # linear, out*in\n","        tmp = 'do nothing'\n","    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n","        gwr = gwr.reshape(1, shape[0])\n","        gws = gws.reshape(1, shape[0])\n","        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n","\n","    dis_weight = torch.sum(1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001))\n","    dis = dis_weight\n","    return dis\n","\n","\n","\n","def match_loss(gw_syn, gw_real, args):\n","    dis = torch.tensor(0.0).to(args.device)\n","\n","    if args.dis_metric == 'ours':\n","        for ig in range(len(gw_real)):\n","            gwr = gw_real[ig]\n","            gws = gw_syn[ig]\n","            dis += distance_wb(gwr, gws)\n","\n","    elif args.dis_metric == 'mse':\n","        gw_real_vec = []\n","        gw_syn_vec = []\n","        for ig in range(len(gw_real)):\n","            gw_real_vec.append(gw_real[ig].reshape((-1)))\n","            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n","        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n","        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n","        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n","\n","    elif args.dis_metric == 'cos':\n","        gw_real_vec = []\n","        gw_syn_vec = []\n","        for ig in range(len(gw_real)):\n","            gw_real_vec.append(gw_real[ig].reshape((-1)))\n","            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n","        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n","        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n","        dis = 1 - torch.sum(gw_real_vec * gw_syn_vec, dim=-1) / (torch.norm(gw_real_vec, dim=-1) * torch.norm(gw_syn_vec, dim=-1) + 0.000001)\n","\n","    else:\n","        exit('unknown distance function: %s'%args.dis_metric)\n","\n","    return dis\n","\n","\n","\n","def get_loops(ipc):\n","    # Get the two hyper-parameters of outer-loop and inner-loop.\n","    # The following values are empirically good.\n","    if ipc == 1:\n","        outer_loop, inner_loop = 1, 1\n","    elif ipc == 10:\n","        outer_loop, inner_loop = 10, 50\n","    elif ipc == 20:\n","        outer_loop, inner_loop = 20, 25\n","    elif ipc == 30:\n","        outer_loop, inner_loop = 30, 20\n","    elif ipc == 40:\n","        outer_loop, inner_loop = 40, 15\n","    elif ipc == 50:\n","        outer_loop, inner_loop = 50, 10\n","    else:\n","        outer_loop, inner_loop = 0, 0\n","        exit('loop hyper-parameters are not defined for %d ipc'%ipc)\n","    return outer_loop, inner_loop\n","\n","\n","\n","def epoch(mode, dataloader, net, optimizer, criterion, args, aug):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(args.device)\n","    criterion = criterion.to(args.device)\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(args.device)\n","        if aug:\n","            if args.dsa:\n","                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n","            else:\n","                pass\n","                #img = augment(img, args.dc_aug_param, device=args.device)\n","        lab = datum[1].long().to(args.device)\n","        n_b = lab.shape[0]\n","\n","        output = net(img)\n","        loss = criterion(output, lab)\n","        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","\n","        loss_avg += loss.item()*n_b\n","        acc_avg += acc\n","        num_exp += n_b\n","\n","        if mode == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","\n","\n","def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args):\n","    net = net.to(args.device)\n","    images_train = images_train.to(args.device)\n","    labels_train = labels_train.to(args.device)\n","    lr = float(args.lr_net)\n","    Epoch = int(args.epoch_eval_train)\n","    lr_schedule = [Epoch//2+1]\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","    criterion = nn.CrossEntropyLoss().to(args.device)\n","\n","    dst_train = TensorDataset(images_train, labels_train)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","    start = time.time()\n","    for ep in range(Epoch+1):\n","        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug = True)\n","        if ep in lr_schedule:\n","            lr *= 0.1\n","            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","\n","    time_train = time.time() - start\n","    loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, aug = False)\n","    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n","\n","    return net, acc_train, acc_test\n","\n","\n","\n","def augment(images, dc_aug_param, device):\n","    # This can be sped up in the future.\n","\n","    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n","        scale = dc_aug_param['scale']\n","        crop = dc_aug_param['crop']\n","        rotate = dc_aug_param['rotate']\n","        noise = dc_aug_param['noise']\n","        strategy = dc_aug_param['strategy']\n","\n","        shape = images.shape\n","        mean = []\n","        for c in range(shape[1]):\n","            mean.append(float(torch.mean(images[:,c])))\n","\n","        def cropfun(i):\n","            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n","            for c in range(shape[1]):\n","                im_[c] = mean[c]\n","            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n","            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n","            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n","\n","        def scalefun(i):\n","            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n","            mhw = max(h, w, shape[2], shape[3])\n","            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n","            r = int((mhw - h) / 2)\n","            c = int((mhw - w) / 2)\n","            im_[:, r:r + h, c:c + w] = tmp\n","            r = int((mhw - shape[2]) / 2)\n","            c = int((mhw - shape[3]) / 2)\n","            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n","\n","        def rotatefun(i):\n","            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n","            r = int((im_.shape[-2] - shape[-2]) / 2)\n","            c = int((im_.shape[-1] - shape[-1]) / 2)\n","            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n","\n","        def noisefun(i):\n","            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n","\n","\n","        augs = strategy.split('_')\n","\n","        for i in range(shape[0]):\n","            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n","            if choice == 'crop':\n","                cropfun(i)\n","            elif choice == 'scale':\n","                scalefun(i)\n","            elif choice == 'rotate':\n","                rotatefun(i)\n","            elif choice == 'noise':\n","                noisefun(i)\n","\n","    return images\n","\n","\n","\n","def get_daparam(dataset, model, model_eval, ipc):\n","    # We find that augmentation doesn't always benefit the performance.\n","    # So we do augmentation for some of the settings.\n","\n","    dc_aug_param = dict()\n","    dc_aug_param['crop'] = 4\n","    dc_aug_param['scale'] = 0.2\n","    dc_aug_param['rotate'] = 45\n","    dc_aug_param['noise'] = 0.001\n","    dc_aug_param['strategy'] = 'none'\n","\n","    if dataset == 'MNIST':\n","        dc_aug_param['strategy'] = 'crop_scale_rotate'\n","\n","    if model_eval in ['ConvNetBN']: # Data augmentation makes model training with Batch Norm layer easier.\n","        dc_aug_param['strategy'] = 'crop_noise'\n","\n","    return dc_aug_param\n","\n","\n","def get_eval_pool(eval_mode, model, model_eval):\n","    if eval_mode == 'M': # multiple architectures\n","        model_eval_pool = ['MLP', 'ConvNet', 'LeNet', 'AlexNet', 'VGG11', 'ResNet18']\n","    elif eval_mode == 'B':  # multiple architectures with BatchNorm for DM experiments\n","        model_eval_pool = ['ConvNetBN', 'ConvNetASwishBN', 'AlexNetBN', 'VGG11BN', 'ResNet18BN']\n","    elif eval_mode == 'W': # ablation study on network width\n","        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n","    elif eval_mode == 'D': # ablation study on network depth\n","        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n","    elif eval_mode == 'A': # ablation study on network activation function\n","        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL', 'ConvNetASwish']\n","    elif eval_mode == 'P': # ablation study on network pooling layer\n","        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n","    elif eval_mode == 'N': # ablation study on network normalization layer\n","        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n","    elif eval_mode == 'S': # itself\n","        if 'BN' in model:\n","            print('Attention: Here I will replace BN with IN in evaluation, as the synthetic set is too small to measure BN hyper-parameters.')\n","        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n","    elif eval_mode == 'SS':  # itself\n","        model_eval_pool = [model]\n","    else:\n","        model_eval_pool = [model_eval]\n","    return model_eval_pool\n","\n","\n","class ParamDiffAug():\n","    def __init__(self):\n","        self.aug_mode = 'S' #'multiple or single'\n","        self.prob_flip = 0.5\n","        self.ratio_scale = 1.2\n","        self.ratio_rotate = 15.0\n","        self.ratio_crop_pad = 0.125\n","        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n","        self.brightness = 1.0\n","        self.saturation = 2.0\n","        self.contrast = 0.5\n","\n","\n","def set_seed_DiffAug(param):\n","    if param.latestseed == -1:\n","        return\n","    else:\n","        torch.random.manual_seed(param.latestseed)\n","        param.latestseed += 1\n","\n","\n","def DiffAugment(x, strategy='', seed = -1, param = None):\n","    if strategy == 'None' or strategy == 'none' or strategy == '':\n","        return x\n","\n","    if seed == -1:\n","        param.Siamese = False\n","    else:\n","        param.Siamese = True\n","\n","    param.latestseed = seed\n","\n","    if strategy:\n","        if param.aug_mode == 'M': # original\n","            for p in strategy.split('_'):\n","                for f in AUGMENT_FNS[p]:\n","                    x = f(x, param)\n","        elif param.aug_mode == 'S':\n","            pbties = strategy.split('_')\n","            set_seed_DiffAug(param)\n","            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n","            for f in AUGMENT_FNS[p]:\n","                x = f(x, param)\n","        else:\n","            exit('unknown augmentation mode: %s'%param.aug_mode)\n","        x = x.contiguous()\n","    return x\n","\n","\n","# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n","def rand_scale(x, param):\n","    # x>1, max scale\n","    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n","    ratio = param.ratio_scale\n","    set_seed_DiffAug(param)\n","    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    set_seed_DiffAug(param)\n","    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    theta = [[[sx[i], 0,  0],\n","            [0,  sy[i], 0],] for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.Siamese: # Siamese augmentation:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape).to(x.device)\n","    x = F.grid_sample(x, grid)\n","    return x\n","\n","\n","def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n","    ratio = param.ratio_rotate\n","    set_seed_DiffAug(param)\n","    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n","    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n","        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.Siamese: # Siamese augmentation:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape).to(x.device)\n","    x = F.grid_sample(x, grid)\n","    return x\n","\n","\n","def rand_flip(x, param):\n","    prob = param.prob_flip\n","    set_seed_DiffAug(param)\n","    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n","    if param.Siamese: # Siamese augmentation:\n","        randf[:] = randf[0]\n","    return torch.where(randf < prob, x.flip(3), x)\n","\n","\n","def rand_brightness(x, param):\n","    ratio = param.brightness\n","    set_seed_DiffAug(param)\n","    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        randb[:] = randb[0]\n","    x = x + (randb - 0.5)*ratio\n","    return x\n","\n","\n","def rand_saturation(x, param):\n","    ratio = param.saturation\n","    x_mean = x.mean(dim=1, keepdim=True)\n","    set_seed_DiffAug(param)\n","    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        rands[:] = rands[0]\n","    x = (x - x_mean) * (rands * ratio) + x_mean\n","    return x\n","\n","\n","def rand_contrast(x, param):\n","    ratio = param.contrast\n","    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n","    set_seed_DiffAug(param)\n","    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        randc[:] = randc[0]\n","    x = (x - x_mean) * (randc + ratio) + x_mean\n","    return x\n","\n","\n","def rand_crop(x, param):\n","    # The image is padded on its surrounding and then cropped.\n","    ratio = param.ratio_crop_pad\n","    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        translation_x[:] = translation_x[0]\n","        translation_y[:] = translation_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n","    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n","    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n","    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n","    return x\n","\n","\n","def rand_cutout(x, param):\n","    ratio = param.ratio_cutout\n","    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    if param.Siamese:  # Siamese augmentation:\n","        offset_x[:] = offset_x[0]\n","        offset_y[:] = offset_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n","    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n","    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n","    mask[grid_batch, grid_x, grid_y] = 0\n","    x = x * mask.unsqueeze(1)\n","    return x\n","\n","\n","AUGMENT_FNS = {\n","    'color': [rand_brightness, rand_saturation, rand_contrast],\n","    'crop': [rand_crop],\n","    'cutout': [rand_cutout],\n","    'flip': [rand_flip],\n","    'scale': [rand_scale],\n","    'rotate': [rand_rotate],\n","}"]},{"cell_type":"markdown","metadata":{"id":"PbCoX2FdkFx5"},"source":["### dataDAM.py"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1729642724305,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"DuDxIsufkD5z","outputId":"f24a4ea4-f7fa-4fca-dd33-838da8971773"},"outputs":[],"source":["\"\"\"\n","https://github.com/DataDistillation/DataDAM/\n","\n","A. Sajedi, S. Khaki, E. Amjadian, L. Z. Liu, Y. A. Lawryshyn, and K. N. Plataniotis,\n","DataDAM: Efficient Dataset Distillation with Attention Matching. 2023. [Online].\n","Available: https://arxiv.org/abs/2310.00093\n","\"\"\"\n","\n","import os\n","import time\n","import copy\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","\n","#from utils import get_dataset, get_network, get_eval_pool, evaluate_synset, get_time, DiffAugment, ParamDiffAug, get_attention\n","\n","def dataDAM():\n","    \n","    parser = argparse.ArgumentParser(description='DataDAM')\n","    parser.add_argument('--dataset', default='MNIST', type=str, help='dataset name')\n","    parser.add_argument('--data_path', default='../datasets', type=str, help='data path')\n","    parser.add_argument('--model', default='ConvNet', type=str, help='model name')\n","    parser.add_argument('--K', default=100, type=int, help='Number of random weight initializations')\n","    parser.add_argument('--T', default=10, type=int, help='Number of iterations')\n","    parser.add_argument('--eta_S', default=0.1, type=int, help='earning rate for the condensed samples')\n","    parser.add_argument('--eta_theta', default=0.01, type=int, help='earning rate for the model')\n","    parser.add_argument('--zeta_S', default=1, type=int, help='Number of optimization steps for condensed samples')\n","    parser.add_argument('--zeta_theta', default=50, type=int, help='Number of optimization steps for the model')\n","    parser.add_argument('--batch_real', default=256, type=int, help='Batch size for real data')\n","    parser.add_argument('--batch_train', default=256, type=int, help='Batch size for training networks')\n","    parser.add_argument('--ipc', default=10, type=int, help='Images per class')\n","    parser.add_argument('--init', default='real', type=str, help='Initialization of synthetic dataset')\n","    \n","    args = parser.parse_args()\n","    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    args.dis_metric = 'ours'\n","    \n","    # Create a list of tuples with argument names and their values\n","    arguments = [\n","        ('dataset', args.dataset),\n","        ('data_path', args.data_path),\n","        ('model', args.model),\n","        ('K', args.K),\n","        ('T', args.T),\n","        ('eta_S', args.eta_S),\n","        ('eta_theta', args.eta_theta),\n","        ('zeta_S', args.zeta_S),\n","        ('zeta_theta', args.zeta_theta),\n","        ('batch_real', args.batch_real),\n","        ('batch_train', args.batch_train),\n","        ('ipc', args.ipc),\n","        ('init', args.init),\n","        ('device', args.device)\n","    ]\n","\n","    # create pandas dataframe of args\n","    print(arguments)\n","    \n","    \n","    (\n","        channel,\n","        im_size,\n","        num_classes,\n","        class_names,\n","        mean,\n","        std,\n","        train_dataset,\n","        test_dataset,\n","        test_datasetLoader,\n","        train_datasetLoader,\n","        ) = get_dataset(args.dataset, args.data_path)\n","\n","    ''' organize the real dataset '''\n","    images_all = []\n","    labels_all = []\n","    indices_class = [[] for c in range(num_classes)]\n","    images_all = [torch.unsqueeze(train_dataset[i][0], dim=0) for i in range(len(train_dataset))]\n","    labels_all = [train_dataset[i][1] for i in range(len(train_dataset))]\n","    indices_class = [[] for c in range(num_classes)]\n","    for i, lab in enumerate(labels_all):\n","        indices_class[lab].append(i)\n","    images_all = torch.cat(images_all, dim=0).to(args.device)\n","    labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n","\n","    def get_images(c, n): # get random n images from class c\n","        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n","        return images_all[idx_shuffle]\n","        \n","    ''' initialize the synthetic data '''\n","    image_syn = torch.randn(\n","        size=(num_classes * args.ipc, channel, im_size[0], im_size[1]),\n","        dtype=torch.float,\n","        requires_grad=True,\n","        device=args.device,\n","    )\n","    label_syn = torch.tensor(\n","        [c for c in range(num_classes) for _ in range(args.ipc)],\n","        dtype=torch.long,\n","        device=args.device,\n","    )\n","    \n","    if (args.init == 'real'):\n","        for c in range(num_classes):\n","            image_syn.data[c * args.ipc: (c + 1) * args.ipc] = get_images(c, args.ipc).detach().data\n","    data_save = []\n","    \n","    data_save.append([\n","        image_syn.detach().cpu().numpy(),\n","        label_syn.detach().cpu().numpy(),\n","    ])\n","    ''' Optimizer for the synthetic data '''\n","    optimizer_img = torch.optim.SGD([image_syn], lr=args.eta_S, momentum=0.5)\n","    optimizer_img.zero_grad()\n","    criterion = torch.nn.CrossEntropyLoss().to(args.device)\n","    \n","    total_steps = args.K * args.T * args.zeta_theta\n","    \n","    for k in range(args.K):\n","        \n","        ''' initialize the model '''\n","        net = get_network(args.model, channel, num_classes, im_size).to(args.device)\n","        net.train()\n","        net_parameters = list(net.parameters())\n","        \n","        ''' Optimizer for the model '''\n","        optimizer_net = torch.optim.SGD(\n","            net_parameters,\n","            lr=args.eta_theta\n","        )\n","        optimizer_net.zero_grad()\n","        loss_avg = 0\n","        \n","        for it in range (args.T):\n","            loss = torch.tensor(0.0, device=args.device)\n","            \n","            ''' Update synthetic data samples '''\n","            for c in range (num_classes):\n","                img_real = get_images(c, args.batch_real)\n","                lab_real = torch.ones(img_real.shape[0], dtype=torch.long, device=args.device) * c\n","                \n","                img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n","                lab_syn = torch.ones((args.ipc,), device=args.device, dtype=torch.long) * c\n","                \n","                output_real = net(img_real)\n","                loss_real = criterion(output_real, lab_real)\n","                \n","                gw_real = torch.autograd.grad(loss_real, net_parameters)\n","                gw_real = list((_.detach().clone() for _ in gw_real))\n","                \n","                output_syn = net(img_syn)\n","                loss_syn = criterion(output_syn, lab_syn)\n","                gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n","                \n","                loss += match_loss(gw_syn, gw_real, args)\n","\n","            \n","            optimizer_img.zero_grad()\n","            loss.backward()\n","            optimizer_img.step()\n","            \n","            ''' Prepare the synthetic data for the next iteration '''\n","            img_syn_train = copy.deepcopy(image_syn.detach())\n","            label_syn_train = copy.deepcopy(label_syn.detach())\n","            dst_syn_train = TensorDataset(img_syn_train, label_syn_train)\n","            syn_train_loader = torch.utils.data.DataLoader(\n","                dst_syn_train,\n","                batch_size=args.batch_train,\n","                shuffle=True,\n","                num_workers=0,\n","            )\n","            \n","            ''' Train model using synthetic data '''\n","            for z in range(args.zeta_theta):\n","                loss_avg, acc_avg = epoch(\n","                    'train',\n","                    syn_train_loader,\n","                    net,\n","                    optimizer_net,\n","                    criterion,\n","                    args,\n","                    aug=False,\n","                )\n","    data_save.append([\n","        image_syn.detach().cpu().numpy(),\n","        label_syn.detach().cpu().numpy(),\n","    ])\n","    torch.save(data_save, 'data.pt')"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('dataset', 'MNIST'), ('data_path', '../datasets'), ('model', 'ConvNet'), ('K', 20), ('T', 10), ('eta_S', 0.1), ('eta_theta', 0.01), ('zeta_S', 1), ('zeta_theta', 50), ('batch_real', 256), ('batch_train', 256), ('ipc', 10), ('init', 'real'), ('device', device(type='cuda'))]\n"]}],"source":["sys.argv = [\n","    'dataDAM',\n","    '--dataset', 'MNIST',\n","    '--data_path', '../datasets',\n","    '--model', 'ConvNet',\n","    '--K', '20'\n","]\n","dataDAM()"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(100, 1, 28, 28) (100,)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 2000x500 with 10 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABiEAAACtCAYAAADWI9yPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXBElEQVR4nO29Z9BmVZW3v32dGQUlNE03dEOTmxwaA404kkEJSgYVHFTEgDCKIBjGqRkttSyigEqBiAkVBQQUVJKggJKRIKFJDU1HmigKM+Pw//J/p+asdfU8a276vg8vdV3f9nr2OfcOa6+9zzn1rN8rXnjhhReaiIiIiIiIiIiIiIjIYub/9N0AERERERERERERERF5eeJHCBERERERERERERERGQp+hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGh4EcIEREREREREREREREZCn6EEBERERERERERERGRoeBHCBERERERERERERERGQp+hBARERERERERERERkaHgRwgRERERERERERERERkKfoQo8Pzzz7ejjz66TZ48uS2xxBJt+vTp7dJLL+27WfIyRp+TPtDvZNToc9IH+p30gX4no0afkz7Q76QP9DsZNfrcYPgRosB73/vedvzxx7f999+/ffWrX22vfOUr284779yuvvrqvpsmL1P0OekD/U5GjT4nfaDfSR/odzJq9DnpA/1O+kC/k1Gjzw3GK1544YUX+m7ES5nrr7++TZ8+vR1zzDHtyCOPbK219txzz7UNN9ywTZw4sV177bU9t1Bebuhz0gf6nYwafU76QL+TPtDvZNToc9IH+p30gX4no0afGxz/E2IMzjnnnPbKV76yffCDH/wv26tf/ep20EEHtd/97nftkUce6bF18nJEn5M+0O9k1Ohz0gf6nfSBfiejRp+TPtDvpA/0Oxk1+tzg+BFiDG655Za29tprt6WXXrpj32yzzVprrd166609tEpezuhz0gf6nYwafU76QL+TPtDvZNToc9IH+p30gX4no0afGxw/QozBnDlz2qRJk5L9/9pmz5496ibJyxx9TvpAv5NRo89JH+h30gf6nYwafU76QL+TPtDvZNToc4PjR4gx+Mtf/tJe9apXJfurX/3q//q7yOJEn5M+0O9k1Ohz0gf6nfSBfiejRp+TPtDvpA/0Oxk1+tzg+BFiDJZYYon2/PPPJ/tzzz33X38XWZzoc9IH+p2MGn1O+kC/kz7Q72TU6HPSB/qd9IF+J6NGnxscP0KMwaRJk9qcOXOS/f/aJk+ePOomycscfU76QL+TUaPPSR/od9IH+p2MGn1O+kC/kz7Q72TU6HOD40eIMZg2bVq7995729NPP92xX3fddf/1d5HFiT4nfaDfyajR56QP9DvpA/1ORo0+J32g30kf6HcyavS5wfEjxBjsvffe7a9//Ws77bTT/sv2/PPPtzPPPLNNnz69TZkypcfWycsRfU76QL+TUaPPSR/od9IH+p2MGn1O+kC/kz7Q72TU6HOD8zd9N+ClzvTp09s+++zTPv3pT7f58+e3tdZaq33nO99pDz30UDvjjDP6bp68DNHnpA/0Oxk1+pz0gX4nfaDfyajR56QP9DvpA/1ORo0+NziveOGFF17ouxEvdZ577rn2uc99rn3/+99vTzzxRNt4443bF77whfbWt76176bJyxR9TvpAv5NRo89JH+h30gf6nYwafU76QL+TPtDvZNToc4PhRwgRERERERERERERERkKakKIiIiIiIiIiIiIiMhQ8COEiIiIiIiIiIiIiIgMBT9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFDwI4SIiIiIiIiIiIiIiAwFP0KIiIiIiIiIiIiIiMhQ+JtqxZVXXjnZnn/++U75Fa94Rarzn//5n8n2f/7P2N8+KnVaa+2Vr3xlp/w3f5O79Ne//rV0r3/7t3/rlP/2b/821Xn22WfHvP8LL7yQ6iyzzDLJtsQSSyTbX/7ylzHvVbFRHRpTGpt47ate9apUZ9asWck2DCZOnJhs0c+oX3EuF1WvAl33H//xH/9jmxYF+VS8lu5F8/nv//7vnXJ1rdEaifenNsTfay37D/kKtT2u29ZqYzpv3rxkW9yst956yfba1762U6Y+LViwINlo/OO1r3nNa1KdGFtby2NGazeOIV3XWl5XtJ7JT+I+8NBDD6U61C6Kf/H+zz33XKpDtkFj3fjx45NtrDa11tqNN9445nWLg4022ijZ4pqm+EHjTf2IewvVqcSxP/3pT8lGPhbXTGt5/6Q5IR+O11F8X3LJJZON4tETTzzRKdP4vfrVr0622EcaKxpT6k+cR6pz++23J9swmDRpUrLFNVWJY4uqR/tS5V5xvKtnIfLFOMeVPXBRtgqVcRj0PELQGJMtjgO1Ye7cuYutXYti3XXXHbNO5YywKCpnquqYVepUYgGdn+hef/d3fzdmHfJfIl476PNYtQ20X8d6tKbuu+++MduwOFhttdWSLc4Lxf7qWn366afHrEPPfZWzL4037cXRf2K5tVpco3bSdbTHzp8/v1OunlEidN6huaC1VXk+ojP74mbatGnJFsexem6oPM/RdTRvcU6qv1d5HiYonsf5pXbSWqA2VN5HURtiPapD7ar0mXxuVOe6KVOmJFtcE9X3ZZX3RIO+/6vOL81LvH/1TBqfTWguq+uhUqdy1qiesWku4vMQjekoznWt8fNE5Z0dxfDKPlh5N9Za7fmtEh9ay+NN/aHr4jMqnZeo7ZV3t9UzYbxX5b1ea7wXx7MFPZfPnj37f27P//hXERERERERERERERGRAfEjhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFAoa0JQriyyVaCcm5WcYZWcvZU8uIuyxZyCzzzzTKpDeTJjTkzK517N5R1t1bzvcRyqeQ0pb1klT2OfDJoTvqIdUdVQqPj+oHmIqU4l9zX9XjXXYSU/KPl+bCtdN2i+0778jvL6PvXUU53yhAkTUp2ll1462ajvccz+/Oc/pzpLLbVUssX8gZQDn8aV4m3Mxf/ggw+OWae1PCePP/54qkNaAMstt1yyRZ0Iys9b0YmojHFrHM8jizNP+/8W8vcYn6ivNN40blF7hMaI8kPGe9H+RjbKFR19saqlE/tIOirUdspRWdEiof06rq1qPt2KxsVLjcoeO2hO1Wo+2EFzDldyNZNfEHGOaX6rZ9XYrmr+2co+Xxk/uldFA2EYVM4gVb2PioZWNV9uvI7mu/rcU9HfqOQ8rs439THaqlo6sV71XEe22O+qRt8woFhfWRM0brS/VdYT3SvuB1UfozNn9Bc6X1bypFNf6F6V3NdRh2lRbahAvl/RXarG/MUNtS32ofq8WllzFDejHlhred4qa3dRttjHqjZdZb1U9/14/2pu/spcVM8/lfPCqCB/j32t9osY9D1MRa+JqJyNaLxpzh977LH/8T6tsZZOxa+r2k+xrZX3K63VxrnPd3aDrumqFio9a1aI4z2otldrebyrz9KVs1B1HVU0xyp6UPR+iNpQec8yyJ7uf0KIiIiIiIiIiIiIiMhQ8COEiIiIiIiIiIiIiIgMBT9CiIiIiIiIiIiIiIjIUChrQlTynFVzuFbuT/k2KzkFKV9YNadgzCFJeaEpfxblcosMmkeWcoZVcpFW8jYuql68f5950olKDkHyxYoeQ/W6Sq5HmvOKLkXVVyp5jqu+H9tVzUdayXFHflfJCdiX39H4xBz01M9KHvnWcl7iSv5wsr2YHLex/aRB8fDDDydbnKOVV1451SGNgopeyfz581MdaleMT6ThQfvApEmTxmzDoPkmFweVnKfV/M6U+zpSzYkZ9zfa7+j3oo4KXUu+Quso5jQm/ZDqXhnnnPQZKNd2hM4Cf/rTn5KtkseZ2jkqBtXaGlR3alAtCaJ6pqlQyWVbzetfuZb8onqvSPXMMGg+5sVN5Xerfaqc2aq5dyPVc1Blf6uulwjtSZTjndoVY2k1T3ps+6DaK4u6ti8qz6gvJk96HN/qc+ygz32Vvbiiwdha7jedLynHNO39lbMp9WfQuFDJ5U3vMEYB+VNFL4aorEMai4pOZVWzoZKnvpojvTLf1bVX0ewiKvGJzt20rl5K70oqz9jV9x2VeDGoJkRVZ4vaFfdBmpOKTmLlfeCiiLGOYl/lfQFdR7bKOqU43SeVNVY9V1ViTeWdU1Wnt7JPUXytaAbSPFX13eJv0rMn6XrGtlfPcRUfHkSL5KUTMUVERERERERERERE5GWFHyFERERERERERERERGQo+BFCRERERERERERERESGgh8hRERERERERERERERkKJSFqUmcoyLcUhWdqYjVEFEwqCpSQoJeUXx2vfXWS3VIEPOJJ57olKntJGxEAiTxWhJmITGT2MeKAHJruc+t5TGsCpf0RVX4jsayIshMYitxDkicjYRpBhW9qwgf0vxSf2jdxmvJh8lfK2KbVf+J9+pLXInEraJ4La0b8oFB+1ARJyK/pzaQbe7cuZ1yVSz2mWee6ZRJWLgifN5aawsXLuyUaazIV2MspXuToHVlXqtiqMOA1knsPwlNkSAVjeWTTz7ZKdMeSOs+2iZOnJjqkOh3RZg6+lNrPOfRz8in6V6VvXKllVYas52t5f68GFHkeO0ggl6Li4rQJ9UhG/U/+k9FqJOoiijSnA8qZjeo0GRF0LgqIFoRk66KScZ6VfHFxQ2NdfSBirhga4PH7Mo+RXWqIoRxD6frKmKbtDYoPlXOrrQ2KE7Hsaf9lKicd/oUqq6chwcV724tzzFdN2iso/2N1kM8y1fHO84dtZP6TL4f/XPppZdOdWgPr4hHUx26fxyHvmJd5UxFY1gRk24t95POuZXnMlq75F/Un3g+ozNpjGvUhup+W4m39DxcEYqnPi+77LLJRuMV1xr5eJ/E8a28E2mtdtajOEO+GO8Vn8Fa43mi9RB9kd6zUSyN8Yl8pRrrYvupnfSM8fTTTydbpDJ+reV5pP70SWxzdT+leBf7Sv5aeQ6pXkdxK84x+SvNXWwDzVNFjJvaRb5J8aciyl59lxivHeR5yf+EEBERERERERERERGRoeBHCBERERERERERERERGQp+hBARERERERERERERkaHgRwgRERERERERERERERkKZWFqoiKmSGIZJKwSRTYqwiJkIzEQEhXaZ599km3HHXfslFdYYYVUh7jooos65WOOOSbVIbERamtFVIzERuJ1VWFWEn6J81oVCR8GFRFzGqOqCFlFGIuE0KIPL7nkkqkOCRSRkG9lvGkdRWGsqgAdrdvY74r4EV1XnYvKWu5LrJXmuyIOWhFeay2LWpOfUN/jOJLPEST8FWPBiiuumOpMnTo12bbaaqtOeeutt051Tj/99GT7/e9/n2xRKJmEuih2x3GgcSehKPK5GEv7FM2kNRfXPbWPRBkpXsQ9gvyHxPaiyCDFSBLNJPHUuB5oT6K967HHHuuUaS4rgsSttTZlypROmfyHxi+2tRInWuOYH32ffm9UUJsHbQ/556BrqnLuoDVD10WfJR+rCLHSWFWp7JUVG+2n1T32pQL5V1yrVSFy2mNjvcrvEdU9lsY/7rsUZ8ifolA0zSNdR7Zx48Z1yuPHj091lltuuWSL8ZbGj9pF4xDnp889lvoR21MRLG+tJkZJeyCdc+K9FixYkOrQPFXO5FVfqexJyyyzTMkW+0i+T8Q9lQSOKebTOSLeqyKaPgwq+xHFoqpo6aDE8SBfra7xeI6m+Zg7d26yxTmi8z7NN8X8eC/qD7U9zgW1ncad2jBInWExiEhsa+yLlf5TXyvnM7o3XUfn6FiP5pfWfWxr5dmrtdbe+c53Jlt8fr/mmmtSnfnz5yfb5MmTO2UaB3quovcFcc/q8+xH/hP3N6pDz1OVuFg928UxobNdda+s1KE4EttKPr355psn24QJE5Jthx126JRJlP3MM89MtosvvrhTpmchOtvQHl55phkL/xNCRERERERERERERESGgh8hRERERERERERERERkKPgRQkREREREREREREREhoIfIUREREREREREREREZCiU1QdJJCmKUpCwSBRhpeuIinBta1l0hkRKdtppp2TbYostki2Ki5DQB4nvbLTRRp3y8ssvn+qQSAmJ1TzxxBPJFqkI9JCgFxGF6wgS7RkVJDBTETCsijJFf6HrKmKRJHJDc07+E3+T2kDiSlE8htYHrVvqTxyHqhhYFIKqiCYtyhbXX1+iXiSOFttWEThsjcV84vqle9F1MZZWRYBIoO3xxx/vlLfZZptUZ9999022KJh4ww03pDp77LFHsp1xxhnJFseZBJbvvffeZFtrrbU6ZeofrUeK53Gc+4x1FWHc6nqmPWK11VbrlGmtVgSJabxJmOv1r399skUh8zXWWCPVIeHO73znO53ytddem+rMmzcv2dZZZ51ki35AfY7isERFDLM1nrPIS01EuLLH0rhV9l0aI1p3lb3l6KOPTra99tprzHudffbZqc6JJ56YbBVROmo72aIfUMylsYlCctV9kcTlFoeQ3LCg9RShdVIRISS/rMRS2jNobum8HX2OBABJ6PLggw/ulOnsR2cUilm77rprp0x9pmeOKKpNgp9EFKj931w7CirPTrS+qF9E9GHyV/KDKEBaOYO2VhPRpmdw8uH4m+T7G2+8cbLNmDEj2WIfoz+1xueI2K5KndZYwDWOQ19+SLGn8sxHfacYGfdF2icroq/k49QGWkPRV1ZeeeVUZ9ttt022k08+uVMm8fUYD1trbZNNNkm22H7yy/PPPz/ZYtyk/sXnJfq91rJv0t40KioC0+R31WfbWI/OG5Vnmuq6pOeCyvhSH1dfffVOmeIaCQRPnDgx2Z588slOebPNNkt1aB1dcMEFnfIvfvGLVKfyPNZaTSR8VFTOY0RFNJ5stB9U3i/R+qXfo3hA7xci1Od45t9nn31Kv1d5H0e/t9VWWyXbOeec0ynTO2B6/1RpA51Vx8L/hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGhUNaEoHxTMccW5QKr5iaLuc8oHyUR82dR3sGYF7W11iZNmpRsMUfbN7/5zVTnlFNOSbaYB+vYY48t/R7lOoz9pjGt5N2n3JGUx4xyoMX8kX3mNaTcbrE9lCuV/LWqTRChvHyVPLKVfNKttTZlypROmXKpb7rppsl24IEHjtmGU089NdluvPHGZIttnT17dqpzxx13JFv8zUqO2kXVi23oK181jWNcc9QnWl+DappU8v/GXJSLuo5y+b3xjW/slCk39cKFC5MtxrpHHnkk1ZkwYUKyfe5zn0u2L37xi50y5SGkHLEx/zatT8r3SGMTbYPkNFxcUP9jPyhXdCU+tZbz2pIP03XRz0hjab311iu1K+7X999/f6kNMZfmdtttl+qcdtppyTZz5sxkW2mllTrlRx99NNUh/4m+Qet20JhVzf06DKq5xiNVLaZKPlj6vegH733ve1Od/ffff6Dfq55VY25zyq1Nv0caAQcccECnfNRRR5XaEM+OlTNoazwXsd99aeBU9rzqWqK1U7lXJd9wRcNrUcQY9fnPfz7VoXz90Z/I52jfpbzTX/va1zplWi80NrFdpCVBY9NnHKtA2gExFzXNbzVfdRzL6vqKz2uVs0prPAfjx4/vlGmtkVZS1ORac801Ux3Kd37IIYck25w5czpl8mGKWXGcadxpHMg/Y+x+KenfVGPIIFT1m2KcqWoYPvbYY8kW9+bddtst1aF5i+dGOu9TjKR7RW0H8h06Q1x22WWd8t13353q0HxVNIzI70dF5d1JFbqu0rfK8yjVodhAvhHfl1G8JY2wqE1HeydpJZHvx+fkynNza61NnTq1U6bx/NWvfpVs5NdRj6NPbUNi0NhbeYdW9en4foaetyne0bzEOaD954Mf/GCyRS0b8ovq/hbbRe/MKW6ddNJJnfKnP/3pVIfGoaLPVH3n+d/xPyFERERERERERERERGQo+BFCRERERERERERERESGgh8hRERERERERERERERkKPgRQkREREREREREREREhsJiVREjEYyqmFZFSC4K+LbW2sc+9rFOOQqutsZCzkcffXSy3XXXXZ1yFLZqLQu9tdba2972tk55tdVWS3XoXlEIu7XWLrzwwk75jDPOSHXuu+++ZIvCJSSqQ+ImUdCmtTyPfYrNkU9F0VAS4SHxHvK76GfkKzR3UdSLxpHEKT/ykY8k2y677NIpT5s2LdWhcXj66ac7ZRLoefe7351sUSCT2vrwww+nOr///e+TbcaMGZ3yueeem+qQgBf1J84PzeEoIFHaOLYVYe3WauJldF1lfKr32njjjZMt+lycx9aygG9rWcT3DW94Q6pTFe2OQkckfLT88ssnWxSgi4KKrfHaI4GpGBP7FC+Mos2tZf8hH6D4R/E/9o36Svd/3/ve1ylHkejWWOSL2hXv/9vf/nbMdraW/YeENT/xiU8k27x585ItCvtSO2kfqNShttO+GwXJKOaMCup/jCPkF7TfkDha3Csr57/WWrv11lvHvI7Gls4rJ554Yqd89tlnpzq0h8c+kih73Idba+073/lOss2aNatTpjhJbT/yyCM7ZTqPHHfccck26H4xCqifcTyqAqtE9GlalyQoHvegVVddNdXZYostko2EWGOMWnbZZVMd6uOzzz7bKd9zzz2pzr333ptsG220UbLFvTI+L7XGwoRR3Jj2ZhpTssU56zPWVaCzA8UGOk/QtRES2o37QfSBRf1enKfW8jmC/OKjH/1ossX9jNpA8xvFNltr7cEHH+yUb7/99lSHYk8UDicxWjon0ZjGeFKZm2FAcb4iHFsVr6Y4FiEB1Hh/2i/IByZOnJhsUcR85syZqQ7Fuhjbpk+fnupUz2fxLBCfVVrj2BPbTn2+4447ko32othHOp+MCvKf6CvV552Kv9Kc0DN8HF+qQ2uVbDH+/cu//EuqE8XPW8vn1AULFqQ65D/xTNpa9g36vbXXXnvMNrzpTW9KdegdIRH3hmEK349FRWi8+hxL6yf6LD17kk/FtUlnGoqlFDPib77+9a9PdcgW20XPSxR/6J1vfO9RPY/F36Q+0x5L8xOvHSTe+Z8QIiIiIiIiIiIiIiIyFPwIISIiIiIiIiIiIiIiQ8GPECIiIiIiIiIiIiIiMhT8CCEiIiIiIiIiIiIiIkOhrDhMokLRVhFIWlS9KFQyYcKEVOcLX/hCsq2//vqdMgmykGDlbbfdNmY7ScyJBEJOOumkTpkE6JZZZplk23fffZMt9ueII45IdQ499NBki0IiJJZIYi3Uxzg/JJ4yKiqivSSYQjYSCY5+R2JTZItjSeNN4uf77bdfspEITKUNce6qwlAktBNtK664Yqrz5je/Odm23XbbTpn68u1vfzvZqD/x2ooY1jAgwaLoO9T+xx9/PNkobkYqgj+tZcEiGmvyARIOjH5PYnNEFKumPpNQ1OTJk5Nt3LhxnTKNaUUsj+Itjc2TTz6ZbHGc+xJqbY37GgUXKV7TeiY/iDb6vWnTpiXb3//933fKtB+QsNu6666bbD/72c86ZRL8I3GrKLRN+zz58FprrZVs73jHOzrlb33rW6kOjV9cp5X9pDX2qSgYVhGwHxaVsx3VIaHUSv9pPD7wgQ+MeV1VlPi0005Lttj+6ty9+93v7pQ/85nPpDokmLhw4cIx20ACdLS+4zgcdNBBqU4U3m6N95B4HuhLwLDiJ+Rz5DsVv6ieJeL9SdSXnkNoLcR70V45b968ZLvllls65a9+9aupzty5c5PtsMMOS7YYg5dccslUZ5VVVkm26NPV9UJ7UaxXEUwfFuQHcUzI76qimXG/Jn+lOXjsscc6ZYoDJI4dz2OttbbBBht0ykcddVSqQ+2K+3o15n/lK19Jtigwfe2116Y6F154YbJFAWtqAz3TELE/VRHexU3lGYCo9jOuTeonrbl4xiFx76lTpyYbiZpHf7rssstSnUsvvTTZPvzhD3fKU6ZMSXUovh9wwAHJFuPr1ltvneq87nWvS7a4B9LzS5U4DrReRgXt/3Esq2K25MOV9xa0b8SzPMUiiq3rrLNOsm233Xad8sYbb5zq0P3j88qMGTNSHTpHklh1FAimd4sf+chHki2KVdP5gN6D0vNXnNc+BdEr7zMq7/UWZYt9pX2RfD/6NbWB3v3QWT6+96J3shRP45k/roXW+HmU/PMf//EfO2V6Z/fUU08lW/zN6j5PaznuUYPETv8TQkREREREREREREREhoIfIUREREREREREREREZCj4EUJERERERERERERERIZCWROCiLmkKJdczA3ZGufiirlEDzzwwFRniy22SLaYu+pd73pXqnPvvfcmG+X5irk6qe2Uq3P27Nmd8qxZs1Kd1VZbLdmOPfbYZNtrr706ZdIQ+MEPfpBsn/zkJztlyt9F80O506Jt0JyWi4NB81VTbrJKPyh3JuWkjP56yCGHpDoHH3xwslGOtjgvlFOvkqeW2kk+TLlGYy68Sj7p1rKfkb9S3trzzz8/2R588MFk6wPKaxxzJFdz/VKOwZjDkOrEmNJaHsdHHnkk1VljjTWSjXLzx7ySlEvz1FNPTbaHH364U37Pe96T6lAeR8pj+frXv75Tvvrqq1Md8rmYw5rWFNlofsaPH98pV/PODwNa49HvyFdorZJGTcxRSTldYz7p1nL+8VtvvTXV2XXXXZNtueWWS7a4X1fzg8cYecEFF6Q6lIc46jW1ln12zpw5qc7Xv/71ZIu+SL5C+UlpzmLs7jNPOv12RSuAzhOUizbuEZ/97GdTnfe+973JFmPsH//4x1Qn6nu0Vts/6Rz35S9/OdnieYzGitbtNddck2xxTOkc96UvfSnZok9VNEZaY/+MY9OXJkTlfFH1r4omGPWT9oO499NYU7ygfM4xBn/sYx9LdWgPr+jfkMbcL3/5y2SLGnPkq5tttlmy/fCHP+yUaRxoDin+9fn8EKH1W8mfTXtlRSuPciuTFkz0u6rOFekdRg0wui7mMW+ttfvvv3/M6+i5ivoTnztIm+njH/94ssV86sccc0yqQ2djinUVrY9RUFk7tMarGpv0vBIhH4+2mTNnpjrkJ3S2jvpu55xzTqpDz3c777xzp0w+d8kllyRb5fmLzgukaxi14uIzQWutPfTQQ8lGcxb3LMoLPyrojBPbTNpn1Wf/yvs/akP0FYrJlN+e4kV8Jp40aVKqQ32MsY7OYhRnaBxiTFxqqaVSnZtuuinZ4nM5adptuummyfab3/wm2SJ9PsfSGFXeKZLf0dmuouNKz1zxusr6bY2fMeKZiTQvKZbF9yekl0qaItTHGFvI76LWVGtZ14T2dNovaH7i+h7E7/xPCBERERERERERERERGQp+hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGhUFaRIBGPKBpCgiQkCkP3iiIbJJS6wgorJNvnP//5TpnEj0h4g4S/IiT4ROIcUWiH+kxCHyTiduGFF3bKJGy0/fbbJ1sUVSRhb5ofItarCmQNAxKgilREm1tjkbgorELXUf+jMCCJppNgGwm3RBv1mdoe1xGJ0X70ox9NNhJJiv2mNpCA1GmnndYpkwD7jjvumGxXXXVVst1zzz2dctVfFzckKBShGEbxgoSOYnwgEXm6f7RR/CBhLrJFQcwoPtxaTQSUBFh32WWXZFt99dWTLYqP3XDDDakOjemCBQs6ZVr/lXFvLY8pzcWoIBG3uO4pFlFsoDgW/YXml9b4jBkzOuUo5Ncai3SSWOvdd9/dKUcBy9Z4z5swYUKyRW655ZZkO/fcc5Pt0EMP7ZQpPp1//vnJFn2RYjnFTVrLcZyjAPkooTZX9l3ysUpfSYSafD/uU4cffniqQ2ua9o3Yn/e///2pzgEHHJBs0depnd///veT7fjjj0+2Sqy56667km369OmdMq21qlh1jB80h6OA2hb9pBrXB/096nt8Lthzzz1TnSgc3Rr74QUXXNApk/ArEQUGSUCbhBdJPDX2m84CUci4tdZuvfXWTpn6TGuBGHTOhgHFp9gP2gNpzZF/Vs7kNG7RRu2kmEJCu5Hzzjsv2X784x8nWxTEJFFU+r3dd9892eLeGM9srbU2b968ZItCs3QWqJyN6dqKgHNfUPurxDhWEWVtrXae2WqrrZKN3qfEeDFnzpxUh+Yyns9ITPrKK69Mtvnz5ydbjN30LFoRWI6ixa3xubuy/vt8d0I+FZ8VqmLtFMPj/Wls6Rkj3osEmemsR+//os/Snk7vGk444YROeeHChaXfo3GIz1UU12gc4nsGavtuu+2WbL/85S+TLQrI03ofFZXniRcjTB3ngMaW7h/PUfTsST689957J1scb7qO3ql861vf6pSrsYaef+M5sfo8Gp+Tqe30bFJ5VhjE7/xPCBERERERERERERERGQp+hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGhUBamJioiFCQGRQIaUXhjgw02SHWicFZrWXSLhMBIuLMiNETCPiR4svzyy3fKJML12GOPJRsJ38yePbtT/slPfpLqkGj35MmTO2USlLrsssuSLYqbtPbSES9srSZgQ+IrVeGbKHJTFbr83ve+1ymTCBYJ0JFPRQFrEhwmccIoLkdiqlEItjWe87iWqZ0kAHTRRRd1yp/85CdTHaIyFy8lIbmKiBvFQxqzeC2JYpEwUGwDCerGONAai8RFf7366qtTHRKxjCJiv/3tb1MdEkOnuEx+HiE/jOKFdG+yRTGp1rIgcJ8imhURy6pwNomrx9hGdX7xi18k20477dQpP/XUU6nOcsstl2wklBqFuCiuUTyPe3gUVm+Nx4/qRbHhddddN9XZcsstk+3yyy/vlGm9V+NCpLIWhgXF4jgHtA4pPse12VoWAaS+0hkj7i2zZs1KdWhsSXBujz326JRJ8I/ODHfccUenvN9++6U6dD6orD9qO81FXPPUzuqYxnnsK95VhFirgunUzziOFDfJT+K6pz0wntdaa+0b3/hGsv3sZz/rlOk8SCKy0Z9ovmkcyOciFG/j80trrS2zzDKdcnwuaY37Qz4dfawiej8s6Hwfz2j0vFgRoW0tn61pPdO4xT32Ax/4QKqzyiqrJBuJXx522GGdcvXsFWP3o48+mupEn26ttV//+tfJ9qEPfahTpmddmovow29605tSnUsuuSTZqD9xf3ox4s8vBjoTxDVBdcgPiRgvSESdnkNi/PvUpz6V6pAgauXcOH78+FSH5vu73/1up0yxnM7t1IZo22ijjVIdik8xBpPwNu2TtKfEen2e66jNcXyr77jorBeFwGl+yTZlypRO+Z//+Z9THRIxp7gZ55ziLT3bxmcYmnN6T0LtimNIdWj84h5Lc0FnFJqf+BxbfU4cFbHNtM6r796iH5Cfkx/Esdxiiy1SHXrvHP28tdZWXHHFTnnNNddMdaZNmzZmG6idFPff+MY3Jtv666/fKd9zzz2pzuOPP55ssV70w9Z4TGkPie2vCt3/d/xPCBERERERERERERERGQp+hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGh8KKEqaOQColSkLAIiV6svvrqnfLUqVNTHRKYicKT9Hvjxo1LNhKXo2sjJKgShURINIkESEjgM4rhkJDSN7/5zWQ76qijOmUSpr7wwguTjYhiSn2K3JCvRBGvqgh1RZiMfu/MM89Mtk022aRTfuKJJ1IdEnIhH4vjS4Kuu+yyS7JFgR7qH4nckE9VxMfp/hWxTRJDpHtFW19CchWBaRovEs6qiCFTnSie21oWvN9zzz1THRKmJoHBBx54oFO++OKLUx3qzzrrrNMpUwybMWNGspH4ZRTwet3rXpfq/PKXv0y26BckKkaCnxXRVxICGxU03jEe0b5FvkJrpyJKR7546aWXdsprr712qQ3kG3EdURvi+mgt70kkhE37wDXXXJNsP/7xjztlEhvecMMNk+2KK67olCsxrDUWpYsxsa9Yt6jfjraK+G9rLAYZ1zVdRzH3+9//fm7sGO1sjc8rxx9/fKe86qqrpjp33nlnskWRVxLVrYqRx9hCQnx0/og2mgs6V/TpU4NA4xih8wz5U6QqLLz33nt3yjTfFKfnz5+fbHFOBhUppTaQ+CyNw7nnntspR39ujceh4nOVszldS2tjVFA/om/QPFGbK8LpFIve/va3J9uhhx7aKdMz6+WXX55sJ5xwQrLFvTgKTrfGItfRV8gvKL7TeJ144omdchTebq216dOnJ9vChQs75f333z/V+c1vfpNs5HfxfFmJE8OAYk9lfZHPUT/j+ZfWJcWLeCavCpTS82nc26idtEfFsameDeh5It5rt912S3VuvPHGZHvsscc6ZYrlJE5LxPXfl8+1xvEpzmelTmscx6LPPv3006kOxYujjz66U6ZnNdrnKc7EOT/rrLNSnSia3lqOkXRGpz4PKjJPz+px3ZKPxTi6qHbFMezT7ypi55UzB13XWh5fmrvK+7+tt9461aF3t7RGYrtOOeWUVIfeS8RYQ3Gf3peRb8R3xdX3jdFWfbarPPtQnbH4f+spRURERERERERERERE/p/BjxAiIiIiIiIiIiIiIjIU/AghIiIiIiIiIiIiIiJDoawJQfm6Yi4pqkM5qSjH39ve9rZOmXIYUp64mAeLclpT7l3KuxX7U8n/Rr9JecxiPvfWcs7K1nIuLsorRjnXY67wHXbYIdWh3NeUCz62gfo8KmgsY96xar5fyn0W/Yd8jHLqxxyYlCeY7kVzfv3113fKn/3sZ1Md8oPYdsqNV8nP11pNZ4P6QznqI+Q/tP7iGFIMGAX0u7FtFNcoLz4RfYBy4NNYT5kypVOmWEe5AydOnJhsn/jEJzrlyny0lseGcgDGXIWtcb7Z++67r1Ou5n6N40U5RQnKKRrnkfIqjgpaJ3EOaBzJRv4T8y0TNAdxzkn/hnJMkyZEbBetNZqDOHdRm6Q19pV58+Yl2913390pU/7LuNZaY7+OULyt+CettVFBflfR+6J5quwt9Huf+tSnxrwX3ZuguBjnjtrwnve8J9mir1e1vejsGGNlNR9szAFMbaD+VPLYV7SghgGNT5xf2luqGnPRRjGLxieeZ2hdUrso9qywwgqdMp3PKs9V1E7yAVqP0Xdo/CgPcjwzRO291up5nWO9PvdYms843hUNn9b4vBfPJmuttVaqs++++yZbXA+0b33pS19KNvLruMfSPNG5PdarahvefPPNyRbXyOmnn57q0Jkh6k1Rrm06L48fPz7ZYh/7inU0ZhUtBPJD6kM8X9D4kB/us88+nTLtm7Tnkq5CjH/kX3TWmz17dqdMZ1maW4rBO+64Y6ccde9a49gTtUarGpjUnxgnaA77JPpZVTeqsgdRjDz44IOTLZ7FKCZTu8gP4ruTH/3oR6kO3T/OMfk+PUNV3iXSuxp69oz3f/zxx1OdWbNmJVtFu7Z6Vh4GlfNY5XzcGs9BvBf5Co1R1MmdNGlS6Toay/e///2dcoxjrfHaj+9nqH/0vEgaiLGt9O6H9G0q7/poLioaKYNoavqfECIiIiIiIiIiIiIiMhT8CCEiIiIiIiIiIiIiIkPBjxAiIiIiIiIiIiIiIjIU/AghIiIiIiIiIiIiIiJDoSxMTYId0UbiSiRAQuJAW2yxRadMYhnXXHNNskVRGBJ3IaEPalfsD9UhgZ4oUEZtJ7EaEsyJ9caNG5fqULtOPvnkTplEzDbaaKNk+/nPfz5mu/oUuSFie6rzRPWiqM0222yT6uy0007JFgWKSNiIRKijCFZrre29996dMq0juldsO11XFb6JYj90L/KfY489tlMmX7nggguS7Yorrki2KIZTFc0aBVFwh/yLRONIkOqxxx7rlEnwlsZ/u+22G/M6iq0kZB/Xwm233Zbq7LXXXsm26qqrdsokVnTdddcl2+677z5mWydMmJDqkJhyFM0ksbCqCGgcZ4oRo6Ii6kl+QZAfRP8k8Slav9E2derUVIfOBxQvNttss075wQcfTHVobUVfpzZQbCUxu7jHkrDwSiutlGxxLZP4Iq13GtNYj+a+T6KvkD9VRHVbyz5LexmNZbw/renKOY7aQOfEGJfpN0msleJP5cxEsZN8P9poT6e4QOu7L3HWCO3tlVhcET5vLfsA+S+NYxQEXn/99VOdhx9+ONmuuuqqZIuQcCfNd0V4sSpkH32T/ITuX9kHqQ7dP9rIL0cF/XYcb4pr1XNBjA907omC5a1lQdUbbrgh1SFxcIo96667bqdMayae41rL4pckjk0xkvoTx4vEkskW/ZrEv2mvWHHFFZMtPue8lESCox/SHFX2gtbyfkr7Cs3R008/3SnTu4abbrop2e66665ki+2n2FoRcp4yZUqqQ2LV8X1Ra61tv/32Y7aTxKqjuDud4ShGVs5sfT5PUCyO/kPnATo/UT+i7UMf+lCqQ/MZz9t0pqI2kDhvFASmfZ7iRRTxpThKeyy954lr+Zhjjkl1aH3H+5933nlj3ntR7aK40Be0LirxgdYYrcVIXL+t5XclrbW2+eabd8o0ZuQHRxxxRLJFwXB6pqFxiPevnqEodka/Jh+jvSCOffWZoOJjFdH0yEvnLZ+IiIiIiIiIiIiIiLys8COEiIiIiIiIiIiIiIgMBT9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDIWyag4JhETRCxJMIXEOEriYNGlSp0yCVNdff32ykcBghYrQEImnkPBGvFdFFLQ17mMU6SFRHepzbMN9992X6uy5557JdsoppyRbFPCqiqEOAxJbqQjbUZvJF6PgMAmI072iUOrZZ5+d6vzud78r2WK7SKipIqxIa5TGqiJOSP564IEHjnkvaieNDa2jOA59iS1RH+I4VkWoKYZEUSwS+9t4442Tbb311uuUKd5Onjw52X77298mWxRWiuLorbX25je/Odni+qiKHpIoYIxRtDaojxVRP4rvFWFCirejIvpFa7lvFSHT1lhALd6/KtS41lprdco0v8svv3yyzZkzJ9miuNyCBQtSHRKZjGsyCtK1xkLtUZyytSy4ec0116Q673rXu5JtjTXW6JRvv/32VKcquFYR4BwVtMaiYBrtGSS0e9pppyVbjBkx9rTG4nJxnVM7KeaSH3z4wx/ulL/yla+kOttss02yXXfddZ0ynb1oTVbEkqsinNH3aU+vxoU4jyRcNwpoj43rhPpEfkhignH8K7/XWp5fimF/+MMfko18k+4foTmKe1BVFJXWQtwHaF9YuHBhssX+0PhVxzRCMXJUkE/FPZb8iZ4LqN5RRx3VKdNeedFFFyXbBRdc0CnTGFFMieLCreXnBxJ0JaHLuKdSn6kNJMoZf5POGhMnTky2OD9//OMfUx0SWaa5iPei56pRQD4X1xfFNXrGIFuMWQ899FCqQ+KqUSB4/Pjxqc7MmTOTjYh7EsUUmqN4tqT4QWejww47LNmuvfbaTpnG6uqrr0626L/Uzup7rCi23qdgMJ0T4n5f7VelrzvssEOqQ/MZnxWonffff3+y0bPtLrvs0inTuwY6p958882d8tprr53qEDReX/jCFzrlVVZZpXTdhRde2ClH/22Nz2eV9zd9QueC2H8aD3onRL4xf/78Tpn8bvr06ckWYy49s9I7CIplEyZM6JTp7EXPNHHN0N5Jzy/rr79+ssVzIo3V9773vWSL40DjTvs1xYDod4Oc7V46nisiIiIiIiIiIiIiIi8r/AghIiIiIiIiIiIiIiJDwY8QIiIiIiIiIiIiIiIyFMqaEJWci5QLjHKlVvL1U34rynsWc+jGPId079Y4X1fMDUq5sij3V8xRSX0mKP94zBlGbafcljHvXcy12Fpd2yGOc7U/w6Ca771CJe/m1772tVSH8orH3Kh33HFHqkMaJlGDorWcq53aSblYY/41uo5yu9Hain5NeQ2jbktrefzuuuuuVIdsFf2KvvJpkr9T/ItQeykfZcxxuvrqq6c6lIs65gpcc801U51bbrkl2R588MFki/kQ99tvv1TniiuuSLbYduof7QOUuzjmIKYcjTH/I1HV4qB6cV4p3oyKSi5NyhFK+xT5a4wzFFMq8YLyWI4bNy7ZKrnyKd8vtSHWo3vTnMfcna3lMaX82DTO0VbJRd8az09sP103Kmgs45hU9AUWVS/27cYbb0x1brjhhmSLa5HOPTRutI6ifseRRx6Z6lAsi2NT1Smo6Gx84xvfSHWi7k9reRxOPfXUVIfyX9M5Kbahr1zC5CcRGtdKXtrW8phRnalTpyZbPMvTPvzDH/4w2cgPY9ykHPsUG6LvVGMd7V0HHXTQmPcinZOok0P9q8aE6JvU51FB55A4bhQHaGzpPBw1NygWVfLs0xjRnNOzSXyGpNhA+1s8J5I2E8Unamt8/txuu+1SHdKqiHnSST+IcsNX1ghpr/VF9As6m1KfaC4jW265ZbJRXvN43qZnU8rNTz4X36fQGY727+g7pMu0/fbbJxu954nt/9a3vpXq0PjFuaC2V+J0azkmVuZrlFTeAVV9cY899uiUK/o0reU9lc4zDzzwQLJRPKqcI1ZaaaVki3NMc05nhne+853JFvUb6bmA1t+ll17aKdMeS/sO+RQ9w/RF5bmA6lSek1rL+8ZWW22V6pDfxeffu+++O9U588wzk43WedxjqT+0X8ezBT1L03X0PjeOKWn6kK9U3rNVNFuJQfSD/U8IEREREREREREREREZCn6EEBERERERERERERGRoeBHCBERERERERERERERGQp+hBARERERERERERERkaHwooSpo+gFCa+R6AUJXCxYsKBTJlGYddddN9miyE1FDLM1FuetCCGS+EcUtSGRGBL7oT5GsSgSFiHxjyhwQuJqF198cbJVhEv6FL2piA5WBX3IFkWpqkKX0YfJp0noi9ZRbBf5JrUr2ip+0Rr3JwrqnXTSSanOaqutNmYbzjvvvFSnIuDVWh4HWjOjIAoYtZZF5EnYj+aI4lGMISSKRSLNd955Z6e8xRZbpDrExIkTk2333XfvlElEnWJI9DESZP+Hf/iHZKMYEv2cBMxpnCMkblwVMI1z1qdAMK3LCrTvUiyYN29epxzXfGtZcKu1vH5/9atfpToHHHBAstEav+mmmzplEkWl6x5++OEx66yxxhrJRsLmUViRhA9p/ObOndspU5yu7Fet5fbTuI8Kal8UR6O+0tmB7hWvrZw5qB4JtpEfVM6qJI5N94rzWRG3b60mVLf22muXrotr5LLLLkt1qgKYcZxpvkYBzXfsO60l6ifF7LhvUD8PP/zwZFtmmWU65fvuuy/VoT2PzkbRV+gcRO2ifTcSzyOtse/EuEL3jvtCa3n8yMdpPdKc0Vz3ReU5jMaI+kUizXEOaNxI3DSeYaIweGsshE3nnDh30adba+2RRx5Jtnge+PjHP57q0Lk0irm3ln2Dxo/OEWeccUanXBXuHFSgexRUzp20RshXK7GB+kmxJ9rIV0mglM5UcazJT1ZeeeVk23nnnTvlI444ItUhkeLbb7892S6//PJOefbs2akOnbHjXFCfaY+h80KcR/LVUVF5xqZ+UZtJdDoKMtNzCMWeuN/cdtttqQ61a+bMmckWYx3NL+2x0V9prdFz7L777ptsUZSdnmk+85nPJFsc00osX1S92MdBBIIXFzSWsT2VZ6JF1dthhx06ZdojKuLdZ599dqpTedfXWp4Dihlki+96xo0bl+pMnz492eidVHwmpvUX67SW93nyFbLRe6o4P7TXjYX/CSEiIiIiIiIiIiIiIkPBjxAiIiIiIiIiIiIiIjIU/AghIiIiIiIiIiIiIiJDwY8QIiIiIiIiIiIiIiIyFMoqnCRUEUUoSIiDRD1IuCQKwJHQ22GHHZZsUUSHxK5IcInETKLYCAluEVEUi8RkSKyGhJqiIA+1nQRCPv/5z3fKSy21VKpDwiVEnFeaw1ExqHBiVUgq+nVFtLm11i688MJO+dxzz011aJ5IuCX6D7WB/CD2pyrATuNw3HHHdcokmklz8ZWvfKVTvuqqq1IdEkWj/rxUBAxp/GMsIKFlmu9BxYYp3sa4QvHjta99bbKRiGUUeyO/nDVrVrJFH3vLW96S6pCAIvlF7CONX0WYkPadisBya1nAqk9halpfcYyqQne0x0ZfJB9esGBBssX5vP/++1MdWs8koLbBBht0ys8880yqQ2y44YadMvkKiVPSXhzHlOI7iSE++uijnXJFJHJRxBjcl2jmoojto9hMse20005Ltg984AOd8iabbJLqRJHD1lq7/vrrO2Va5xQnaQ1X1jX1MZ6ZqA75wete97pkO/DAAzvlqhDfoYce2inffPPNqQ71j9ZfbH9fAob0u7FtNNY0PrR3Rd+k8aG9Jd5rxRVXTHVWWmmlZHvqqaeSLfoFiQvS3hzbQPNI40d78ZQpUzrlOXPmpDok0BihuaiuR2p/X1A/YvtoTig+77LLLskWhVgpRtI55GMf+1in/OlPfzrV+eY3v5ls06ZNS7YDDjigU6Y9nfw17m/rr79+qnPLLbckG4lfxjG94YYbUp0oJNxaHi86x9GeXjln9/UcW3l3QjGsuj/EPYrenVRi6R133JHqVMRIW2ttueWW65TJxykO7Lrrrp1yFGhvjc+IX/rSl5ItCmZTfKpA+3lFnLa1HCf6FKYmoi/Svkj+Sv2IzwVVQfQ//vGPnTKNI+3NFMcidC96XonPzieccEKqQ+9AaG+IsfRTn/rUmO1srXYmpXd2tJbj2Pf5PFF95xShGEhreIsttuiUKfbTHnHyySd3yvGdc2vsdxTvoo38nGyxPw899FCqQ7GtIlp+1llnpTq0buO7geo7VvKpxfEc639CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFDwI4SIiIiIiIiIiIiIiAyFsgoniahUBIxJ9ILEjj772c92yl/84hdTHRKK/qd/+qdOOYogttbav/7rvybbgw8+mGxRACkKdbVWE8GL4mRUpzUW2okimSSC9773vS/ZJk+e3CnPnj071bnpppuSrSJWTSKno6IiMF0ROVzUvaJ/kvAQzV1sA/k5CZ4SUcCGBG2oXVGshsR4SDB28803T7ZVV121UyaBsCjG3Vpr55xzTqdMQnJV/4lzNqio84uFxj+uJ2ob+QAJHcX4R+JB1IYovEZteOSRR5Jt/PjxyRbFtEiEcOrUqckWha/XXXfdVOe6665LNop1cUzJx5dffvlki35P4lW09kg0KfrroGJ2i4PKb5OvkCAzxb+4L5G/jhs3Ltnivkh7EglskQDxtttu2ymTryxcuDDZ4m/GeNUaC22TAOf73//+TpnG6tZbb022p59+ulOmtU0+VhGZ75OKODGd7cgX6YwRhVLpfDRx4sRki2NJfk7jTXtepCrIGOtRG0gc9kc/+lGyxXVEvxdFG1vLY0pxguaH9pBIVZRucVMR0q6KetJ8x3M69TOeXVpr7aCDDuqU11prrVRnyy23TLZrr712zHZVRAJby+cDimsU//bcc89ki/0mYeq5c+cmW2Rxxit6hhoVUUC3tfwMROcq2pPOO++8ZNtpp506ZdojyBbXdDxntdbaZz7zmWQj34hxhupQDFlllVU6ZYqttFeeeuqpyRZjz4wZM1IdIu7X1T22ct7p63mC9owKtE4qIu8bb7xxsi211FLJtuyyy3bKdBajsx7tKzG2bbjhhqnOgQcemGwx/pHw+e23355sd911V7LFPYVifkXUtyIkvihinKTfGxWVeaL2VUXSo606RnE+KRZR2+lZM/aH9rJTTjkl2fbbb79OmcTPKebT+40jjjiiU66+G4jQOYbWO52d4hj2KYheefdGMZxsb37zm5MtrjEaD3oejTGD3otUnh1ay3sL+QrFkfibe+yxR+k66k88I9CzLq2j2Ef6PfLXipD6IM8T/ieEiIiIiIiIiIiIiIgMBT9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDIWyJgTleoo54ap5aSln2mOPPdYpH3rooalOzOXcWtZHWGeddVKds846K9koD9bFF1/cKVMuTcqNGvOdx760xjk+t99++2SL+UkpnySNacxJe/LJJ6c68+bNSzbK4xfbQLnaRgX5Xcx3V80RWsk9eNRRR6U6J510UrJtt912nTLlsvzpT3+abJQDOI43zQnNeWX9Udt33HHHZIv5+ElThPIeV7QxKjnHW8tz1ldew0pewAkTJpSuoxySMYaQXy655JLJNmnSpE757rvvTnW22mqrZKO8knHN3HbbbanO1ltvnWwbbLBBp0xaD5TXnHJuxnhE40C5FmPbKd8trYVKrOszhyutnTh3NJeV3O+t5TEhH6tomNDvUY5eygscc2ZTXvbHH3882eI6ojNE1GxojXO6xzMC5UmnPNdxD6ccmaR3RfVi+6u5SIcB+V1Fn4Su+8Mf/pBsN9xwQ6dMGgof/vCHk+2yyy4bsw0VHQeqR/2j+BBjzde//vVUh3yMzo5xzu+4445Uh84fcR+saFu1xrGiz/g2FnGOqK0UsyoxkfSG7rnnnmSrrEPSXrj33nuTLe6NFJ8ojkU9I9K522abbZKNYnfc36688spUp+JP1dzUtB7jtZXcwsOCzpRxvGn/oTb/+te/TraYw7rye63lMyGdX8iHScMpxqwVV1wx1SF/jXz3u99Ntp/85CfJVokzpD1IMSvqFFTz01O8jdf2pcNU0XGo5tOvaE5VxyeeqapnZmpD3NMPP/zwVId8IL4rIZ+j5056nxLbX82xH/tIdao50uNvvtT22+gr1b7SeSnOC2kl0Zy/8Y1v7JRnzpyZ6tBeSVpiUbfwxBNPTHXWWGONZIsxntYHPZvQmTSe+WlPp3UUx7Sam7/yjoXW8qig/se+VuMd+U98h0b3ivtIa/mcTuNNWqj0DqJypllttdWSbZ999umU3/Oe96Q6tDfTc2XUnSM9UHo3Es8kVS0JmtcY39SEEBERERERERERERGRlwx+hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGhUFYII+GNaCMRHrKRgFoUOyJRodNPPz3ZorjfIYcckupssskmyRZFXlvLoiEkBEYCPVGMg8RUSPCEBEjmzp2bbJHf/e53yXbTTTd1yiTsQ2NKoi5RAKgqLDwMKiLN5GNVgZQoYEOCVyQuF8fk4YcfHrNOayxUHPtIdYh4fxKhJtFMEi36wQ9+0Cn/5je/SXVISDOu28r6WFQb4r2qgruLGxIBin5RFWsnIc3YLxqLW2+9NdmikP1DDz2U6pCNRCxjPCIhchqHWbNmdcozZsxIdaLYXGssYP3EE090ylWx7xhfaW8iQSa6VxSJ7Eu8sDWOY5VYXBE+bC37MMW1JZZYItnifk0CWMcff3yy7bfffskWfYNEXh988MFkizGSRLJoHD75yU8m2/LLL98pH3PMMakO+Wscr+WWWy7VofVeEWLt0+8oPleE4yjWk6jrwQcf3CnHs0prra2++urJdt5553XKp512Wqrzi1/8Itkqwn0bbbRRqkO+uP/++3fKNC6VONlaa7fcckunfOSRR6Y6RPxNakM1BkSq+9goiG2heFgVfYx7Ku0Ht912W7LFPSmWW+PnCYp/MdZVhZxjnNl0001THRLSJG6++eZOmZ4dKlDb6dxCtrge+/Q52vPiWYjWEsV1ulcUM6Uzze67755skydP7pTpDEVxhtoQz+nf/va3U53bb7892eI6IgHt6jkxnjXomYbuXxFUpX2HznYxfvTld1FItbW87qti7dT36Jvz5s1LdaZMmZJs8Rz9kY98JNUhP4nvSVrLPhD3utY4Tl988cWd8uWXX57qVM8ncX7pLEvvZipnL5of2p/i/asCvMOAxq2yBug6emd35513dspbbbVVqkPxIgpT0/ivtNJKybbyyisnW2Wff+CBB5Itnu+//OUvpzqxf62xT8UYTHXo3BIhHyP/qZxv+/Q7iuFxrVD7qP9XXnllskU/2HzzzVMdirmf+MQnxmxn9SwfobZvu+22yRbf/TzzzDOpDq3RG2+8MdnOP//8TpmE28eNG5dsceyr70+pXuVeY+F/QoiIiIiIiIiIiIiIyFDwI4SIiIiIiIiIiIiIiAwFP0KIiIiIiIiIiIiIiMhQ8COEiIiIiIiIiIiIiIgMhbIwNYl4RDGUqphFpR6Jc5DtD3/4Q6f88Y9/PNVZYYUVkm3nnXdOtg033LBTJoEyansUgY6Ciq2xMPVTTz2VbFHAhsadBHujMBcJdVHbSegqiqyQ+NmoqAhM0zzRGFWE03/1q1+lOm95y1uS7e1vf3unvOWWW6Y6JC5MAkXRttlmm6U6JCx24IEHdsrrrrtuqhOFCVtr7eyzz062H//4x50yiUXRXERbVZy+4lN9iSuNHz8+2aKwHq0vspFYUPRXEv2i8YkCg2ussUaqQ5Afxt+MAu2t8bqKgpgkwkVi1STuFMULyU8qgn1VkWIa5ygMVRUIHAY05zH+k3BWFFqm61qriTuTUOrEiRPHvPezzz6bbL/+9a+Tbfr06Z3yrrvumurQ+ouC6/fdd1+qE4XbW+MYcu6553bKV1xxRapD+0e0VcWASUQ2+llFkHNY0LmqIjBGY0s+HO91+OGHpzpRALq11t7whjd0yqecckqq8/Of/zzZaF7iOWfatGmpzoorrphssT8kUEv74kc/+tFki0KHFGuo7VFkryr+VjlnV85Xw6AiLFrtZ0UMmfyS9uYojLreeuulOhTr6BkjxmWKKTTf8RxBv0cx5f7770+2Y489dsx7EdE3q6K+NK+DiBUOi8rzaDyXtJaf8VrjZ6eZM2d2ylEkurXWrrrqqmSLZxNqA/kP+X4Ux6V9ntZDPAOSKHv1eTT6AcXNyrMn+R2dUaheHAcSKx0Fg/o/jSvF6xhDyFep73GOojh6a3y2JNF0EkCNHHfccck2Z86cTpliMomaV87KFIvId+L6qMawilBrX2LordWep+gMQvGC+hHFx3/4wx+mOnvttVeyxXmiczsJiNMzamw/7YvXXnttssX3HSSgXRUjr5zlKyLI1fcd5MOVd7Gjgvoa/YfGltY07RGnn356p7z22munOvReIo4JiZ/THEydOnXMerRm6H1Q3FOXXnrpVOess85KtngubS3vqTRWtI6iD9N7kYqQemt5Xsk3x8L/hBARERERERERERERkaHgRwgRERERERERERERERkKfoQQEREREREREREREZGhUE6ATfmtKjnMqrnJKNd15fdi/jHKefrAAw8k26mnnjrm/SlHGeXKirm4KH9eJUc3XUu502isYrsGzZvZWu53X7n5W6vl+6V+0biRL8a+UY62Si7W973vfakO5eYkNtpoo06Z8hBPmjQp2WI+e8pXGPPntdbalVdemWwxLxzl4aSccxVtB5qfSr7TvvJpVvLe0hqkvKtkizlcaa2uvPLKyRY1ZA455JBUJ+ratNbadtttl2xRP4RypD/88MPJFnNpUq5CyiNb0VogHRIam+gnFJNpfsjn4v3jmhol5CuxzbQGKa84xYKK3hCt+7h/Uhyl88Ghhx6abFEPYM0110x1qI8x/q211lqpDu39lH/7oosu6pQr+0JreW+gOhTzKedm/E3q86ggX4nrgvpK40brPMbxq6++OtWhOHLCCSeM2c7ddtst2Sprn3z/pz/9abLFebn99ttTHdpjK2ND65ZiYBz7F6MfEq/tS4uE4npsS/XcQMR7UWylOYoaI7R2oxZXa5wnvbLG6fwU/YTGKq6N1rI+XmvZd8i/Kjou1T2mmuu5Lyp7V2UNtsb9ivUoNzXNZ7yOfo/mgM4+Fb+r5Mym6+h8RM9M8V7U5wkTJiRbHFM6V1DMIl98qcS6yrN/5d1GazwnMUaRThLlP486g/Pnzx+zna2xvsd1113XKV9yySWpzoMPPphsMYc/xWnSAqisY4pFg77LIP+qPMP2mZuffntxrok4vnSGI12ZOJ9VzUiKf7fddlunTD5MMTLGnsoZuDUer3h/WqO0p0Qo1hHkw/Ha6jlpVMT5pLmk8x6N5VJLLdUpH3300anOVlttNeb9aRxpnvbYY49ki5C/fu9730u2qD2yYMGCVIdiIMXcylqmMY1xoXod2SrvFMbC/4QQEREREREREREREZGh4EcIEREREREREREREREZCn6EEBERERERERERERGRoeBHCBERERERERERERERGQqveKGonDNx4sR8cUG8jKiIblEdskUBFhJ3qYoKRSEaErSpiEVVhMdaq4m4VQVC4m9Sn4mKgDW189FHHy3d/8UyefLkZKuMUVWAKvodzS8J0+y0006d8pve9KZSGyrzUvW7m2++uVMmwUQiCvu0lv2gKtZaua7qixXBHBKAXNxEofDW8hogQXeKf6997WuTLcaoijhla3nNkQA0tWGFFVZItjjWd911V6qz2WabJdvMmTPHbGcUvGuNBcpiW0kUiu4f71UVoaeYH/2J2jBjxoxkGwYkSL/MMst0yjSOVfHLivA7xZ4I+T6NdxQdpHqbb755qvOOd7wj2eLckQje3Llzk+3OO+9Mtjim1OfKWq6KhA+6x956663JNgxItLISs2nOK4LMdG86t8Xxpf3grW99a7JVhEBpTi644IJki2urGmtobcV2VUUr43hVBCfpOrqW7jVnzpxSu14MJCxfOYuRaB/NSeVZpCKESCLnJBK44447Jlvc1+m6NddcM9nuv//+TvlnP/tZqkN7GZ3PopAwjVVlrdN4Vs96lbkY1R67+uqrJ9uf/vSnTpnmifygIkxNcY3mIO4tdA6piKIuqq0Rmqfx48d3yk8//XSqE8eqNe5PhOJt/L3Wau8BKntFa9n36Zw0iufYTTfdNNni2qkIa7dWE0N+9tlnUx1ag9tvv32nTALQU6dOTbYHHngg2c4999xOmdYQ+VNcQ+T35DuV9UG+Q+sl1qNxpzbQXlQ5P48q1q266qrJFs/uNI50vqf1VTnP0BjF62hs6dmB9usYe2jOK3t49UxKbYjQWqNxjm0l36Hfo7bGOaOYTKLdw4De2UUqgsmLqld510m2iohy9fkt3r/yjNdafvdGZwjylcr+UH3nGaH1QfeiGFAZU3ou7/zWWA0UEREREREREREREREZBD9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFAYW1Hq/6ciJEJiKCSkRCIbUUyJfo8ENKIgSFWcg0RgBhUNjkIiFVGU1upighESCInjRQIrND8VwSGqMyoq4nc0jlWRm4oY8g9+8INku+yyyzrladOmpTokSPaud70r2Y466qhOuSrkfMstt4x5HdlIDCf6S1W0No4frXfyu4o4PYn9jIInn3wy2aJoGwluUXwicasIjfVrXvOaZIuCT7Quye8r4ohrrLFGqvPUU08lW/QdEpubOHFislXiLYkekkhTFBisiMFVoTaMCvKDOHe0JkiIkMat4ou0fisCwdW9JUIiuCeccMKY96c5p7EZN25cskW/q4q8VkQzq2LDsa0Vcc9REvtBfa2eteK9aP+hfSrGU1ofJNpLvhF9kXyzIqJYbXtFHLsavyPk54POT0U0eFTEuE5tq4hTEjQfdK84ZhQbKN7+9Kc/HbMN1XN0bAONQ1UoOtariJUvqt6gDCqYOAxojVeeIelMQ+e96Bs0tnT/ON5Uh/ycYkH0WRIJJh+ONlozUViz2i5qZ0UYlPyQ1i2t02WWWaZTrpx/hgH5STyf0XquvjOIRJHz1rJId2utXXLJJWPeuyKoS1B/aO1F36wKC1f2rWoMq4iEV891cS1UhIyHBY1RHF8ab5rzyt4yaLsoNpCIfFXMOELxqfIOqSoMH8+E1f20Mn6V+N5a7iPVGRUVIfnq2qyIQlNfK+dv8n2616BnTtpv4m/SvekZg+pV3tnRvSoxqnJda4vnnPjSOR2KiIiIiIiIiIiIiMjLCj9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFAoqyGSAEUUDXkxgsyVe5HgS0W8tipoE8U46LqKmGBFPKq1WturgnqxDSTmVBX0in1cnCJ1/1sqAjNVYZVKP6gOiQTPnz+/U45C1YuyHXfccckWBV8qfaa2ktAR9aciJFdZ79TWqhBXRdinL7+rCJ1XRLpbY5GveH8Sd6b7R1tVRIkEkmJb//KXv4xZp7UsVk2CjRQjqQ1R5HDWrFmpzsorr5xsUeCuKl5IguOx31Xx0GFA4x19g/yC/IDif7w/jRGt8RhXSJyyKjwZ20pxgNZMnPOq4CeNTewP/V5FDJb2GPIfEgaN7a+cBYYFzVNlHdB1tBYr+zUR21CNr5WYWxWajG2tCjRW9t3qmom/SW0YVAC3LwYVIh90nVT8srU8lzQfJJpZEaOsxKLWaqL11XGonC0rzyvVuaiIfVfX3jCgc8if//znTpnmd+7cuclGfa0InlbWJe3z5AeVeEEi1LHPreV9ndpA/krtqsQ6EiqOv1k929G89ikK/N+hdsR5I5+onmfimNHcVuI+nVOq7zIqz3M039X7R2hs4r2q+2tse0V4u7XaO6o+91vqRxyTSvxojec3nn+r56DoB/TsWX3vFc+IdB35XWVPqr5Xiv2unCuorRUR5kXZ4jhUfXgYVN6RVvtVgeaE+l/xlUGfaWj/IeI4UDur5/sItWHQMaX1XjnjDuJ3/ieEiIiIiIiIiIiIiIgMBT9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDIWyJgTlK4v5n6qaEJRvKubhpHtRXrWY/3fQPICt5f5UcskR1fyUlMusksO1kn/xxeQNHjR/8jCozGc1Bx8R61V8rLU8B5QHr5L3mH6T/K6S241y2VbHIf5mNd9vpJozm4jzOmi+0BcLzTfl1Y1Qbssnnngi2aKvUBygcSS/iNC8kW/GNUMxpaI/QHlkl1566WRbffXVc2MDCxcuHLNOa1mXgsaP5nDZZZct1esLWuNxfGl+ae4G1XCq7POVtdBaLecm+VhVnyRS0XGge1FMruTEJL+r5mqv5NMdFeQHlTNANd957GtVgyL6RiUHe2s1PZTKuYfuX81lO2j+4grVnM2VM0NfuYOpD3FvqearrvhvpQ5RyedevX81x/7inJOKjk3l96paAJXx6vN5gvoR/Y7miea3cm6j8aDzUZwD2gNpnuj+sf3VM07s44vRIqmMKRGfYarPv5Xn675iXeVdBtUh/YeK1hD5OO2vsR79XkU7s7U8/uQndP+oX1HdSwc9D5I/Rd+s7ssVnZw+tb4qZ99Bn/Nbq+kQVPZF0tykNU7PHfGsN+ieVNExoOtayzGrOueVsyVRObv2+TxReQdBdarvLOO1g15H7y4oRlXms7LWWsuxs+or5J/xWorL1J/K2biiO0JtGMTv/E8IEREREREREREREREZCn6EEBERERERERERERGRoeBHCBERERERERERERERGQp+hBARERERERERERERkaHwihf6VAkTEREREREREREREZGXLf4nhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFDwI4SIiIiIiIiIiIiIiAwFP0KIiIiIiIiIiIiIiMhQ8COEiIiIiIiIiIiIiIgMBT9CiIiIiIiIiIiIiIjIUPAjhIiIiIiIiIiIiIiIDAU/QoiIiIiIiIiIiIiIyFDwI4SIiIiIiIiIiIiIiAyF/w+vr503x/tbfgAAAABJRU5ErkJggg==","text/plain":["<Figure size 2000x500 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["data = torch.load('data.pt')\n","\n","orig_img = data[0][0]\n","orig_lab = data[0][1]\n","\n","\n","syn_img = data[1][0]\n","syn_lab = data[1][1]\n","\n","print(orig_img.shape, orig_lab.shape)\n","\n","# plot 10 images from the original dataset\n","n = 10\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(1, n, figsize=(20, 5))\n","for i in range(n):\n","    ax[i].imshow(orig_img[i][0], cmap='gray')\n","    ax[i].set_title(orig_lab[i])\n","    ax[i].axis('off')\n","plt.show()\n","\n","\n","# plot 10 images from the synthetic dataset\n","fig, ax = plt.subplots(1, n, figsize=(20, 5))\n","for i in range(n):\n","    ax[i].imshow(syn_img[i][0], cmap='gray')\n","    ax[i].set_title(syn_lab[i])\n","    ax[i].axis('off')\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"DKk2luqUkJ2D"},"source":["### Google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26461,"status":"ok","timestamp":1729642615066,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"3LbCnOvD4YFP","outputId":"810a895d-3bd8-4f7c-f6db-9d17c3975c1d"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks/ECE1512_2024F_ProjectRepo_SwapnilPatel/project_A/src\n","\n","!pip install thop"]},{"cell_type":"markdown","metadata":{"id":"_U9GlicdkL_C"},"source":["### Init"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2071,"status":"ok","timestamp":1729642664063,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"YYOVF7fc3Tup","outputId":"0cc5765c-012a-4d3b-b000-df90e533ed64"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from tqdm.auto import tqdm\n","from torchvision.utils import save_image\n","from thop import profile\n","#from utils import get_network, get_dataset, TensorDataset\n","#import dataDAM\n","import sys\n","\n","figures_dir = '../report/figures/'\n","output_dir = '../output/'\n","\n","print(f\"PyTorch Version: {torch.__version__}\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    device_name = torch.cuda.get_device_name(0)\n","    properties = torch.cuda.get_device_properties(0)\n","    compute_capability = f\"{properties.major}.{properties.minor}\"\n","    total_memory = properties.total_memory / 1024**3\n","\n","    print(f\"CUDA Device: {device_name}\")\n","    print(f\"CUDA Compute Capability: {compute_capability}\")\n","    print(f\"Total Memory: {total_memory:.2f} GB\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available\")\n","\n","\n","def epoch_S(mode, dataloader, net, optimizer, criterion, device, progress_bar):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(device)\n","    criterion = criterion.to(device)\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(device)\n","        lab = datum[1].long().to(device)\n","        n_b = lab.shape[0]\n","\n","        output = net(img)\n","        loss = criterion(output, lab)\n","        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","\n","        loss_avg += loss.item()*n_b\n","        acc_avg += acc\n","        num_exp += n_b\n","\n","        if mode == 'train':\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        progress_bar.set_postfix(loss=loss.item() / (i + 1))\n","        progress_bar.update(1)\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","# function to get FLOPS for a given model\n","def get_flops(model, dataloader, device):\n","    for inputs, _ in dataloader:\n","        # Get a single image from the batch\n","        # add an extra batch dimension to the image, as the models expect a batch of\n","        # images as input, not a single image.\n","        single_image = inputs[0].unsqueeze(0).to(device)\n","        break\n","    flops = profile(model, inputs=(single_image, ), verbose=False)\n","    return flops\n","\n","# function to get syn dataset from the output file\n","def get_syn_dataset (sym_name):\n","    syn_dataset_file = output_dir + sym_name\n","    results = torch.load(syn_dataset_file, weights_only=True)\n","    syn_imgs = results['data'][0][0]\n","    syn_labels = results['data'][0][1]\n","\n","    syn_dataset = TensorDataset(syn_imgs, syn_labels)\n","    channel = syn_imgs.shape[1]\n","    num_classes = syn_labels.max().item() + 1\n","    im_size = (syn_imgs.shape[2], syn_imgs.shape[3])\n","\n","    dataloader = torch.utils.data.DataLoader(syn_dataset, batch_size=32, shuffle=True)\n","    return syn_dataset, channel, num_classes, im_size, dataloader"]},{"cell_type":"markdown","metadata":{"id":"L6gd4UsA3Tur"},"source":["## MNIST"]},{"cell_type":"markdown","metadata":{"id":"2HJ5YyuIjglO"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"executionInfo":{"elapsed":7415,"status":"ok","timestamp":1729626781373,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"Qe-20Rwx3Tus","outputId":"1acf1b1d-6d54-4478-c7c9-e338425063a0"},"outputs":[],"source":["# load MNIST dataset from utils\n","(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MNIST_dataset,\n","    test_MNIST_dataset,\n","    test_MNIST_dataloader,\n","    train_MNIST_dataloader,\n",") = get_dataset(\"MNIST\", \"../datasets\")\n","\n","# visualize 10 classes of MNIST (2 by 5)\n","fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(\n","        train_MNIST_dataset.data[train_MNIST_dataset.targets == i][0], cmap=\"gray\"\n","    )\n","    ax.set_title(f\"{i}\")\n","    ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"MNIST_dataset.png\", dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"uHHNti603Tus"},"source":["### ConvNet3\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"elapsed":459,"status":"error","timestamp":1729626770640,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"bWNt-yCp3Tut","outputId":"6c481d55-3d84-4fa3-fa58-9846b6e4900b"},"outputs":[],"source":["ConvNet3 = get_network('ConvNetD3', channel, num_classes, im_size)\n","print(ConvNet3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98,"referenced_widgets":["28e39ec029124058b79477593f9cc705","48cdad7737884a5fa27c5a4c29935f44","7c297763bc854e97b38ed34be0c19ac9","4abeefd064fd4e589265227df362bb20","b4710ac36fb046279ba50f6be15460fb","c9724c51375c4023abfedda82e68f88c","09906df43e64476994045e5e787fca25","7560d99a22bf49f4ae369217dcccc965","00705a21a0074b92bf4beecf33058617","dbabcd7224d04feaa15b1a9ee61c3d82","f012406ec0d3480193deed026785f7c4","6ac49717585e4ce7908721b6afd463db","0391213c99ca456c816261759c84dc4e","3f4232df43a64b108841cb8651354842","b4b08a8544a247898c35f95040951dc9","53d0cb62fee94dbebcb91af437647019","4cb5d61a7fa24b66bd3ba41ccec4482f","9e577742214a4136aee778b8675b04f1","7cf09cdffb314c9db9bf8865b2f50cce","387ff275594b46cea795f41e9a6fd2f1","9d19481a1a86466aa005df60e5bfab68","706ee0493c6c420fa9ae8c715e606c65"]},"executionInfo":{"elapsed":45369,"status":"ok","timestamp":1729623324965,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"wPRIq5kc3Tut","outputId":"f5fa4d57-8273-460c-b681-3b2271cb5891"},"outputs":[],"source":["n_epochs = 2\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet3.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = train_MNIST_dataloader\n","testLoader = test_MNIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet3, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet3, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3_train_acc = train_acc\n","ConvNet3_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet3, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":1077,"status":"ok","timestamp":1729623330717,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"CHCNfCrU3Tuu","outputId":"bdf1124e-dc22-4c65-cc6b-09adf644628b"},"outputs":[],"source":["# plot the training and test accuracy\n","plt.plot(train_acc, label=\"Train\")\n","plt.plot(test_acc, label=\"Test\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yxEStI5E3Tuu"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28938,"status":"ok","timestamp":1729642760913,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"HYONQKws3Tuv","outputId":"d2c4d6f0-b186-4655-cf74-e15eb82b9377"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'real',\n","    '--model', 'ConvNetD3',\n","    '--dataset', 'MNIST',\n","    '--output_file', 'MNIST_real_res.pt',       # Output file\n","    '--ipc', '10',                              # Images/class\n","    '--lr_img', '1.0',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '1',                          #\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '500',                        # T\n","]\n","\n","MNIST_real_res = dataDAM()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"executionInfo":{"elapsed":1073,"status":"ok","timestamp":1729642766499,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"vS5bJbmS5EYQ","outputId":"12783c4e-f313-4373-8756-c46b5b1b2325"},"outputs":[],"source":["\n","orig_MNIST_real_res = output_dir + 'orig_MNIST_real_res.pt'\n","results = torch.load(orig_MNIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","tmp = syn_imgs.permute(0, 2, 3, 1).squeeze()\n","# display syn_imgs[0]\n","plt.imshow(syn_imgs[0].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.tight_layout()\n","plt.show()\n","\n","MNIST_real_res = output_dir + 'MNIST_real_res.pt'\n","results = torch.load(MNIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","tmp2 = syn_imgs.permute(0, 2, 3, 1).squeeze()\n","\n","plt.imshow(syn_imgs[0].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4478,"status":"ok","timestamp":1729627091389,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"mioHw6Nq3Tuv","outputId":"dd817b3e-eacc-4095-ef86-e8dce3f706f5"},"outputs":[],"source":["# plot the results\n","MNIST_real_res = output_dir + 'MNIST_real_res.pt'\n","results = torch.load(MNIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","\n","plt.suptitle(\"MNIST Synthetic Images\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455593,"status":"ok","timestamp":1729625286168,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"1w8t6bK-3Tuw","outputId":"f72e4e51-e195-4ec0-ddb6-f8dc26f4a13d"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'noise',\n","    '--model', 'ConvNetD3',\n","    '--dataset', 'MNIST',\n","    '--output_file', 'MNIST_noise_res.pt',      # Output file\n","    '--ipc', '10',                              # Images/class\n","    '--lr_img', '0.1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '2000',                        # T\n","]\n","\n","MNIST_noise_res = dataDAM()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4510,"status":"ok","timestamp":1729625442229,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"cY5ZKVkW3Tuw","outputId":"2aee5ee7-7923-42f9-cc90-28dcd563639a"},"outputs":[],"source":["# plot the results\n","MNIST_noise_res = output_dir + 'MNIST_noise_res.pt'\n","results = torch.load(MNIST_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"MNIST Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkHV7ZlUu8SQ"},"outputs":[],"source":["# plot the results\n","MNIST_noise_res = output_dir + 'MNIST_noise_res.pt'\n","results = torch.load(MNIST_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"MNIST Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"e2VBde713Tux"},"source":["### ConvNet3 with Synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWJxZsx13Tux"},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MNIST_real_res.pt')\n","ConvNet3Syn = get_network('ConvNetD3', channel, num_classes, im_size)\n","n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet3Syn.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MNIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet3Syn, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet3Syn, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3Syn_train_acc = train_acc\n","ConvNet3Syn_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet3Syn, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaRsLgJW3Tux"},"outputs":[],"source":["plt.figure(figsize=(7, 5))  # Set figure size\n","\n","# Plotting with different line styles for train (dashed) and test (solid) data\n","plt.plot(ConvNet3_train_acc, label=\"Original Dataset [Train]\", linestyle='--', color='b')  # Dashed line for training\n","plt.plot(ConvNet3_test_acc, label=\"Original Dataset [Test]\", linestyle='-', color='b')    # Solid line for testing\n","plt.plot(ConvNet3Syn_train_acc, label=\"Synthetic Dataset [Train]\", linestyle='--', color='r')  # Dashed line for training\n","plt.plot(ConvNet3Syn_test_acc, label=\"Synthetic Dataset [Test]\", linestyle='-', color='r')    # Solid line for testing\n","\n","# Add grid\n","plt.grid(False)\n","\n","# Ensure x-axis displays only whole numbers, every 5 epochs\n","plt.xticks(np.arange(0, n_epochs+1, 5))\n","\n","\n","plt.xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n","plt.ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n","plt.title(\"ConvNet3 Model Accuracy for MNIST Dataset\", fontsize=14, fontweight='bold')\n","\n","# Add legend with a better location\n","plt.legend(loc='best', fontsize=10)\n","\n","# Add a tight layout to minimize unnecessary whitespace\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_4GzQObCu_6A"},"source":["## CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"AEBtSwUDvNrR"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192226,"status":"ok","timestamp":1729626504834,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"cwJxJ6_yu-rb","outputId":"895fe4ac-629a-4045-9dfe-8f05efe310b7"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'real',\n","    '--model', 'ConvNetD3',\n","    #'--dataset', 'MNIST',\n","    '--output_file', 'CIFAR10_noise_res.pt',      # Output file\n","    '--ipc', '10',                              # Images/class\n","    '--lr_img', '1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '50',                        # T\n","]\n","\n","CIFAR10_noise_res = dataDAM()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6537,"status":"ok","timestamp":1729626534006,"user":{"displayName":"Swapnil Patel","userId":"15017096891012395357"},"user_tz":240},"id":"rNaL9aD6vTh9","outputId":"0bf5ec63-1909-40fd-846e-a8e3fc733182"},"outputs":[],"source":["# plot the results\n","CIFAR10_noise_res = output_dir + 'CIFAR10_noise_res.pt'\n","results = torch.load(CIFAR10_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# clip the images to [0, 1]\n","\n","# 10 images per class\n","fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(syn_imgs[i].permute(1, 2, 0).squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"CIFAR10 Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"VgyQFdNY3Tuy"},"source":["## MHIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4trkVu1P3Tuy"},"outputs":[],"source":["(\n","    channel,\n","    im_size,\n","    num_classes,\n","    class_names,\n","    mean,\n","    std,\n","    train_MHIST_dataset,\n","    test_MHIST_dataset,\n","    test_MHIST_dataloader,\n","    train_MHIST_dataloader,\n",") = get_dataset(\"MHIST\", \"../datasets\")\n","\n","indices = [100, 1560]\n","\n","# plot 2 images from the dataset\n","fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n","for i, ax in enumerate(axes.flat):\n","    image, label = train_MHIST_dataset[indices[i]]\n","    # Transpose the image from [3, 224, 224] to [224, 224, 3] for plotting\n","    image = image.permute(1, 2, 0)\n","    ax.imshow(image)\n","    if label == 0:\n","        ax.set_title(\"HP\")\n","    else:\n","        ax.set_title(\"SSA\")\n","    ax.axis(\"off\")\n","plt.tight_layout()\n","plt.savefig(figures_dir + \"MHIST_dataset.png\", dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"pX3MQTu93Tuy"},"source":["### ConvNet7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhMZfqCN3Tuy"},"outputs":[],"source":["ConvNet7 = get_network('ConvNetD7', channel, num_classes, im_size)\n","print(ConvNet7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ekz5CVpL3Tuz"},"outputs":[],"source":["n_epochs = 5\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet7.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = train_MHIST_dataloader\n","testLoader = test_MHIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet7, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet7, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","flops, _ = get_flops(ConvNet7, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]},{"cell_type":"markdown","metadata":{"id":"JPHNre3z3Tuz"},"source":["### Synthetic dataset generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eg396AJR3Tuz"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'real',\n","    '--model', 'ConvNetD7',\n","    '--dataset', 'MHIST',\n","    '--output_file', 'MHIST_real_res.pt',       # Output file\n","    '--ipc', '50',                              # Images/class\n","    '--lr_img', '0.1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '10',                        # T\n","]\n","\n","MHIST_real_res = dataDAM.main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FivmMeFX3Tuz"},"outputs":[],"source":["MHIST_real_res = output_dir + 'MHIST_real_res.pt'\n","results = torch.load(MHIST_real_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_labels = results['data'][0][1]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# 50 images per class (HP, SSA)\n","# create a grid of 50 images per class\n","# create 1 image per class\n","\n","# label 0 is HP, label 1 is SSA\n","HP_imgs = syn_imgs[syn_labels == 0]\n","SSA_imgs = syn_imgs[syn_labels == 1]\n","# plot only HP images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(HP_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"HP Synthesized Images\", fontsize=16)\n","plt.tight_layout()\n","\n","# plot only SSA images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(SSA_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"SSA Synthesized Images\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YmBPU773Tu0"},"outputs":[],"source":["sys.argv = [\n","    'dataDAM.py',\n","    '--init', 'noise',\n","    '--model', 'ConvNetD7',\n","    '--dataset', 'MHIST',\n","    '--output_file', 'MHIST_noise_res.pt',      # Output file\n","    '--ipc', '50',                              # Images/class\n","    '--lr_img', '0.1',                          # eta_s\n","    '--lr_net', '0.01',                         # eta_theta\n","    '--num_eval', '50',                         # zeta_theta\n","    '--epoch_eval_train', '1',                  # zeta_s\n","    '--Iteration', '10',                        # T\n","]\n","\n","MHIST_noise_res = dataDAM.main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RhsasRM3Tu0"},"outputs":[],"source":["MHIST_noise_res = output_dir + 'MHIST_noise_res.pt'\n","results = torch.load(MHIST_noise_res, weights_only=True)\n","syn_imgs = results['data'][0][0]\n","syn_labels = results['data'][0][1]\n","syn_imgs = torch.clamp(syn_imgs, 0, 1)\n","\n","# 50 images per class (HP, SSA)\n","# create a grid of 50 images per class\n","# create 1 image per class\n","\n","# label 0 is HP, label 1 is SSA\n","HP_imgs = syn_imgs[syn_labels == 0]\n","SSA_imgs = syn_imgs[syn_labels == 1]\n","# plot only HP images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(HP_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"HP Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()\n","\n","# plot only SSA images\n","fig, axes = plt.subplots(5, 10, figsize=(10, 5))\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(SSA_imgs[i].permute(1, 2, 0).squeeze())\n","    ax.axis(\"off\")\n","# figure title\n","plt.suptitle(\"SSA Synthesized Images (Noise Initialization)\", fontsize=16)\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"UZqV01fd3Tu0"},"source":["### ConvNet7 with Synthetic dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_zc9k5J3Tu0"},"outputs":[],"source":["syn_dataset, channel, num_classes, im_size, dataloader = get_syn_dataset('MHIST_real_res.pt')\n","ConvNet7Syn = get_network('ConvNetD7', channel, num_classes, im_size)\n","n_epochs = 20\n","lr = 0.01\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(ConvNet7Syn.parameters(), lr=lr)\n","train_acc, test_acc = [], []\n","trainLoader = dataloader\n","testLoader = test_MNIST_dataloader\n","for ep in range (n_epochs):\n","    progress_bar = tqdm(\n","        enumerate(trainLoader, 0),\n","        total=len(trainLoader) + len(testLoader),\n","        desc=f\"Epoch {ep+1}\",\n","    )\n","\n","    train_loss_avg, train_acc_avg = epoch_S('train', trainLoader, ConvNet7Syn, optimizer, criterion, device, progress_bar)\n","    test_loss_avg, test_acc_avg = epoch_S('test', testLoader, ConvNet7Syn, optimizer, criterion, device,progress_bar)\n","\n","    train_acc.append(train_acc_avg)\n","    test_acc.append(test_acc_avg)\n","\n","ConvNet3Syn_train_acc = train_acc\n","ConvNet3Syn_test_acc = test_acc\n","\n","flops, _ = get_flops(ConvNet7Syn, testLoader, device)\n","print(\"FLOPS: {:,}\".format(flops))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00705a21a0074b92bf4beecf33058617":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0391213c99ca456c816261759c84dc4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb5d61a7fa24b66bd3ba41ccec4482f","placeholder":"​","style":"IPY_MODEL_9e577742214a4136aee778b8675b04f1","value":"Epoch 2: 100%"}},"09906df43e64476994045e5e787fca25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28e39ec029124058b79477593f9cc705":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48cdad7737884a5fa27c5a4c29935f44","IPY_MODEL_7c297763bc854e97b38ed34be0c19ac9","IPY_MODEL_4abeefd064fd4e589265227df362bb20"],"layout":"IPY_MODEL_b4710ac36fb046279ba50f6be15460fb"}},"387ff275594b46cea795f41e9a6fd2f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f4232df43a64b108841cb8651354842":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cf09cdffb314c9db9bf8865b2f50cce","max":1095,"min":0,"orientation":"horizontal","style":"IPY_MODEL_387ff275594b46cea795f41e9a6fd2f1","value":1095}},"48cdad7737884a5fa27c5a4c29935f44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9724c51375c4023abfedda82e68f88c","placeholder":"​","style":"IPY_MODEL_09906df43e64476994045e5e787fca25","value":"Epoch 1: 100%"}},"4abeefd064fd4e589265227df362bb20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbabcd7224d04feaa15b1a9ee61c3d82","placeholder":"​","style":"IPY_MODEL_f012406ec0d3480193deed026785f7c4","value":" 1095/1095 [00:50&lt;00:00, 58.23it/s, loss=0.00165]"}},"4cb5d61a7fa24b66bd3ba41ccec4482f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53d0cb62fee94dbebcb91af437647019":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ac49717585e4ce7908721b6afd463db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0391213c99ca456c816261759c84dc4e","IPY_MODEL_3f4232df43a64b108841cb8651354842","IPY_MODEL_b4b08a8544a247898c35f95040951dc9"],"layout":"IPY_MODEL_53d0cb62fee94dbebcb91af437647019"}},"706ee0493c6c420fa9ae8c715e606c65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7560d99a22bf49f4ae369217dcccc965":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c297763bc854e97b38ed34be0c19ac9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7560d99a22bf49f4ae369217dcccc965","max":1095,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00705a21a0074b92bf4beecf33058617","value":1095}},"7cf09cdffb314c9db9bf8865b2f50cce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d19481a1a86466aa005df60e5bfab68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e577742214a4136aee778b8675b04f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4710ac36fb046279ba50f6be15460fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b08a8544a247898c35f95040951dc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d19481a1a86466aa005df60e5bfab68","placeholder":"​","style":"IPY_MODEL_706ee0493c6c420fa9ae8c715e606c65","value":" 1095/1095 [00:36&lt;00:00, 56.95it/s, loss=0.002]"}},"c9724c51375c4023abfedda82e68f88c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbabcd7224d04feaa15b1a9ee61c3d82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f012406ec0d3480193deed026785f7c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
