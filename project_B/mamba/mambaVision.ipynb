{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, ViTImageProcessor, ViTForImageClassification\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout):\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MambaVisionModelForImageClassification(\n",
       "  (model): MambaVision(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Identity()\n",
       "      (conv_down): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (levels): ModuleList(\n",
       "      (0): MambaVisionLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): GELU(approximate='tanh')\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): GELU(approximate='tanh')\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop_path): DropPath(drop_prob=0.010)\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): GELU(approximate='tanh')\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop_path): DropPath(drop_prob=0.020)\n",
       "          )\n",
       "        )\n",
       "        (downsample): Downsample(\n",
       "          (reduction): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MambaVisionLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): GELU(approximate='tanh')\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop_path): DropPath(drop_prob=0.030)\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): GELU(approximate='tanh')\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop_path): DropPath(drop_prob=0.040)\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): GELU(approximate='tanh')\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (drop_path): DropPath(drop_prob=0.050)\n",
       "          )\n",
       "        )\n",
       "        (downsample): Downsample(\n",
       "          (reduction): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MambaVisionLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (x_proj): Linear(in_features=256, out_features=48, bias=False)\n",
       "              (dt_proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (conv1d_x): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "              (conv1d_z): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.060)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (x_proj): Linear(in_features=256, out_features=48, bias=False)\n",
       "              (dt_proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (conv1d_x): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "              (conv1d_z): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (x_proj): Linear(in_features=256, out_features=48, bias=False)\n",
       "              (dt_proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (conv1d_x): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "              (conv1d_z): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.080)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (x_proj): Linear(in_features=256, out_features=48, bias=False)\n",
       "              (dt_proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (conv1d_x): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "              (conv1d_z): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.090)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (x_proj): Linear(in_features=256, out_features=48, bias=False)\n",
       "              (dt_proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (conv1d_x): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "              (conv1d_z): Conv1d(256, 256, kernel_size=(3,), stride=(1,), groups=256, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.110)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.120)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.130)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.140)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.150)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Downsample(\n",
       "          (reduction): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MambaVisionLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (x_proj): Linear(in_features=512, out_features=80, bias=False)\n",
       "              (dt_proj): Linear(in_features=64, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (conv1d_x): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512, bias=False)\n",
       "              (conv1d_z): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.160)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (x_proj): Linear(in_features=512, out_features=80, bias=False)\n",
       "              (dt_proj): Linear(in_features=64, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (conv1d_x): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512, bias=False)\n",
       "              (conv1d_z): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.170)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): MambaVisionMixer(\n",
       "              (in_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (x_proj): Linear(in_features=512, out_features=80, bias=False)\n",
       "              (dt_proj): Linear(in_features=64, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (conv1d_x): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512, bias=False)\n",
       "              (conv1d_z): Conv1d(512, 512, kernel_size=(3,), stride=(1,), groups=512, bias=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.180)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.190)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mixer): Attention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.200)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MambaVision Model\n",
    "mamba_model = AutoModelForImageClassification.from_pretrained(\"nvidia/MambaVision-B-1K\", trust_remote_code=True)\n",
    "mamba_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://images.cocodataset.org/val2017/000000020247.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://i.ibb.co/JQLxsgX/aircraft-jet-landing-cloud-46148.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m      5\u001b[0m input_resolution \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m)  \u001b[38;5;66;03m# MambaVision supports any input resolutions\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# display image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "url = 'http://images.cocodataset.org/val2017/000000020247.jpg'\n",
    "url = 'https://i.ibb.co/JQLxsgX/aircraft-jet-landing-cloud-46148.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "input_resolution = (3, 34, 34)  # MambaVision supports any input resolutions\n",
    "\n",
    "# display image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "transform = create_transform(input_size=input_resolution,\n",
    "                             is_training=False,\n",
    "                             mean=mamba_model.config.mean,\n",
    "                             std=mamba_model.config.std,\n",
    "                             crop_mode=mamba_model.config.crop_mode,\n",
    "                             crop_pct=mamba_model.config.crop_pct)\n",
    "\n",
    "# output flops\n",
    "input = torch.randn(1, 3, 224, 224).cuda()\n",
    "flops, params = profile(mamba_model, inputs=(input, ))\n",
    "print(flops)\n",
    "\n",
    "\n",
    "inputs = transform(image).unsqueeze(0).cuda()\n",
    "# model inference\n",
    "outputs = mamba_model(inputs)\n",
    "logits = outputs['logits']\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", mamba_model.config.id2label[predicted_class_idx])\n",
    "\n",
    "#  Feature Extraction\n",
    "#out_avg_pool, features = model_feature_extraction(inputs)\n",
    "#print(\"Size of the averaged pool features:\", out_avg_pool.size())  # torch.Size([1, 640])\n",
    "#print(\"Number of stages in extracted features:\", len(features)) # 4 stages\n",
    "#print(\"Size of extracted features in stage 1:\", features[0].size()) # torch.Size([1, 80, 56, 56])\n",
    "#print(\"Size of extracted features in stage 4:\", features[3].size()) # torch.Size([1, 640, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViT Model and Processor\n",
    "vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops(model, input_shape):\n",
    "    input = torch.randn(1, *input_shape).cuda()\n",
    "    return torch.cuda.get_device_properties(0).multi_processor_count * torch.cuda.get_device_properties(0).core_count * model(input).shape.numel()\n",
    "\n",
    "mamba_flops = get_flops(mamba_model, (3, 224, 224))\n",
    "vit_flops = get_flops(vit_model, (3, 224, 224))\n",
    "\n",
    "print(f\"MambaVision-B-1K: {mamba_flops / 1e9:.2f} GFLOPs\")\n",
    "print(f\"ViT-Base: {vit_flops / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CIFAR-100 Dataset Loader\n",
    "batch_size = 32\n",
    "\n",
    "# MambaVision Transform\n",
    "input_resolution = (3, 224, 224)\n",
    "mamba_transform = create_transform(input_size=input_resolution,\n",
    "                                   is_training=False,\n",
    "                                   mean=mamba_model.config.mean,\n",
    "                                   std=mamba_model.config.std,\n",
    "                                   crop_mode=mamba_model.config.crop_mode,\n",
    "                                   crop_pct=mamba_model.config.crop_pct)\n",
    "\n",
    "# CIFAR-100 Dataset for MambaVision\n",
    "cifar100_test_mamba = datasets.CIFAR100(root='./data', train=False, download=True, transform=mamba_transform)\n",
    "test_loader_mamba = DataLoader(cifar100_test_mamba, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# CIFAR-100 Dataset for ViT\n",
    "vit_transform = vit_processor  # Use ViT's processor directly\n",
    "cifar100_test_vit = datasets.CIFAR100(root='./data', train=False, download=True, transform=lambda img: vit_transform(images=img, return_tensors=\"pt\")[\"pixel_values\"].squeeze())\n",
    "test_loader_vit = DataLoader(cifar100_test_vit, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MambaVision\n",
    "def evaluate_mamba(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating MambaVision\"):\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            logits = outputs['logits']\n",
    "            predicted_class_idxs = logits.argmax(dim=-1)\n",
    "            correct += (predicted_class_idxs == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ViT\n",
    "def evaluate_vit(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating ViT\"):\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_idxs = logits.argmax(dim=-1)\n",
    "            correct += (predicted_class_idxs == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "accuracy_mamba = evaluate_mamba(mamba_model, test_loader_mamba)\n",
    "print(f\"MambaVision Accuracy on CIFAR-100: {accuracy_mamba:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vit = evaluate_vit(vit_model, test_loader_vit)\n",
    "print(f\"ViT Accuracy on CIFAR-100: {accuracy_vit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Pre-Trained ViT Model\n",
    "vit_model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=100,  # Adjust for CIFAR-100\n",
    "    id2label={i: str(i) for i in range(100)},\n",
    "    label2id={str(i): i for i in range(100)},\n",
    "    ignore_mismatched_sizes=True  # Ignore size mismatch for the classification head\n",
    ")\n",
    "vit_model.to(device)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Data Preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 Dataset\n",
    "train_dataset = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(data_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(data_loader), accuracy\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(data_loader), accuracy\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc = train_model(vit_model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    val_loss, val_acc = evaluate_model(vit_model, test_loader, criterion, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "vit_model.save_pretrained(\"./vit-finetuned-cifar100\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
