{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, ViTImageProcessor, ViTForImageClassification\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MambaVision Model\n",
    "mamba_model = AutoModelForImageClassification.from_pretrained(\"nvidia/MambaVision-B-1K\", trust_remote_code=True)\n",
    "mamba_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViT Model and Processor\n",
    "vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops(model, input_shape):\n",
    "    input = torch.randn(1, *input_shape).cuda()\n",
    "    return torch.cuda.get_device_properties(0).multi_processor_count * torch.cuda.get_device_properties(0).core_count * model(input).shape.numel()\n",
    "\n",
    "mamba_flops = get_flops(mamba_model, (3, 224, 224))\n",
    "vit_flops = get_flops(vit_model, (3, 224, 224))\n",
    "\n",
    "print(f\"MambaVision-B-1K: {mamba_flops / 1e9:.2f} GFLOPs\")\n",
    "print(f\"ViT-Base: {vit_flops / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CIFAR-100 Dataset Loader\n",
    "batch_size = 32\n",
    "\n",
    "# MambaVision Transform\n",
    "input_resolution = (3, 224, 224)\n",
    "mamba_transform = create_transform(input_size=input_resolution,\n",
    "                                   is_training=False,\n",
    "                                   mean=mamba_model.config.mean,\n",
    "                                   std=mamba_model.config.std,\n",
    "                                   crop_mode=mamba_model.config.crop_mode,\n",
    "                                   crop_pct=mamba_model.config.crop_pct)\n",
    "\n",
    "# CIFAR-100 Dataset for MambaVision\n",
    "cifar100_test_mamba = datasets.CIFAR100(root='./data', train=False, download=True, transform=mamba_transform)\n",
    "test_loader_mamba = DataLoader(cifar100_test_mamba, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# CIFAR-100 Dataset for ViT\n",
    "vit_transform = vit_processor  # Use ViT's processor directly\n",
    "cifar100_test_vit = datasets.CIFAR100(root='./data', train=False, download=True, transform=lambda img: vit_transform(images=img, return_tensors=\"pt\")[\"pixel_values\"].squeeze())\n",
    "test_loader_vit = DataLoader(cifar100_test_vit, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MambaVision\n",
    "def evaluate_mamba(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating MambaVision\"):\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            logits = outputs['logits']\n",
    "            predicted_class_idxs = logits.argmax(dim=-1)\n",
    "            correct += (predicted_class_idxs == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ViT\n",
    "def evaluate_vit(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating ViT\"):\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_idxs = logits.argmax(dim=-1)\n",
    "            correct += (predicted_class_idxs == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "accuracy_mamba = evaluate_mamba(mamba_model, test_loader_mamba)\n",
    "print(f\"MambaVision Accuracy on CIFAR-100: {accuracy_mamba:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vit = evaluate_vit(vit_model, test_loader_vit)\n",
    "print(f\"ViT Accuracy on CIFAR-100: {accuracy_vit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Pre-Trained ViT Model\n",
    "vit_model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=100,  # Adjust for CIFAR-100\n",
    "    id2label={i: str(i) for i in range(100)},\n",
    "    label2id={str(i): i for i in range(100)},\n",
    "    ignore_mismatched_sizes=True  # Ignore size mismatch for the classification head\n",
    ")\n",
    "vit_model.to(device)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Data Preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 Dataset\n",
    "train_dataset = datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(data_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(data_loader), accuracy\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(data_loader), accuracy\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc = train_model(vit_model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    val_loss, val_acc = evaluate_model(vit_model, test_loader, criterion, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "vit_model.save_pretrained(\"./vit-finetuned-cifar100\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
