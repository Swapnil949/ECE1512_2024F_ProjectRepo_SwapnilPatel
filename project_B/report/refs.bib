@misc{gu2024mamba,
	title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
	author={Albert Gu and Tri Dao},
	year={2024},
	eprint={2312.00752},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2312.00752}, 
}
@misc{hatamizadeh2024mambavision,
	title={MambaVision: A Hybrid Mamba-Transformer Vision Backbone}, 
	author={Ali Hatamizadeh and Jan Kautz},
	year={2024},
	eprint={2407.08083},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2407.08083}, 
}
@misc{liu2023llava,
	title={Visual Instruction Tuning}, 
	author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
	year={2023},
	eprint={2304.08485},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2304.08485}, 
}

@TECHREPORT{Krizhevsky09cifar100,
	author = {Alex Krizhevsky},
	title = {Learning multiple layers of features from tiny images},
	institution = {University of Toronto},
	year = {2009}
}
@misc{vaswani2023attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}

@misc{katharopoulos2020transformersrnnsfastautoregressive,
	title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
	author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
	year={2020},
	eprint={2006.16236},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2006.16236}, 
}

@misc{rao2022hornetefficienthighorderspatial,
	title={HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions}, 
	author={Yongming Rao and Wenliang Zhao and Yansong Tang and Jie Zhou and Ser-Nam Lim and Jiwen Lu},
	year={2022},
	eprint={2207.14284},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2207.14284}, 
}

@misc{bulatov2022recurrentmemorytransformer,
	title={Recurrent Memory Transformer}, 
	author={Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
	year={2022},
	eprint={2207.06881},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2207.06881}, 
}

@misc{gu2022efficientlymodelinglongsequences,
	title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
	author={Albert Gu and Karan Goel and Christopher Ré},
	year={2022},
	eprint={2111.00396},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2111.00396}, 
}

@misc{dao2024transformersssmsgeneralizedmodels,
	title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
	author={Tri Dao and Albert Gu},
	year={2024},
	eprint={2405.21060},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2405.21060}, 
}

@misc{wu2020visual,
	title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, 
	author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
	year={2020},
	eprint={2006.03677},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{hu2021llora,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}

@misc{yoshimura2024mambapeft,
	title={MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba}, 
	author={Masakazu Yoshimura and Teruaki Hayashi and Yota Maeda},
	year={2024},
	eprint={2411.03855},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2411.03855}, 
}
@misc{halloran2024mamba,
	title={Mamba State-Space Models Are Lyapunov-Stable Learners}, 
	author={John T. Halloran and Manbir Gulati and Paul F. Roysdon},
	year={2024},
	eprint={2406.00209},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2406.00209}, 
}

@misc{zhao2024cobraextendingmambamultimodal,
	title={Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference}, 
	author={Han Zhao and Min Zhang and Wei Zhao and Pengxiang Ding and Siteng Huang and Donglin Wang},
	year={2024},
	eprint={2403.14520},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2403.14520}, 
}