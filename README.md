# [Dataset Distillation: A Data-Efficient Learning Framework - Project A](/project_A)
This project explores dataset distillation techniques by comparing two methods, DataDAM and PAD, for creating compact synthetic datasets. Dataset distillation compresses data into smaller, information-rich sets that allow for faster training with less computational load.

Using MNIST as a benchmark, this study examines DataDAM’s Attention Matching and PAD’s alignment-based approach, with PAD customized for MNIST through a data selection strategy inspired by DeepCore. DataDAM is also applied to the larger MHIST dataset to test its adaptability to more complex data.

# [ECE1512 - Digital Image Processing and Applications - Project B](/project_B)
## Part 1/2 - State Space Models (SSMs) - Mamba: Linear-Time Sequence Modeling with Selective State Spaces
## Part 3/4 - Vision Language Models (VLMs) - LLaVA: Large Language and Vision Assistant
